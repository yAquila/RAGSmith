networks:
  rag_pipeline_network:

x-ollama: &service-ollama
  image: ollama/ollama
  container_name: rag-pipeline-ollama-gpu
  networks: ['rag_pipeline_network']
  restart: unless-stopped
  ports:
    - 11435:11434
  volumes:
    - ./ollama_storage_rag_pipeline:/root/.ollama

services:

  rag_pipeline:
    image: rag_pipeline_deneme_rag_pipeline
    container_name: rag-pipeline
    networks: ['rag_pipeline_network']
    restart: unless-stopped
    volumes:
      - .:/app
      - ./rag_pipeline_reranker_cache  # Persistent cache for reranker models
    depends_on:
      - qdrant
      - rag-pipeline-ollama-gpu
    ports:
      - 8060:8060
    environment:
      - QDRANT_URL=http://rag-pipeline-qdrant:6333
      - OLLAMA_API_URL=http://rag-pipeline-ollama-gpu:11434/api
      - OLLAMA_API_URL2=http://rag-pipeline-ollama-gpu-2:11434/api
      - GEMINI_API_KEY=${GEMINI_API_KEY:-your-gemini-api-key-here}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']  # GPU for reranker acceleration - change if using different GPU
              capabilities: [gpu]
    # Uncomment to test GPU and cache setup:
    # command: ["python", "test_docker_setup.py"]

  genetic_search_rag:
    image: rag_pipeline_deneme_genetic_search_rag
    container_name: rag-pipeline-genetic-search-rag
    networks: ['rag_pipeline_network']
    restart: unless-stopped
    volumes:
      - .:/app
      - ./genetic_algorithm_results:/app/results  # Save GA results and cache to local directory
    depends_on:
      - rag_pipeline
    environment:
      - RAG_PIPELINE_URL=http://rag_pipeline:8060
      - GA_RESULTS_DIR=/app/results  # Directory for saving optimization results
      - CACHE_FILE_PATH=/app/results/rag_evaluation_cache.json  # Persistent cache file path

  qdrant:
    image: airs_qdrant_server
    container_name: rag-pipeline-qdrant
    networks: ['rag_pipeline_network']
    restart: unless-stopped
    ports:
      - "6335:6333"
    volumes:
      - rag_pipeline_qdrant_storage:/qdrant/storage
      
  rag-pipeline-ollama-gpu:
    <<: *service-ollama
    container_name: rag-pipeline-ollama-gpu
    networks: ['rag_pipeline_network']
    ports:
      - "11435:11434"
    volumes:
      - ./ollama_storage_rag_pipeline:/root/.ollama
    logging:
      driver: "none"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  rag-pipeline-ollama-gpu-2:
    image: ollama/ollama
    container_name: rag-pipeline-ollama-gpu-2
    networks: ['rag_pipeline_network']
    restart: unless-stopped
    ports:
      - "11436:11434"
    volumes:
      - ./ollama_storage_rag_pipeline:/root/.ollama
    logging:
      driver: "none"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2', '3']
              capabilities: [gpu]



  neo4j:
    image: neo4j:latest
    container_name: rag-pipeline-neo4j
    networks: ['rag_pipeline_network']
    restart: unless-stopped
    ports:
      - 7687:7687
      - 7474:7474
    volumes:
      - ./neo4j_storage:/data
    environment:
      - NEO4J_AUTH=neo4j/admin123
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

volumes:
  ollama_storage_rag_pipeline:
  rag_pipeline_qdrant_storage:
  rag_pipeline_reranker_cache:
  neo4j_storage: