{
  "article": {
    "id": "quicksort_b43160f8",
    "title": "Quicksort",
    "url": "https://en.wikipedia.org/wiki/Quicksort",
    "lang": "en",
    "created_at": "2025-07-30T10:21:11.209842",
    "content": "---\nid: quicksort_b43160f8\nurl: https://en.wikipedia.org/wiki/Quicksort\ntitle: Quicksort\nlang: en\ncreated_at: '2025-07-30T10:17:55.149608'\nchecksum: 203a4ae3fdf98bf99004367782bc2c0872326b8898023f4afd66f09892ab3342\noptions:\n  chunk_size: 1000\n  chunk_overlap: 200\n  split_strategy: header_aware\n  total_questions: 10\n  llm_model: gemini-2.5-pro\nstats:\n  word_count: 6223\n  char_count: 35912\n  num_chunks: 24\n  original_chunks: 29\n  filtered_out: 5\n  num_sections: 0\n---\nQuicksort is an efficient, general-purpose sorting algorithm. Quicksort was developed by British computer scientist Tony Hoare in 1959 and published in 1961. It is still a commonly used algorithm for sorting. Overall, it is slightly faster than merge sort and heapsort for randomized data, particularly on larger distributions. Quicksort is a divide-and-conquer algorithm. It works by selecting a \"pivot\" element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. For this reason, it is sometimes called partition-exchange sort. The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting. Quicksort is a comparison sort, meaning that it can sort items of any type for which a \"less-than\" relation (formally, a total order) is defined. It is a comparison-based sort since elements a and b are only swapped in case their relative order has been obtained in the transitive closure of prior comparison-outcomes. Most implementations of quicksort are not stable, meaning that the relative order of equal sort items is not preserved. Mathematical analysis of quicksort shows that, on average, the algorithm takes O ( n log ⁡ n ) {\\displaystyle O(n\\log {n})} comparisons to sort n items. In the worst case, it makes O ( n 2 ) {\\displaystyle O(n^{2})} comparisons. == History == The quicksort algorithm was developed in 1959 by Tony Hoare while he was a visiting student at Moscow State University. At that time, Hoare was working on a machine translation project for the National Physical Laboratory. As a part of the translation process, he needed to sort the words in Russian sentences before looking them up in a Russian-English dictionary, which was in alphabetical order on magnetic tape. After recognizing that his first idea, insertion sort, would be slow, he came up with a new idea. He wrote the partition part in Mercury Autocode but had trouble dealing with the list of unsorted segments. On return to England, he was asked to write code for Shellsort. Hoare mentioned to his boss that he knew of a faster algorithm and his boss bet a sixpence that he did not. His boss ultimately accepted that he had lost the bet. Hoare published a paper about his algorithm in The Computer Journal Volume 5, Issue 1, 1962, Pages 10–16. Later, Hoare learned about ALGOL and its ability to do recursion, which enabled him to publish an improved version of the algorithm in ALGOL in Communications of the Association for Computing Machinery, the premier computer science journal of the time. The ALGOL code is published in Communications of the ACM (CACM), Volume 4, Issue 7 July 1961, pp 321 Algorithm 63: partition and Algorithm 64: Quicksort. Quicksort gained widespread adoption, appearing, for example, in Unix as the default library sort subroutine. Hence, it lent its name to the C standard library subroutine qsort and in the reference implementation of Java. Robert Sedgewick's PhD thesis in 1975 is considered a milestone in the study of Quicksort where he resolved many open problems related to the analysis of various pivot selection schemes including Samplesort, adaptive partitioning by Van Emden as well as derivation of expected number of comparisons and swaps. Jon Bentley and Doug McIlroy in 1993 incorporated various improvements for use in programming libraries, including a technique to deal with equal elements and a pivot scheme known as pseudomedian of nine, where a sample of nine elements is divided into groups of three and then the median of the three medians from three groups is chosen. Bentley described another simpler and compact partitioning scheme in his book Programming Pearls that he attributed to Nico Lomuto. Later Bentley wrote that he used Hoare's version for years but never really understood it but Lomuto's version was simple enough to prove correct. Bentley described Quicksort as the \"most beautiful code I had ever written\" in the same essay. Lomuto's partition scheme was also popularized by the textbook Introduction to Algorithms although it is inferior to Hoare's scheme because it does three times more swaps on average and degrades to O(n2) runtime when all elements are equal. McIlroy would further produce an AntiQuicksort (aqsort) function in 1998, which consistently drives even his 1993 variant of Quicksort into quadratic behavior by producing adversarial data on-the-fly. == Algorithm == Quicksort is a type of divide-and-conquer algorithm for sorting an array, based on a partitioning routine; the details of this partitioning can vary somewhat, so that quicksort is really a family of closely related algorithms. Applied to a range of at least two elements, partitioning produces a division into two consecutive non empty sub-ranges, in such a way that no element of the first sub-range is greater than any element of the second sub-range. After applying this partition, quicksort then recursively sorts the sub-ranges, possibly after excluding from them an element at the point of division that is at this point known to be already in its final location. Due to its recursive nature, quicksort (like the partition routine) has to be formulated so as to be callable for a range within a larger array, even if the ultimate goal is to sort a complete array. The steps for in-place quicksort are: If the range has fewer than two elements, return immediately as there is nothing to do. Possibly for other very short lengths a special-purpose sorting method is applied and the remainder of these steps skipped. Otherwise pick a value, called a pivot, that occurs in the range (the precise manner of choosing depends on the partition routine, and can involve randomness). Partition the range: reorder its elements, while determining a point of division, so that all elements with values less than the pivot come before the division, while all elements with values greater than the pivot come after it; elements that are equal to the pivot can go either way. Since at least one instance of the pivot is present, most partition routines ensure that the value that ends up at the point of division is equal to the pivot, and is now in its final position (but termination of quicksort does not depend on this, as long as sub-ranges strictly smaller than the original are produced). Recursively apply quicksort to the sub-range up to the point of division and to the sub-range after it, possibly excluding from both ranges the element equal to the pivot at the point of division. (If the partition produces a possibly larger sub-range near the boundary where all elements are known to be equal to the pivot, these can be excluded as well.) The choice of partition routine (including the pivot selection) and other details not entirely specified above can affect the algorithm's performance, possibly to a great extent for specific input arrays. In discussing the efficiency of quicksort, it is therefore necessary to specify these choices first. Here we mention two specific partition methods. === Lomuto partition scheme === This scheme is attributed to Nico Lomuto and popularized by Bentley in his book Programming Pearls and Cormen et al. in their book Introduction to Algorithms. In most formulations this scheme chooses as the pivot the last element in the array. The algorithm maintains index i as it scans the array using another index j such that the elements at lo through i-1 (inclusive) are less than the pivot, and the elements at i through j (inclusive) are equal to or greater than the pivot. As this scheme is more compact and easy to understand, it is frequently used in introductory material, although it is less efficient than Hoare's original scheme e.g., when all elements are equal. The complexity of Quicksort with this scheme degrades to O(n2) when the array is already in order, due to the partition being the worst possible one. There have been various variants proposed to boost performance including various ways to select the pivot, deal with equal elements, use other sorting algorithms such as insertion sort for small arrays, and so on. In pseudocode, a quicksort that sorts elements at lo through hi (inclusive) of an array A can be expressed as: // Sorts (a portion of) an array, divides it into partitions, then sorts those algorithm quicksort(A, lo, hi) is // Ensure indices are in correct order if lo >= hi || lo < 0 then return // Partition array and get the pivot index p := partition(A, lo, hi) // Sort the two partitions quicksort(A, lo, p - 1) // Left side of pivot quicksort(A, p + 1, hi) // Right side of pivot // Divides array into two partitions algorithm partition(A, lo, hi) is pivot := A[hi] // Choose the last element as the pivot // Temporary pivot index i := lo for j := lo to hi - 1 do // If the current element is less than or equal to the pivot if A[j] <= pivot then // Swap the current element with the element at the temporary pivot index swap A[i] with A[j] // Move the temporary pivot index forward i := i + 1 // Swap the pivot with the last element swap A[i] with A[hi] return i // the pivot index Sorting the entire array is accomplished by quicksort(A, 0, length(A) - 1). === Hoare partition scheme === The original partition scheme described by Tony Hoare uses two pointers (indices into the range) that start at both ends of the array being partitioned, then move toward each other, until they detect an inversion: a pair of elements, one greater than the pivot at the first pointer, and one less than the pivot at the second pointer; if at this point the first pointer is still before the second, these elements are in the wrong order relative to each other, and they are then exchanged. After this the pointers are moved inwards, and the search for an inversion is repeated; when eventually the pointers cross (the first points after the second), no exchange is performed; a valid partition is found, with the point of division between the crossed pointers (any entries that might be strictly between the crossed pointers are equal to the pivot and can be excluded from both sub-ranges formed). With this formulation it is possible that one sub-range turns out to be the whole original range, which would prevent the algorithm from advancing. Hoare therefore stipulates that at the end, the sub-range containing the pivot element (which still is at its original position) can be decreased in size by excluding that pivot, after (if necessary) exchanging it with the sub-range element closest to the separation; thus, termination of quicksort is ensured. With respect to this original description, implementations often make minor but important variations. Notably, the scheme as presented below includes elements equal to the pivot among the candidates for an inversion (so \"greater than or equal\" and \"less than or equal\" tests are used instead of \"greater than\" and \"less than\" respectively; since the formulation uses do...while rather than repeat...until which is actually reflected by the use of strict comparison operators). While there is no reason to exchange elements equal to the pivot, this change allows tests on the pointers themselves to be omitted, which are otherwise needed to ensure they do not run out of range. Indeed, since at least one instance of the pivot value is present in the range, the first advancement of either pointer cannot pass across this instance if an inclusive test is used; once an exchange is performed, these exchanged elements are now both strictly ahead of the pointer that found them, preventing that pointer from running off. (The latter is true independently of the test used, so it would be possible to use the inclusive test only when looking for the first inversion. However, using an inclusive test throughout also ensures that a division near the middle is found when all elements in the range are equal, which gives an important efficiency gain for sorting arrays with many equal elements.) The risk of producing a non-advancing separation is avoided in a different manner than described by Hoare. Such a separation can only result when no inversions are found, with both pointers advancing to the pivot element at the first iteration (they are then considered to have crossed, and no exchange takes place). In pseudocode, // Sorts (a portion of) an array, divides it into partitions, then sorts those algorithm quicksort(A, lo, hi) is if lo >= 0 && hi >= 0 && lo < hi then p := partition(A, lo, hi) quicksort(A, lo, p) // Note: the pivot is now included quicksort(A, p + 1, hi) // Divides array into two partitions algorithm partition(A, lo, hi) is // Pivot value pivot := A[lo] // Choose the first element as the pivot // Left index i := lo - 1 // Right index j := hi + 1 loop forever // Move the left index to the right at least once and while the element at // the left index is less than the pivot do i := i + 1 while A[i] < pivot // Move the right index to the left at least once and while the element at // the right index is greater than the pivot do j := j - 1 while A[j] > pivot // If the indices crossed, return if i >= j then return j // Swap the elements at the left and right indices swap A[i] with A[j] The entire array is sorted by quicksort(A, 0, length(A) - 1). Hoare's scheme is more efficient than Lomuto's partition scheme because it does three times fewer swaps on average. Also, as mentioned, the implementation given creates a balanced partition even when all values are equal., which Lomuto's scheme does not. Like Lomuto's partition scheme, Hoare's partitioning also would cause Quicksort to degrade to O(n2) for already sorted input, if the pivot was chosen as the first or the last element. With the middle element as the pivot, however, sorted data results with (almost) no swaps in equally sized partitions leading to best case behavior of Quicksort, i.e. O(n log(n)). Like others, Hoare's partitioning doesn't produce a stable sort. In this scheme, the pivot's final location is not necessarily at the index that is returned, as the pivot and elements equal to the pivot can end up anywhere within the partition after a partition step, and may not be sorted until the base case of a partition with a single element is reached via recursion. Therefore, the next two segments that the main algorithm recurs on are (lo..p) (elements ≤ pivot) and (p+1..hi) (elements ≥ pivot) as opposed to (lo..p-1) and (p+1..hi) as in Lomuto's scheme. Subsequent recursions (expansion on previous paragraph) Let's expand a little bit on the next two segments that the main algorithm recurs on. Because we are using strict comparators (>, <) in the \"do...while\" loops to prevent ourselves from running out of range, there's a chance that the pivot itself gets swapped with other elements in the partition function. Therefore, the index returned in the partition function isn't necessarily where the actual pivot is. Consider the example of [5, 2, 3, 1, 0], following the scheme, after the first partition the array becomes [0, 2, 1, 3, 5], the \"index\" returned is 2, which is the number 1, when the real pivot, the one we chose to start the partition with was the number 3. With this example, we see how it is necessary to include the returned index of the partition function in our subsequent recursions. As a result, we are presented with the choices of either recursing on (lo..p) and (p+1..hi), or (lo..p-1) and (p..hi). Which of the two options we choose depends on which index (i or j) we return in the partition function when the indices cross, and how we choose our pivot in the partition function (floor v.s. ceiling). First examine the choice of recursing on (lo..p) and (p+1..hi), with the example of sorting an array where multiple identical elements exist [0, 0]. If index i (the \"latter\" index) is returned after indices cross in the partition function, the index 1 would be returned after the first partition. The subsequent recursion on (lo..p)would be on (0, 1), which corresponds to the exact same array [0, 0]. A non-advancing separation that causes infinite recursion is produced. It is therefore obvious that when recursing on (lo..p) and (p+1..hi), because the left half of the recursion includes the returned index, it is the partition function's job to exclude the \"tail\" in non-advancing scenarios. Which is to say, index j (the \"former\" index when indices cross) should be returned instead of i. Going with a similar logic, when considering the example of an already sorted array [0, 1], the choice of pivot needs to be \"floor\" to ensure that the pointers stop on the \"former\" instead of the \"latter\" (with \"ceiling\" as the pivot, the index 1 would be returned and included in (lo..p) causing infinite recursion). It is for the exact same reason why choice of the last element as pivot must be avoided. The choice of recursing on (lo..p-1) and (p..hi) follows the exact same logic as above. Because the right half of the recursion includes the returned index, it is the partition function's job to exclude the \"head\" in non-advancing scenarios. The index i (the \"latter\" index after the indices cross) in the partition function needs to be returned, and \"ceiling\" needs to be chosen as the pivot. The two nuances are clear, again, when considering the examples of sorting an array where multiple identical elements exist ([0, 0]), and an already sorted array [0, 1] respectively. It is noteworthy that with version of recursion, for the same reason, choice of the first element as pivot must be avoided. === Implementation issues === ==== Choice of pivot ==== In the very early versions of quicksort, the leftmost element of the partition would often be chosen as the pivot element. Unfortunately, this causes worst-case behavior on already sorted arrays, which is a rather common use-case. The problem was easily solved by choosing either a random index for the pivot, choosing the middle index of the partition or (especially for longer partitions) choosing the median of the first, middle and last element of the partition for the pivot (as recommended by Sedgewick). This \"median-of-three\" rule counters the case of sorted (or reverse-sorted) input, and gives a better estimate of the optimal pivot (the true median) than selecting any single element, when no information about the ordering of the input is known. Median-of-three code snippet for Lomuto partition: mid := ⌊(lo + hi) / 2⌋ if A[mid] < A[lo] swap A[lo] with A[mid] if A[hi] < A[lo] swap A[lo] with A[hi] if A[mid] < A[hi] swap A[mid] with A[hi] pivot := A[hi] It puts a median into A[hi] first, then that new value of A[hi] is used for a pivot, as in a basic algorithm presented above. Specifically, the expected number of comparisons needed to sort n elements (see § Average-case analysis) with random pivot selection is 1.386 n log n. Median-of-three pivoting brings this down to Cn, 2 ≈ 1.188 n log n, at the expense of a three-percent increase in the expected number of swaps. An even stronger pivoting rule, for larger arrays, is to pick the ninther, a recursive median-of-three (Mo3), defined as ninther(a) = median(Mo3(first ⁠1/3⁠ of a), Mo3(middle ⁠1/3⁠ of a), Mo3(final ⁠1/3⁠ of a)) Selecting a pivot element is also complicated by the existence of integer overflow. If the boundary indices of the subarray being sorted are sufficiently large, the naïve expression for the middle index, (lo + hi)/2, will cause overflow and provide an invalid pivot index. This can be overcome by using, for example, lo + (hi−lo)/2 to index the middle element, at the cost of more complex arithmetic. Similar issues arise in some other methods of selecting the pivot element. ==== Repeated elements ==== With a partitioning algorithm such as the Lomuto partition scheme described above (even one that chooses good pivot values), quicksort exhibits poor performance for inputs that contain many repeated elements. The problem is clearly apparent when all the input elements are equal: at each recursion, the left partition is empty (no input values are less than the pivot), and the right partition has only decreased by one element (the pivot is removed). Consequently, the Lomuto partition scheme takes quadratic time to sort an array of equal values. However, with a partitioning algorithm such as the Hoare partition scheme, repeated elements generally results in better partitioning, and although needless swaps of elements equal to the pivot may occur, the running time generally decreases as the number of repeated elements increases (with memory cache reducing the swap overhead). In the case where all elements are equal, Hoare partition scheme needlessly swaps elements, but the partitioning itself is best case, as noted in the Hoare partition section above. To solve the Lomuto partition scheme problem (sometimes called the Dutch national flag problem), an alternative linear-time partition routine can be used that separates the values into three groups: values less than the pivot, values equal to the pivot, and values greater than the pivot. (Bentley and McIlroy call this a \"fat partition\" and it was already implemented in the qsort of Version 7 Unix.) The values equal to the pivot are already sorted, so only the less-than and greater-than partitions need to be recursively sorted. In pseudocode, the quicksort algorithm becomes: // Sorts (a portion of) an array, divides it into partitions, then sorts those algorithm quicksort(A, lo, hi) is if lo >= 0 && lo < hi then lt, gt := partition(A, lo, hi) // Multiple return values quicksort(A, lo, lt - 1) quicksort(A, gt + 1, hi) // Divides array into three partitions algorithm partition(A, lo, hi) is // Pivot value pivot := A[(lo + hi) / 2] // Choose the middle element as the pivot (integer division) // Lesser, equal and greater index lt := lo eq := lo gt := hi // Iterate and compare all elements with the pivot while eq <= gt do if A[eq] < pivot then // Swap the elements at the equal and lesser indices swap A[eq] with A[lt] // Increase lesser index lt := lt + 1 // Increase equal index eq := eq + 1 else if A[eq] > pivot then // Swap the elements at the equal and greater indices swap A[eq] with A[gt] // Decrease greater index gt := gt - 1 else // if A[eq] = pivot then // Increase equal index eq := eq + 1 // Return lesser and greater indices return lt, gt The partition algorithm returns indices to the first ('leftmost') and to the last ('rightmost') item of the middle partition. Every other item of the partition is equal to the pivot and is therefore sorted. Consequently, the items of the partition need not be included in the recursive calls to quicksort. The best case for the algorithm now occurs when all elements are equal (or are chosen from a small set of k ≪ n elements). In the case of all equal elements, the modified quicksort will perform only two recursive calls on empty subarrays and thus finish in linear time (assuming the partition subroutine takes no longer than linear time). ==== Optimizations ==== Other important optimizations, also suggested by Sedgewick and widely used in practice, are: To make sure at most O(log n) space is used, recur first into the smaller side of the partition, then use a tail call to recur into the other, or update the parameters to no longer include the now sorted smaller side, and iterate to sort the larger side. When the number of elements is below some threshold (perhaps ten elements), switch to a non-recursive sorting algorithm such as insertion sort that performs fewer swaps, comparisons or other operations on such small arrays. The ideal 'threshold' will vary based on the details of the specific implementation. An older variant of the previous optimization: when the number of elements is less than the threshold k, simply stop; then after the whole array has been processed, perform insertion sort on it. Stopping the recursion early leaves the array k-sorted, meaning that each element is at most k positions away from its final sorted position. In this case, insertion sort takes O(kn) time to finish the sort, which is linear if k is a constant. Compared to the \"many small sorts\" optimization, this version may execute fewer instructions, but it makes suboptimal use of the cache memories in modern computers. ==== Parallelization ==== Quicksort's divide-and-conquer formulation makes it amenable to parallelization using task parallelism. The partitioning step is accomplished through the use of a parallel prefix sum algorithm to compute an index for each array element in its section of the partitioned array. Given an array of size n, the partitioning step performs O(n) work in O(log n) time and requires O(n) additional scratch space. After the array has been partitioned, the two partitions can be sorted recursively in parallel. Assuming an ideal choice of pivots, parallel quicksort sorts an array of size n in O(n log n) work in O(log2 n) time using O(n) additional space. Quicksort has some disadvantages when compared to alternative sorting algorithms, like merge sort, which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, it is difficult to parallelize the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads. Other more sophisticated parallel sorting algorithms can achieve even better time bounds. For example, in 1991 David M W Powers described a parallelized quicksort (and a related radix sort) that can operate in O(log n) time on a CRCW (concurrent read and concurrent write) PRAM (parallel random-access machine) with n processors by performing partitioning implicitly. == Formal analysis == === Worst-case analysis === The most unbalanced partition occurs when one of the sublists returned by the partitioning routine is of size n − 1. This may occur if the pivot happens to be the smallest or largest element in the list, or in some implementations (e.g., the Lomuto partition scheme as described above) when all the elements are equal. If this happens repeatedly in every partition, then each recursive call processes a list of size one less than the previous list. Consequently, it takes n − 1 nested calls before to reach a list of size 1. This means that the call tree is a linear chain of n − 1 nested calls. The ith call does O(n − i) work to do the partition, and ∑ i = 0 n ( n − i ) = O ( n 2 ) {\\displaystyle \\textstyle \\sum _{i=0}^{n}(n-i)=O(n^{2})} , so in that case quicksort takes O(n2) time. === Best-case analysis === In the most balanced case, each partition divides the list into two nearly equal pieces. This means each recursive call processes a list of half the size. Consequently, only log2 n nested calls can be made before reaching a list of size 1. This means that the depth of the call tree is log2 n. But no two calls at the same level of the call tree process the same part of the original list; thus, each level of calls needs only O(n) time all together (each call has some constant overhead, but since there are only O(n) calls at each level, this is subsumed in the O(n) factor). The result is that the algorithm uses only O(n log n) time. === Average-case analysis === To sort an array of n distinct elements, quicksort takes O(n log n) time in expectation, averaged over all n! permutations of n elements with equal probability. Alternatively, if the algorithm selects the pivot uniformly at random from the input array, the same analysis can be used to bound the expected running time for any input sequence; the expectation is then taken over the random choices made by the algorithm (Cormen et al., Introduction to Algorithms, Section 7.3). Three common proofs to this claim use percentiles, recurrences, and binary search trees, each providing different insights into quicksort's workings. ==== Using percentiles ==== If each pivot has rank somewhere in the middle 50 percent, that is, between the 25th percentile and the 75th percentile, then it splits the elements with at least 25% and at most 75% on each side. Consistently choosing such pivots would only have to split the list at most log 4 / 3 ⁡ n {\\displaystyle \\log _{4/3}n} times before reaching lists of size 1, yielding an O(n log n) algorithm. When the input is a random permutation, the pivot has a random rank, and so it is not guaranteed to be in the middle 50 percent. However, when starting from a random permutation, each recursive call's pivot has a random rank in its list, and therefore is in the middle 50 percent approximately half the time. Imagine that a coin is flipped: heads means that the rank of the pivot is in the middle 50 percent, tail means that it isn't. Now imagine that the coin is flipped over and over until it gets k heads. Although this could take a long time, on average only 2k flips are required, and the chance that the coin won't get k heads after 100k flips is highly improbable (this can be made rigorous using Chernoff bounds). By the same argument, Quicksort's recursion will terminate on average at a call depth of only 2 log 4 / 3 ⁡ n {\\displaystyle 2\\log _{4/3}n} . But if its average call depth is O(log n), and each level of the call tree processes at most n elements, the total amount of work done on average is the product, O(n log n). The algorithm does not have to verify that the pivot is in the middle half as long as it is a consistent amount of times. Using more careful arguments, it is possible to extend this proof, for the version of Quicksort where the pivot is randomnly chosen, to show a time bound that holds with high probability: specifically, for any give a ≥ 4 {\\displaystyle a\\geq 4} , let c = ( a − 4 ) / 2 {\\displaystyle c=(a-4)/2} , then with probability at least 1 − 1 n c {\\displaystyle 1-{\\frac {1}{n^{c}}}} , the number of comparisons will not exceed 2 a n log 4 / 3 ⁡ n {\\displaystyle 2an\\log _{4/3}n} . ==== Using recurrences ==== An alternative approach is to set up a recurrence relation for the T(n) factor, the time needed to sort a list of size n. In the most unbalanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size 0 and n−1, so the recurrence relation is T ( n ) = O ( n ) + T ( 0 ) + T ( n − 1 ) = O ( n ) + T ( n − 1 ) . {\\displaystyle T(n)=O(n)+T(0)+T(n-1)=O(n)+T(n-1).} This is the same relation as for insertion sort and selection sort, and it solves to worst case T(n) = O(n2). In the most balanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size n/2, so the recurrence relation is T ( n ) = O ( n ) + 2 T ( n 2 ) . {\\displaystyle T(n)=O(n)+2T\\left({\\frac {n}{2}}\\right).} The master theorem for divide-and-conquer recurrences tells us that T(n) = O(n log n). The outline of a formal proof of the O(n log n) expected time complexity follows. Assume that there are no duplicates as duplicates could be handled with linear time pre- and post-processing, or considered cases easier than the analyzed. When the input is a random permutation, the rank of the pivot is uniform random from 0 to n − 1. Then the resulting parts of the partition have sizes i and n − i − 1, and i is uniform random from 0 to n − 1. So, averaging over all possible splits and noting that the number of comparisons for the partition is n − 1, the average number of comparisons over all permutations of the input sequence can be estimated accurately by solving the recurrence relation: C ( n ) = n − 1 + 1 n ∑ i = 0 n − 1 ( C ( i ) + C ( n − i − 1 ) ) = n − 1 + 2 n ∑ i = 0 n − 1 C ( i ) {\\displaystyle C(n)=n-1+{\\frac {1}{n}}\\sum _{i=0}^{n-1}(C(i)+C(n-i-1))=n-1+{\\frac {2}{n}}\\sum _{i=0}^{n-1}C(i)} n C ( n ) = n ( n − 1 ) + 2 ∑ i = 0 n − 1 C ( i ) {\\displaystyle nC(n)=n(n-1)+2\\sum _{i=0}^{n-1}C(i)} n C ( n ) − ( n − 1 ) C ( n − 1 ) = n ( n − 1 ) − ( n − 1 ) ( n − 2 ) + 2 C ( n − 1 ) {\\displaystyle nC(n)-(n-1)C(n-1)=n(n-1)-(n-1)(n-2)+2C(n-1)} n C ( n ) = ( n + 1 ) C ( n − 1 ) + 2 n − 2 {\\displaystyle nC(n)=(n+1)C(n-1)+2n-2} C ( n ) n + 1 = C ( n − 1 ) n + 2 n + 1 − 2 n ( n + 1 ) ≤ C ( n − 1 ) n + 2 n + 1 = C ( n − 2 ) n − 1 + 2 n − 2 ( n − 1 ) n + 2 n + 1 ≤ C ( n − 2 ) n − 1 + 2 n + 2 n + 1 ⋮ = C ( 1 ) 2 + ∑ i = 2 n 2 i + 1 ≤ 2 ∑ i = 1 n − 1 1 i ≈ 2 ∫ 1 n 1 x d x = 2 ln ⁡ n {\\displaystyle {\\begin{aligned}{\\frac {C(n)}{n+1}}&={\\frac {C(n-1)}{n}}+{\\frac {2}{n+1}}-{\\frac {2}{n(n+1)}}\\leq {\\frac {C(n-1)}{n}}+{\\frac {2}{n+1}}\\\\&={\\frac {C(n-2)}{n-1}}+{\\frac {2}{n}}-{\\frac {2}{(n-1)n}}+{\\frac {2}{n+1}}\\leq {\\frac {C(n-2)}{n-1}}+{\\frac {2}{n}}+{\\frac {2}{n+1}}\\\\&\\ \\ \\vdots \\\\&={\\frac {C(1)}{2}}+\\sum _{i=2}^{n}{\\frac {2}{i+1}}\\leq 2\\sum _{i=1}^{n-1}{\\frac {1}{i}}\\approx 2\\int _{1}^{n}{\\frac {1}{x}}\\mathrm {d} x=2\\ln n\\end{aligned}}} Solving the recurrence gives C(n) = 2 n ln n ≈ 1.39 n log2 n. This means that, on average, quicksort performs only about 39% worse than in its best case. In this sense, it is closer to the best case than the worst case. A comparison sort cannot use less than log2(n!) comparisons on average to sort n items (as explained in the article Comparison sort) and in case of large n, Stirling's approximation yields log2(n!) ≈ n(log2 n − log2 e), so quicksort is not much worse than an ideal comparison sort. This fast average runtime is another reason for quicksort's practical dominance over other sorting algorithms. ==== Using a binary search tree ==== The following binary search tree (BST) corresponds to each execution of quicksort: the initial pivot is the root node; the pivot of the left half is the root of the left subtree, the pivot of the right half is the root of the right subtree, and so on. The number of comparisons of the execution of quicksort equals the number of comparisons during the construction of the BST by a sequence of insertions. So, the average number of comparisons for randomized quicksort equals the average cost of constructing a BST when the values inserted ( x 1 , x 2 , … , x n ) {\\displaystyle (x_{1},x_{2},\\ldots ,x_{n})} form a random permutation. Consider a BST created by insertion of a sequence ( x 1 , x 2 , … , x n ) {\\displaystyle (x_{1},x_{2},\\ldots ,x_{n})} of values forming a random permutation. Let C denote the cost of creation of the BST. We have C = ∑ i ∑ j < i c i , j {\\displaystyle C=\\sum _{i}\\sum _{j 3.0.CO;2-#. Donald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Pages 113–122 of section 5.2.2: Sorting by Exchanging. Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Chapter 7: Quicksort, pp. 145–164. Moller, Faron. \"Analysis of Quicksort\" (PDF). Archived from the original (PDF) on 7 July 2022. Retrieved 3 December 2024. (CS 332: Designing Algorithms. Department of Computer Science, Swansea University.) Martínez, C.; Roura, S. (2001). \"Optimal Sampling Strategies in Quicksort and Quickselect\". SIAM J. Comput. 31 (3): 683–705. CiteSeerX 10.1.1.17.4954. doi:10.1137/S0097539700382108. Bentley, J. L.; McIlroy, M. D. (1993). \"Engineering a sort function\". Software: Practice and Experience. 23 (11): 1249–1265. CiteSeerX 10.1.1.14.8162. doi:10.1002/spe.4380231105. S2CID 8822797. == External links == \"Animated Sorting Algorithms: Quick Sort\". Archived from the original on 2 March 2015. Retrieved 25 November 2008. – graphical demonstration \"Animated Sorting Algorithms: Quick Sort (3-way partition)\". Archived from the original on 6 March 2015. Retrieved 25 November 2008. Open Data Structures – Section 11.1.2 – Quicksort, Pat Morin Interactive illustration of Quicksort, with code walkthrough Fast Sorting with Quicksort, a beginner-friendly deep-dive"
  },
  "chunks": [
    {
      "id": "quicksort_b43160f8_c0000",
      "article_id": "quicksort_b43160f8",
      "section": "Lead",
      "heading_path": "Lead",
      "start_char": 0,
      "end_char": 903,
      "content": "Quicksort is an efficient, general-purpose sorting algorithm. Quicksort was developed by British computer scientist Tony Hoare in 1959 and published in 1961. It is still a commonly used algorithm for sorting. Overall, it is slightly faster than merge sort and heapsort for randomized data, particularly on larger distributions. Quicksort is a divide-and-conquer algorithm. It works by selecting a \"pivot\" element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. For this reason, it is sometimes called partition-exchange sort. The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting. Quicksort is a comparison sort, meaning that it can sort items of any type for which a \"less-than\" relation (formally, a total order) is defined.",
      "char_count": 902,
      "token_estimate": 225,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0001",
      "article_id": "quicksort_b43160f8",
      "section": "Lead",
      "heading_path": "Lead",
      "start_char": 903,
      "end_char": 1424,
      "content": "It is a comparison-based sort since elements a and b are only swapped in case their relative order has been obtained in the transitive closure of prior comparison-outcomes. Most implementations of quicksort are not stable, meaning that the relative order of equal sort items is not preserved. Mathematical analysis of quicksort shows that, on average, the algorithm takes O ( n log ⁡ n ) {\\displaystyle O(n\\log {n})} comparisons to sort n items. In the worst case, it makes O ( n 2 ) {\\displaystyle O(n^{2})} comparisons.",
      "char_count": 521,
      "token_estimate": 130,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0002",
      "article_id": "quicksort_b43160f8",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 1438,
      "end_char": 2407,
      "content": "== History == The quicksort algorithm was developed in 1959 by Tony Hoare while he was a visiting student at Moscow State University. At that time, Hoare was working on a machine translation project for the National Physical Laboratory. As a part of the translation process, he needed to sort the words in Russian sentences before looking them up in a Russian-English dictionary, which was in alphabetical order on magnetic tape. After recognizing that his first idea, insertion sort, would be slow, he came up with a new idea. He wrote the partition part in Mercury Autocode but had trouble dealing with the list of unsorted segments. On return to England, he was asked to write code for Shellsort. Hoare mentioned to his boss that he knew of a faster algorithm and his boss bet a sixpence that he did not. His boss ultimately accepted that he had lost the bet. Hoare published a paper about his algorithm in The Computer Journal Volume 5, Issue 1, 1962, Pages 10–16.",
      "char_count": 968,
      "token_estimate": 242,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0003",
      "article_id": "quicksort_b43160f8",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 2407,
      "end_char": 3345,
      "content": "Later, Hoare learned about ALGOL and its ability to do recursion, which enabled him to publish an improved version of the algorithm in ALGOL in Communications of the Association for Computing Machinery, the premier computer science journal of the time. The ALGOL code is published in Communications of the ACM (CACM), Volume 4, Issue 7 July 1961, pp 321 Algorithm 63: partition and Algorithm 64: Quicksort. Quicksort gained widespread adoption, appearing, for example, in Unix as the default library sort subroutine. Hence, it lent its name to the C standard library subroutine qsort and in the reference implementation of Java. Robert Sedgewick's PhD thesis in 1975 is considered a milestone in the study of Quicksort where he resolved many open problems related to the analysis of various pivot selection schemes including Samplesort, adaptive partitioning by Van Emden as well as derivation of expected number of comparisons and swaps.",
      "char_count": 938,
      "token_estimate": 234,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0004",
      "article_id": "quicksort_b43160f8",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 3346,
      "end_char": 4299,
      "content": "Jon Bentley and Doug McIlroy in 1993 incorporated various improvements for use in programming libraries, including a technique to deal with equal elements and a pivot scheme known as pseudomedian of nine, where a sample of nine elements is divided into groups of three and then the median of the three medians from three groups is chosen. Bentley described another simpler and compact partitioning scheme in his book Programming Pearls that he attributed to Nico Lomuto. Later Bentley wrote that he used Hoare's version for years but never really understood it but Lomuto's version was simple enough to prove correct. Bentley described Quicksort as the \"most beautiful code I had ever written\" in the same essay. Lomuto's partition scheme was also popularized by the textbook Introduction to Algorithms although it is inferior to Hoare's scheme because it does three times more swaps on average and degrades to O(n2) runtime when all elements are equal.",
      "char_count": 953,
      "token_estimate": 238,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0005",
      "article_id": "quicksort_b43160f8",
      "section": "== Algorithm ==",
      "heading_path": "== Algorithm ==",
      "start_char": 4503,
      "end_char": 5389,
      "content": "== Algorithm == Quicksort is a type of divide-and-conquer algorithm for sorting an array, based on a partitioning routine; the details of this partitioning can vary somewhat, so that quicksort is really a family of closely related algorithms. Applied to a range of at least two elements, partitioning produces a division into two consecutive non empty sub-ranges, in such a way that no element of the first sub-range is greater than any element of the second sub-range. After applying this partition, quicksort then recursively sorts the sub-ranges, possibly after excluding from them an element at the point of division that is at this point known to be already in its final location. Due to its recursive nature, quicksort (like the partition routine) has to be formulated so as to be callable for a range within a larger array, even if the ultimate goal is to sort a complete array.",
      "char_count": 885,
      "token_estimate": 221,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0006",
      "article_id": "quicksort_b43160f8",
      "section": "== Algorithm ==",
      "heading_path": "== Algorithm ==",
      "start_char": 5389,
      "end_char": 6084,
      "content": "The steps for in-place quicksort are: If the range has fewer than two elements, return immediately as there is nothing to do. Possibly for other very short lengths a special-purpose sorting method is applied and the remainder of these steps skipped. Otherwise pick a value, called a pivot, that occurs in the range (the precise manner of choosing depends on the partition routine, and can involve randomness). Partition the range: reorder its elements, while determining a point of division, so that all elements with values less than the pivot come before the division, while all elements with values greater than the pivot come after it; elements that are equal to the pivot can go either way.",
      "char_count": 695,
      "token_estimate": 173,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0007",
      "article_id": "quicksort_b43160f8",
      "section": "== Algorithm ==",
      "heading_path": "== Algorithm ==",
      "start_char": 6085,
      "end_char": 7067,
      "content": "Since at least one instance of the pivot is present, most partition routines ensure that the value that ends up at the point of division is equal to the pivot, and is now in its final position (but termination of quicksort does not depend on this, as long as sub-ranges strictly smaller than the original are produced). Recursively apply quicksort to the sub-range up to the point of division and to the sub-range after it, possibly excluding from both ranges the element equal to the pivot at the point of division. (If the partition produces a possibly larger sub-range near the boundary where all elements are known to be equal to the pivot, these can be excluded as well.) The choice of partition routine (including the pivot selection) and other details not entirely specified above can affect the algorithm's performance, possibly to a great extent for specific input arrays. In discussing the efficiency of quicksort, it is therefore necessary to specify these choices first.",
      "char_count": 982,
      "token_estimate": 245,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0008",
      "article_id": "quicksort_b43160f8",
      "section": "== = Lomuto partition scheme ==",
      "heading_path": "== = Lomuto partition scheme ==",
      "start_char": 7132,
      "end_char": 7994,
      "content": "== = Lomuto partition scheme === This scheme is attributed to Nico Lomuto and popularized by Bentley in his book Programming Pearls and Cormen et al. in their book Introduction to Algorithms. In most formulations this scheme chooses as the pivot the last element in the array. The algorithm maintains index i as it scans the array using another index j such that the elements at lo through i-1 (inclusive) are less than the pivot, and the elements at i through j (inclusive) are equal to or greater than the pivot. As this scheme is more compact and easy to understand, it is frequently used in introductory material, although it is less efficient than Hoare's original scheme e.g., when all elements are equal. The complexity of Quicksort with this scheme degrades to O(n2) when the array is already in order, due to the partition being the worst possible one.",
      "char_count": 861,
      "token_estimate": 215,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0009",
      "article_id": "quicksort_b43160f8",
      "section": "== = Lomuto partition scheme ==",
      "heading_path": "== = Lomuto partition scheme ==",
      "start_char": 7994,
      "end_char": 8757,
      "content": "There have been various variants proposed to boost performance including various ways to select the pivot, deal with equal elements, use other sorting algorithms such as insertion sort for small arrays, and so on. In pseudocode, a quicksort that sorts elements at lo through hi (inclusive) of an array A can be expressed as: // Sorts (a portion of) an array, divides it into partitions, then sorts those algorithm quicksort(A, lo, hi) is // Ensure indices are in correct order if lo >= hi || lo < 0 then return // Partition array and get the pivot index p := partition(A, lo, hi) // Sort the two partitions quicksort(A, lo, p - 1) // Left side of pivot quicksort(A, p + 1, hi) // Right side of pivot // Divides array into two partitions algorithm partition(A, lo,",
      "char_count": 763,
      "token_estimate": 190,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0010",
      "article_id": "quicksort_b43160f8",
      "section": "== = Lomuto partition scheme ==",
      "heading_path": "== = Lomuto partition scheme ==",
      "start_char": 8758,
      "end_char": 9272,
      "content": "hi) is pivot := A[hi] // Choose the last element as the pivot // Temporary pivot index i := lo for j := lo to hi - 1 do // If the current element is less than or equal to the pivot if A[j] <= pivot then // Swap the current element with the element at the temporary pivot index swap A[i] with A[j] // Move the temporary pivot index forward i := i + 1 // Swap the pivot with the last element swap A[i] with A[hi] return i // the pivot index Sorting the entire array is accomplished by quicksort(A, 0, length(A) - 1).",
      "char_count": 514,
      "token_estimate": 128,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0011",
      "article_id": "quicksort_b43160f8",
      "section": "== = Hoare partition scheme ==",
      "heading_path": "== = Hoare partition scheme ==",
      "start_char": 9272,
      "end_char": 10200,
      "content": "== = Hoare partition scheme === The original partition scheme described by Tony Hoare uses two pointers (indices into the range) that start at both ends of the array being partitioned, then move toward each other, until they detect an inversion: a pair of elements, one greater than the pivot at the first pointer, and one less than the pivot at the second pointer; if at this point the first pointer is still before the second, these elements are in the wrong order relative to each other, and they are then exchanged. After this the pointers are moved inwards, and the search for an inversion is repeated; when eventually the pointers cross (the first points after the second), no exchange is performed; a valid partition is found, with the point of division between the crossed pointers (any entries that might be strictly between the crossed pointers are equal to the pivot and can be excluded from both sub-ranges formed).",
      "char_count": 927,
      "token_estimate": 231,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0012",
      "article_id": "quicksort_b43160f8",
      "section": "== = Hoare partition scheme ==",
      "heading_path": "== = Hoare partition scheme ==",
      "start_char": 10200,
      "end_char": 11135,
      "content": "With this formulation it is possible that one sub-range turns out to be the whole original range, which would prevent the algorithm from advancing. Hoare therefore stipulates that at the end, the sub-range containing the pivot element (which still is at its original position) can be decreased in size by excluding that pivot, after (if necessary) exchanging it with the sub-range element closest to the separation; thus, termination of quicksort is ensured. With respect to this original description, implementations often make minor but important variations. Notably, the scheme as presented below includes elements equal to the pivot among the candidates for an inversion (so \"greater than or equal\" and \"less than or equal\" tests are used instead of \"greater than\" and \"less than\" respectively; since the formulation uses do...while rather than repeat...until which is actually reflected by the use of strict comparison operators).",
      "char_count": 935,
      "token_estimate": 233,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0013",
      "article_id": "quicksort_b43160f8",
      "section": "== = Hoare partition scheme ==",
      "heading_path": "== = Hoare partition scheme ==",
      "start_char": 11136,
      "end_char": 11821,
      "content": "While there is no reason to exchange elements equal to the pivot, this change allows tests on the pointers themselves to be omitted, which are otherwise needed to ensure they do not run out of range. Indeed, since at least one instance of the pivot value is present in the range, the first advancement of either pointer cannot pass across this instance if an inclusive test is used; once an exchange is performed, these exchanged elements are now both strictly ahead of the pointer that found them, preventing that pointer from running off. (The latter is true independently of the test used, so it would be possible to use the inclusive test only when looking for the first inversion.",
      "char_count": 685,
      "token_estimate": 171,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0014",
      "article_id": "quicksort_b43160f8",
      "section": "== = Hoare partition scheme ==",
      "heading_path": "== = Hoare partition scheme ==",
      "start_char": 11822,
      "end_char": 12365,
      "content": "However, using an inclusive test throughout also ensures that a division near the middle is found when all elements in the range are equal, which gives an important efficiency gain for sorting arrays with many equal elements.) The risk of producing a non-advancing separation is avoided in a different manner than described by Hoare. Such a separation can only result when no inversions are found, with both pointers advancing to the pivot element at the first iteration (they are then considered to have crossed, and no exchange takes place).",
      "char_count": 543,
      "token_estimate": 135,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0015",
      "article_id": "quicksort_b43160f8",
      "section": "== = Hoare partition scheme ==",
      "heading_path": "== = Hoare partition scheme ==",
      "start_char": 12366,
      "end_char": 13337,
      "content": "In pseudocode, // Sorts (a portion of) an array, divides it into partitions, then sorts those algorithm quicksort(A, lo, hi) is if lo >= 0 && hi >= 0 && lo < hi then p := partition(A, lo, hi) quicksort(A, lo, p) // Note: the pivot is now included quicksort(A, p + 1, hi) // Divides array into two partitions algorithm partition(A, lo, hi) is // Pivot value pivot := A[lo] // Choose the first element as the pivot // Left index i := lo - 1 // Right index j := hi + 1 loop forever // Move the left index to the right at least once and while the element at // the left index is less than the pivot do i := i + 1 while A[i] < pivot // Move the right index to the left at least once and while the element at // the right index is greater than the pivot do j := j - 1 while A[j] > pivot // If the indices crossed, return if i >= j then return j // Swap the elements at the left and right indices swap A[i] with A[j] The entire array is sorted by quicksort(A, 0, length(A) - 1).",
      "char_count": 971,
      "token_estimate": 242,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0016",
      "article_id": "quicksort_b43160f8",
      "section": "== = Hoare partition scheme ==",
      "heading_path": "== = Hoare partition scheme ==",
      "start_char": 13338,
      "end_char": 14329,
      "content": "Hoare's scheme is more efficient than Lomuto's partition scheme because it does three times fewer swaps on average. Also, as mentioned, the implementation given creates a balanced partition even when all values are equal., which Lomuto's scheme does not. Like Lomuto's partition scheme, Hoare's partitioning also would cause Quicksort to degrade to O(n2) for already sorted input, if the pivot was chosen as the first or the last element. With the middle element as the pivot, however, sorted data results with (almost) no swaps in equally sized partitions leading to best case behavior of Quicksort, i.e. O(n log(n)). Like others, Hoare's partitioning doesn't produce a stable sort. In this scheme, the pivot's final location is not necessarily at the index that is returned, as the pivot and elements equal to the pivot can end up anywhere within the partition after a partition step, and may not be sorted until the base case of a partition with a single element is reached via recursion.",
      "char_count": 991,
      "token_estimate": 247,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0017",
      "article_id": "quicksort_b43160f8",
      "section": "== = Hoare partition scheme ==",
      "heading_path": "== = Hoare partition scheme ==",
      "start_char": 14330,
      "end_char": 15242,
      "content": "Therefore, the next two segments that the main algorithm recurs on are (lo..p) (elements ≤ pivot) and (p+1..hi) (elements ≥ pivot) as opposed to (lo..p-1) and (p+1..hi) as in Lomuto's scheme. Subsequent recursions (expansion on previous paragraph) Let's expand a little bit on the next two segments that the main algorithm recurs on. Because we are using strict comparators (>, <) in the \"do...while\" loops to prevent ourselves from running out of range, there's a chance that the pivot itself gets swapped with other elements in the partition function. Therefore, the index returned in the partition function isn't necessarily where the actual pivot is. Consider the example of [5, 2, 3, 1, 0], following the scheme, after the first partition the array becomes [0, 2, 1, 3, 5], the \"index\" returned is 2, which is the number 1, when the real pivot, the one we chose to start the partition with was the number 3.",
      "char_count": 912,
      "token_estimate": 228,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0018",
      "article_id": "quicksort_b43160f8",
      "section": "== = Hoare partition scheme ==",
      "heading_path": "== = Hoare partition scheme ==",
      "start_char": 15243,
      "end_char": 16169,
      "content": "With this example, we see how it is necessary to include the returned index of the partition function in our subsequent recursions. As a result, we are presented with the choices of either recursing on (lo..p) and (p+1..hi), or (lo..p-1) and (p..hi). Which of the two options we choose depends on which index (i or j) we return in the partition function when the indices cross, and how we choose our pivot in the partition function (floor v.s. ceiling). First examine the choice of recursing on (lo..p) and (p+1..hi), with the example of sorting an array where multiple identical elements exist [0, 0]. If index i (the \"latter\" index) is returned after indices cross in the partition function, the index 1 would be returned after the first partition. The subsequent recursion on (lo..p)would be on (0, 1), which corresponds to the exact same array [0, 0]. A non-advancing separation that causes infinite recursion is produced.",
      "char_count": 926,
      "token_estimate": 231,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0019",
      "article_id": "quicksort_b43160f8",
      "section": "== = Hoare partition scheme ==",
      "heading_path": "== = Hoare partition scheme ==",
      "start_char": 16170,
      "end_char": 17139,
      "content": "It is therefore obvious that when recursing on (lo..p) and (p+1..hi), because the left half of the recursion includes the returned index, it is the partition function's job to exclude the \"tail\" in non-advancing scenarios. Which is to say, index j (the \"former\" index when indices cross) should be returned instead of i. Going with a similar logic, when considering the example of an already sorted array [0, 1], the choice of pivot needs to be \"floor\" to ensure that the pointers stop on the \"former\" instead of the \"latter\" (with \"ceiling\" as the pivot, the index 1 would be returned and included in (lo..p) causing infinite recursion). It is for the exact same reason why choice of the last element as pivot must be avoided. The choice of recursing on (lo..p-1) and (p..hi) follows the exact same logic as above. Because the right half of the recursion includes the returned index, it is the partition function's job to exclude the \"head\" in non-advancing scenarios.",
      "char_count": 969,
      "token_estimate": 242,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0020",
      "article_id": "quicksort_b43160f8",
      "section": "== = Hoare partition scheme ==",
      "heading_path": "== = Hoare partition scheme ==",
      "start_char": 17140,
      "end_char": 17598,
      "content": "The index i (the \"latter\" index after the indices cross) in the partition function needs to be returned, and \"ceiling\" needs to be chosen as the pivot. The two nuances are clear, again, when considering the examples of sorting an array where multiple identical elements exist ([0, 0]), and an already sorted array [0, 1] respectively. It is noteworthy that with version of recursion, for the same reason, choice of the first element as pivot must be avoided.",
      "char_count": 458,
      "token_estimate": 114,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0021",
      "article_id": "quicksort_b43160f8",
      "section": "== = Worst-case analysis ==",
      "heading_path": "== = Worst-case analysis ==",
      "start_char": 17649,
      "end_char": 18439,
      "content": "== = Worst-case analysis === The most unbalanced partition occurs when one of the sublists returned by the partitioning routine is of size n − 1. This may occur if the pivot happens to be the smallest or largest element in the list, or in some implementations (e.g., the Lomuto partition scheme as described above) when all the elements are equal. If this happens repeatedly in every partition, then each recursive call processes a list of size one less than the previous list. Consequently, it takes n − 1 nested calls before to reach a list of size 1. This means that the call tree is a linear chain of n − 1 nested calls. The ith call does O(n − i) work to do the partition, and ∑ i = 0 n ( n − i ) = O ( n 2 ) {\\displaystyle \\textstyle \\sum _{i=0}^{n}(n-i)=O(n^{2})} , so in that case quicksort takes O(n2) time.",
      "char_count": 816,
      "token_estimate": 204,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0022",
      "article_id": "quicksort_b43160f8",
      "section": "== = Best-case analysis ==",
      "heading_path": "== = Best-case analysis ==",
      "start_char": 18465,
      "end_char": 19105,
      "content": "== = Best-case analysis === In the most balanced case, each partition divides the list into two nearly equal pieces. This means each recursive call processes a list of half the size. Consequently, only log2 n nested calls can be made before reaching a list of size 1. This means that the depth of the call tree is log2 n. But no two calls at the same level of the call tree process the same part of the original list; thus, each level of calls needs only O(n) time all together (each call has some constant overhead, but since there are only O(n) calls at each level, this is subsumed in the O(n) factor). The result is that the algorithm uses only O(n log n) time.",
      "char_count": 665,
      "token_estimate": 166,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "quicksort_b43160f8_c0023",
      "article_id": "quicksort_b43160f8",
      "section": "== = Average-case analysis ==",
      "heading_path": "== = Average-case analysis ==",
      "start_char": 19134,
      "end_char": 19762,
      "content": "== = Average-case analysis === To sort an array of n distinct elements, quicksort takes O(n log n) time in expectation, averaged over all n! permutations of n elements with equal probability. Alternatively, if the algorithm selects the pivot uniformly at random from the input array, the same analysis can be used to bound the expected running time for any input sequence; the expectation is then taken over the random choices made by the algorithm (Cormen et al., Introduction to Algorithms, Section 7.3). Three common proofs to this claim use percentiles, recurrences, and binary search trees, each providing different insights into quicksort's workings.",
      "char_count": 656,
      "token_estimate": 164,
      "token_start": null,
      "token_end": null
    }
  ],
  "questions": {
    "total_questions": 10,
    "items": [
      {
        "question": "What are the two main reasons Hoare's partition scheme is considered more efficient than Lomuto's scheme?",
        "answer": "Hoare's partition scheme is considered more efficient than Lomuto's for two main reasons: it performs three times fewer swaps on average, and it creates a balanced partition even when all values are equal, which Lomuto's scheme does not.",
        "related_chunk_ids": [
          "quicksort_b43160f8_c0016"
        ],
        "category": "INTERPRETATION"
      },
      {
        "question": "What factors can significantly affect the performance of the quicksort algorithm?",
        "answer": "The choice of the partition routine, which includes pivot selection, and other unspecified details can greatly affect the algorithm's performance, particularly for specific input arrays.",
        "related_chunk_ids": [
          "quicksort_b43160f8_c0007"
        ],
        "category": "FACTUAL"
      },
      {
        "question": "Who developed the quicksort algorithm and in what year?",
        "answer": "The quicksort algorithm was developed in 1959 by Tony Hoare.",
        "related_chunk_ids": [
          "quicksort_b43160f8_c0002"
        ],
        "category": "FACTUAL"
      },
      {
        "question": "Provide a summary of the Quicksort algorithm, covering its inventor, the historical context of its creation, and its performance characteristics.",
        "answer": "Quicksort was developed by British computer scientist Tony Hoare in 1959 while he was a visiting student at Moscow State University working on a machine translation project. It is a divide-and-conquer algorithm that works by selecting a 'pivot' element and partitioning other elements around it. In terms of performance, its mathematical analysis shows an average time complexity of O(n log n) comparisons, but in the worst-case scenario, it makes O(n^2) comparisons.",
        "related_chunk_ids": [
          "quicksort_b43160f8_c0000",
          "quicksort_b43160f8_c0001",
          "quicksort_b43160f8_c0002"
        ],
        "category": "LONG_ANSWER"
      },
      {
        "question": "How have different partitioning schemes for the Quicksort algorithm been developed and compared over time?",
        "answer": "Quicksort is a divide-and-conquer algorithm based on a partitioning routine. The original algorithm published by C. A. R. Hoare in 1961 included a partition method. Later, another scheme attributed to Nico Lomuto was popularized by Jon Bentley, who found it simpler to understand and prove correct. Despite its simplicity, Lomuto's partition scheme is considered inferior to Hoare's because it performs three times more swaps on average and degrades to O(n²) runtime when all elements are equal.",
        "related_chunk_ids": [
          "quicksort_b43160f8_c0003",
          "quicksort_b43160f8_c0004",
          "quicksort_b43160f8_c0005"
        ],
        "category": "INTERPRETATION"
      },
      {
        "question": "How does the Lomuto partition scheme implement the pivot selection and partitioning steps of the general quicksort algorithm?",
        "answer": "The general quicksort algorithm requires picking a pivot value and then partitioning the array so that elements with values less than the pivot come before it, and elements with values greater come after. The Lomuto partition scheme implements this by typically selecting the last element in the array as the pivot. It then partitions the array by maintaining an index `i` while scanning with another index `j`, ensuring that elements from the start of the range up to `i-1` are less than the pivot, and elements from `i` to `j` are equal to or greater than the pivot.",
        "related_chunk_ids": [
          "quicksort_b43160f8_c0006",
          "quicksort_b43160f8_c0007",
          "quicksort_b43160f8_c0008"
        ],
        "category": "INTERPRETATION"
      },
      {
        "question": "Provide a summary of the Quicksort algorithm, including its operational method, key properties, and performance analysis.",
        "answer": "Quicksort is a divide-and-conquer sorting algorithm that works by selecting a 'pivot' element and partitioning other elements into sub-arrays based on whether they are less or greater than the pivot, which are then sorted recursively. It is a comparison sort, meaning it can sort any items for which a 'less-than' relation is defined. While efficient on average with a time complexity of O(n log n), its worst-case performance is O(n^2). Most implementations of Quicksort are not stable, so the relative order of equal items may not be preserved.",
        "related_chunk_ids": [
          "quicksort_b43160f8_c0000",
          "quicksort_b43160f8_c0001"
        ],
        "category": "LONG_ANSWER"
      },
      {
        "question": "Trace the development of the Quicksort algorithm, from its initial conception by Tony Hoare to the later improvements and alternative schemes introduced by others.",
        "answer": "The Quicksort algorithm was developed by Tony Hoare in 1959 for a machine translation project. He initially wrote the partition part in Mercury Autocode but struggled with the implementation until he learned about ALGOL and its support for recursion, which enabled him to publish an improved version in 1961. The algorithm gained widespread adoption, for instance, as the default sort in Unix. Later, Robert Sedgewick's 1975 PhD thesis was a milestone in its analysis. In 1993, Jon Bentley and Doug McIlroy introduced further improvements, including a method for handling equal elements and a pivot scheme called 'pseudomedian of nine'. Another partitioning scheme by Nico Lomuto was also popularized; although less efficient than Hoare's original, it was favored by some, like Bentley, for its simplicity and ease of understanding.",
        "related_chunk_ids": [
          "quicksort_b43160f8_c0002",
          "quicksort_b43160f8_c0003",
          "quicksort_b43160f8_c0004"
        ],
        "category": "LONG_ANSWER"
      },
      {
        "question": "Summarize the Quicksort algorithm, explaining its divide-and-conquer strategy and the key steps involved in sorting an array.",
        "answer": "Quicksort is a divide-and-conquer algorithm that sorts an array using a partitioning routine. The process begins by selecting a pivot value from the array range. The array is then partitioned by reordering its elements so that all values less than the pivot come before it, and all values greater than the pivot come after it. This partitioning divides the array into two sub-ranges and typically places the pivot element in its final sorted position. Finally, the quicksort algorithm is recursively applied to these two smaller sub-ranges until the entire array is sorted.",
        "related_chunk_ids": [
          "quicksort_b43160f8_c0005",
          "quicksort_b43160f8_c0006",
          "quicksort_b43160f8_c0007"
        ],
        "category": "LONG_ANSWER"
      },
      {
        "question": "Explain the complete process of sorting an array using the Quicksort algorithm with the Lomuto partition scheme, from the initial call to the recursive sorting of sub-arrays, based on the provided description and pseudocode.",
        "answer": "The entire array is sorted by making an initial call to `quicksort(A, 0, length(A) - 1)`. The `quicksort` algorithm first checks if the indices are in the correct order and then proceeds to partition the array by calling `p := partition(A, lo, hi)`. The Lomuto partition scheme, as implemented in the `partition` function, chooses the last element of the array segment (`A[hi]`) as the pivot. It then iterates through the array, maintaining an index `i`, to ensure elements from `lo` to `i-1` are less than the pivot, while elements from `i` to the current position `j` are greater than or equal to it. After the loop, the pivot is swapped into its final sorted position at index `i`, which is then returned. Finally, the `quicksort` algorithm makes two recursive calls to sort the partitions on either side of the pivot: `quicksort(A, lo, p - 1)` for the left side and `quicksort(A, p + 1, hi)` for the right side.",
        "related_chunk_ids": [
          "quicksort_b43160f8_c0008",
          "quicksort_b43160f8_c0009",
          "quicksort_b43160f8_c0010"
        ],
        "category": "LONG_ANSWER"
      }
    ]
  },
  "metadata": {
    "export_date": "2025-07-30T10:37:32.045Z",
    "content_format": "markdown",
    "total_chunks": 24,
    "description": "Complete article dataset including content, chunks, and generated questions"
  }
}