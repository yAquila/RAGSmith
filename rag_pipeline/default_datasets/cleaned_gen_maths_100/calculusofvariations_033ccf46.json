{
  "article": {
    "id": "calculusofvariations_033ccf46",
    "title": "Calculus of variations",
    "url": "https://en.wikipedia.org/wiki/Calculus_of_variations",
    "lang": "en",
    "created_at": "2025-07-31T06:35:34.574626",
    "content": "---\nid: calculusofvariations_033ccf46\nurl: https://en.wikipedia.org/wiki/Calculus_of_variations\ntitle: Calculus of variations\nlang: en\ncreated_at: '2025-07-31T06:27:24.854712'\nchecksum: 7ac1c0aa2b1343304cd8dd738655b8c27fdafac2c6deb4b6cb0337e28cdce424\noptions:\n  chunk_size: 1000\n  chunk_overlap: 200\n  split_strategy: header_aware\n  total_questions: 10\n  llm_model: gemini-2.5-pro\nstats:\n  word_count: 5477\n  char_count: 31898\n  num_chunks: 33\n  original_chunks: 42\n  filtered_out: 9\n  num_sections: 0\n---\nThe calculus of variations (or variational calculus) is a field of mathematical analysis that uses variations, which are small changes in functions and functionals, to find maxima and minima of functionals: mappings from a set of functions to the real numbers. Functionals are often expressed as definite integrals involving functions and their derivatives. Functions that maximize or minimize functionals may be found using the Euler–Lagrange equation of the calculus of variations. A simple example of such a problem is to find the curve of shortest length connecting two points. If there are no constraints, the solution is a straight line between the points. However, if the curve is constrained to lie on a surface in space, then the solution is less obvious, and possibly many solutions may exist. Such solutions are known as geodesics. A related problem is posed by Fermat's principle: light follows the path of shortest optical length connecting two points, which depends upon the material of the medium. One corresponding concept in mechanics is the principle of least/stationary action. Many important problems involve functions of several variables. Solutions of boundary value problems for the Laplace equation satisfy the Dirichlet's principle. Plateau's problem requires finding a surface of minimal area that spans a given contour in space: a solution can often be found by dipping a frame in soapy water. Although such experiments are relatively easy to perform, their mathematical formulation is far from simple: there may be more than one locally minimizing surface, and they may have non-trivial topology. == History == The calculus of variations began with the work of Isaac Newton, such as with Newton's minimal resistance problem, which he formulated and solved in 1685, and later published in his Principia in 1687, which was the first problem in the field to be formulated and correctly solved, and was also one of the most difficult problems tackled by variational methods prior to the twentieth century. This problem was followed by the brachistochrone curve problem raised by Johann Bernoulli (1696), which was similar to one raised by Galileo Galilei in 1638, but he did not solve the problem explicity nor did he use the methods based on calculus. Bernoulli had solved the problem, using the principle of least time in the process, but not calculus of variations, whereas Newton did to solve the problem in 1697, and as a result, he pioneered the field with his work on the two problems. The problem would immediately occupy the attention of Jacob Bernoulli and the Marquis de l'Hôpital, but Leonhard Euler first elaborated the subject, beginning in 1733. Joseph-Louis Lagrange was influenced by Euler's work to contribute greatly to the theory. After Euler saw the 1755 work of the 19-year-old Lagrange, Euler dropped his own partly geometric approach in favor of Lagrange's purely analytic approach and renamed the subject the calculus of variations in his 1756 lecture Elementa Calculi Variationum. Adrien-Marie Legendre (1786) laid down a method, not entirely satisfactory, for the discrimination of maxima and minima. Isaac Newton and Gottfried Leibniz also gave some early attention to the subject. To this discrimination Vincenzo Brunacci (1810), Carl Friedrich Gauss (1829), Siméon Poisson (1831), Mikhail Ostrogradsky (1834), and Carl Jacobi (1837) have been among the contributors. An important general work is that of Pierre Frédéric Sarrus (1842) which was condensed and improved by Augustin-Louis Cauchy (1844). Other valuable treatises and memoirs have been written by Strauch (1849), John Hewitt Jellett (1850), Otto Hesse (1857), Alfred Clebsch (1858), and Lewis Buffett Carll (1885), but perhaps the most important work of the century is that of Karl Weierstrass. His celebrated course on the theory is epoch-making, and it may be asserted that he was the first to place it on a firm and unquestionable foundation. The 20th and the 23rd Hilbert problem published in 1900 encouraged further development. In the 20th century David Hilbert, Oskar Bolza, Gilbert Ames Bliss, Emmy Noether, Leonida Tonelli, Henri Lebesgue and Jacques Hadamard among others made significant contributions. Marston Morse applied calculus of variations in what is now called Morse theory. Lev Pontryagin, Ralph Rockafellar and F. H. Clarke developed new mathematical tools for the calculus of variations in optimal control theory. The dynamic programming of Richard Bellman is an alternative to the calculus of variations. == Extrema == The calculus of variations is concerned with the maxima or minima (collectively called extrema) of functionals. A functional maps functions to scalars, so functionals have been described as \"functions of functions.\" Functionals have extrema with respect to the elements y {\\displaystyle y} of a given function space defined over a given domain. A functional J [ y ] {\\displaystyle J[y]} is said to have an extremum at the function f {\\displaystyle f} if Δ J = J [ y ] − J [ f ] {\\displaystyle \\Delta J=J[y]-J[f]} has the same sign for all y {\\displaystyle y} in an arbitrarily small neighborhood of f . {\\displaystyle f.} The function f {\\displaystyle f} is called an extremal function or extremal. The extremum J [ f ] {\\displaystyle J[f]} is called a local maximum if Δ J ≤ 0 {\\displaystyle \\Delta J\\leq 0} everywhere in an arbitrarily small neighborhood of f , {\\displaystyle f,} and a local minimum if Δ J ≥ 0 {\\displaystyle \\Delta J\\geq 0} there. For a function space of continuous functions, extrema of corresponding functionals are called strong extrema or weak extrema, depending on whether the first derivatives of the continuous functions are respectively all continuous or not. Both strong and weak extrema of functionals are for a space of continuous functions but strong extrema have the additional requirement that the first derivatives of the functions in the space be continuous. Thus a strong extremum is also a weak extremum, but the converse may not hold. Finding strong extrema is more difficult than finding weak extrema. An example of a necessary condition that is used for finding weak extrema is the Euler–Lagrange equation. == Euler–Lagrange equation == Finding the extrema of functionals is similar to finding the maxima and minima of functions. The maxima and minima of a function may be located by finding the points where its derivative vanishes (i.e., is equal to zero). The extrema of functionals may be obtained by finding functions for which the functional derivative is equal to zero. This leads to solving the associated Euler–Lagrange equation. Consider the functional J [ y ] = ∫ x 1 x 2 L ( x , y ( x ) , y ′ ( x ) ) d x , {\\displaystyle J[y]=\\int _{x_{1}}^{x_{2}}L\\left(x,y(x),y'(x)\\right)\\,dx,} where x 1 , x 2 {\\displaystyle x_{1},x_{2}} are constants, y ( x ) {\\displaystyle y(x)} is twice continuously differentiable, y ′ ( x ) = d y d x , {\\displaystyle y'(x)={\\frac {dy}{dx}},} L ( x , y ( x ) , y ′ ( x ) ) {\\displaystyle L\\left(x,y(x),y'(x)\\right)} is twice continuously differentiable with respect to its arguments x , y , {\\displaystyle x,y,} and y ′ . {\\displaystyle y'.} If the functional J [ y ] {\\displaystyle J[y]} attains a local minimum at f , {\\displaystyle f,} and η ( x ) {\\displaystyle \\eta (x)} is an arbitrary function that has at least one derivative and vanishes at the endpoints x 1 {\\displaystyle x_{1}} and x 2 , {\\displaystyle x_{2},} then for any number ε {\\displaystyle \\varepsilon } close to 0, J [ f ] ≤ J [ f + ε η ] . {\\displaystyle J[f]\\leq J[f+\\varepsilon \\eta ]\\,.} The term ε η {\\displaystyle \\varepsilon \\eta } is called the variation of the function f {\\displaystyle f} and is denoted by δ f . {\\displaystyle \\delta f.} Substituting f + ε η {\\displaystyle f+\\varepsilon \\eta } for y {\\displaystyle y} in the functional J [ y ] , {\\displaystyle J[y],} the result is a function of ε , {\\displaystyle \\varepsilon ,} Φ ( ε ) = J [ f + ε η ] . {\\displaystyle \\Phi (\\varepsilon )=J[f+\\varepsilon \\eta ]\\,.} Since the functional J [ y ] {\\displaystyle J[y]} has a minimum for y = f {\\displaystyle y=f} the function Φ ( ε ) {\\displaystyle \\Phi (\\varepsilon )} has a minimum at ε = 0 {\\displaystyle \\varepsilon =0} and thus, Φ ′ ( 0 ) ≡ d Φ d ε | ε = 0 = ∫ x 1 x 2 d L d ε | ε = 0 d x = 0 . {\\displaystyle \\Phi '(0)\\equiv \\left.{\\frac {d\\Phi }{d\\varepsilon }}\\right|_{\\varepsilon =0}=\\int _{x_{1}}^{x_{2}}\\left.{\\frac {dL}{d\\varepsilon }}\\right|_{\\varepsilon =0}dx=0\\,.} Taking the total derivative of L [ x , y , y ′ ] , {\\displaystyle L\\left[x,y,y'\\right],} where y = f + ε η {\\displaystyle y=f+\\varepsilon \\eta } and y ′ = f ′ + ε η ′ {\\displaystyle y'=f'+\\varepsilon \\eta '} are considered as functions of ε {\\displaystyle \\varepsilon } rather than x , {\\displaystyle x,} yields d L d ε = ∂ L ∂ y d y d ε + ∂ L ∂ y ′ d y ′ d ε {\\displaystyle {\\frac {dL}{d\\varepsilon }}={\\frac {\\partial L}{\\partial y}}{\\frac {dy}{d\\varepsilon }}+{\\frac {\\partial L}{\\partial y'}}{\\frac {dy'}{d\\varepsilon }}} and because d y d ε = η {\\displaystyle {\\frac {dy}{d\\varepsilon }}=\\eta } and d y ′ d ε = η ′ , {\\displaystyle {\\frac {dy'}{d\\varepsilon }}=\\eta ',} d L d ε = ∂ L ∂ y η + ∂ L ∂ y ′ η ′ . {\\displaystyle {\\frac {dL}{d\\varepsilon }}={\\frac {\\partial L}{\\partial y}}\\eta +{\\frac {\\partial L}{\\partial y'}}\\eta '.} Therefore, ∫ x 1 x 2 d L d ε | ε = 0 d x = ∫ x 1 x 2 ( ∂ L ∂ f η + ∂ L ∂ f ′ η ′ ) d x = ∫ x 1 x 2 ∂ L ∂ f η d x + ∂ L ∂ f ′ η | x 1 x 2 − ∫ x 1 x 2 η d d x ∂ L ∂ f ′ d x = ∫ x 1 x 2 ( ∂ L ∂ f η − η d d x ∂ L ∂ f ′ ) d x {\\displaystyle {\\begin{aligned}\\int _{x_{1}}^{x_{2}}\\left.{\\frac {dL}{d\\varepsilon }}\\right|_{\\varepsilon =0}dx&=\\int _{x_{1}}^{x_{2}}\\left({\\frac {\\partial L}{\\partial f}}\\eta +{\\frac {\\partial L}{\\partial f'}}\\eta '\\right)\\,dx\\\\&=\\int _{x_{1}}^{x_{2}}{\\frac {\\partial L}{\\partial f}}\\eta \\,dx+\\left.{\\frac {\\partial L}{\\partial f'}}\\eta \\right|_{x_{1}}^{x_{2}}-\\int _{x_{1}}^{x_{2}}\\eta {\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}\\,dx\\\\&=\\int _{x_{1}}^{x_{2}}\\left({\\frac {\\partial L}{\\partial f}}\\eta -\\eta {\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}\\right)\\,dx\\\\\\end{aligned}}} where L [ x , y , y ′ ] → L [ x , f , f ′ ] {\\displaystyle L\\left[x,y,y'\\right]\\to L\\left[x,f,f'\\right]} when ε = 0 {\\displaystyle \\varepsilon =0} and we have used integration by parts on the second term. The second term on the second line vanishes because η = 0 {\\displaystyle \\eta =0} at x 1 {\\displaystyle x_{1}} and x 2 {\\displaystyle x_{2}} by definition. Also, as previously mentioned the left side of the equation is zero so that ∫ x 1 x 2 η ( x ) ( ∂ L ∂ f − d d x ∂ L ∂ f ′ ) d x = 0 . {\\displaystyle \\int _{x_{1}}^{x_{2}}\\eta (x)\\left({\\frac {\\partial L}{\\partial f}}-{\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}\\right)\\,dx=0\\,.} According to the fundamental lemma of calculus of variations, the part of the integrand in parentheses is zero, i.e. ∂ L ∂ f − d d x ∂ L ∂ f ′ = 0 {\\displaystyle {\\frac {\\partial L}{\\partial f}}-{\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}=0} which is called the Euler–Lagrange equation. The left hand side of this equation is called the functional derivative of J [ f ] {\\displaystyle J[f]} and is denoted δ J {\\displaystyle \\delta J} or δ f ( x ) . {\\displaystyle \\delta f(x).} In general this gives a second-order ordinary differential equation which can be solved to obtain the extremal function f ( x ) . {\\displaystyle f(x).} The Euler–Lagrange equation is a necessary, but not sufficient, condition for an extremum J [ f ] . {\\displaystyle J[f].} A sufficient condition for a minimum is given in the section Variations and sufficient condition for a minimum. === Example === In order to illustrate this process, consider the problem of finding the extremal function y = f ( x ) , {\\displaystyle y=f(x),} which is the shortest curve that connects two points ( x 1 , y 1 ) {\\displaystyle \\left(x_{1},y_{1}\\right)} and ( x 2 , y 2 ) . {\\displaystyle \\left(x_{2},y_{2}\\right).} The arc length of the curve is given by A [ y ] = ∫ x 1 x 2 1 + [ y ′ ( x ) ] 2 d x , {\\displaystyle A[y]=\\int _{x_{1}}^{x_{2}}{\\sqrt {1+[y'(x)]^{2}}}\\,dx\\,,} with y ′ ( x ) = d y d x , y 1 = f ( x 1 ) , y 2 = f ( x 2 ) . {\\displaystyle y'(x)={\\frac {dy}{dx}}\\,,\\ \\ y_{1}=f(x_{1})\\,,\\ \\ y_{2}=f(x_{2})\\,.} Note that assuming y is a function of x loses generality; ideally both should be a function of some other parameter. This approach is good solely for instructive purposes. The Euler–Lagrange equation will now be used to find the extremal function f ( x ) {\\displaystyle f(x)} that minimizes the functional A [ y ] . {\\displaystyle A[y].} ∂ L ∂ f − d d x ∂ L ∂ f ′ = 0 {\\displaystyle {\\frac {\\partial L}{\\partial f}}-{\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}=0} with L = 1 + [ f ′ ( x ) ] 2 . {\\displaystyle L={\\sqrt {1+[f'(x)]^{2}}}\\,.} Since f {\\displaystyle f} does not appear explicitly in L , {\\displaystyle L,} the first term in the Euler–Lagrange equation vanishes for all f ( x ) {\\displaystyle f(x)} and thus, d d x ∂ L ∂ f ′ = 0 . {\\displaystyle {\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}=0\\,.} Substituting for L {\\displaystyle L} and taking the derivative, d d x f ′ ( x ) 1 + [ f ′ ( x ) ] 2 = 0 . {\\displaystyle {\\frac {d}{dx}}\\ {\\frac {f'(x)}{\\sqrt {1+[f'(x)]^{2}}}}\\ =0\\,.} Thus f ′ ( x ) 1 + [ f ′ ( x ) ] 2 = c , {\\displaystyle {\\frac {f'(x)}{\\sqrt {1+[f'(x)]^{2}}}}=c\\,,} for some constant c {\\displaystyle c} . Then [ f ′ ( x ) ] 2 1 + [ f ′ ( x ) ] 2 = c 2 , {\\displaystyle {\\frac {[f'(x)]^{2}}{1+[f'(x)]^{2}}}=c^{2}\\,,} where 0 ≤ c 2 < 1. {\\displaystyle 0\\leq c^{2}<1.} Solving, we get [ f ′ ( x ) ] 2 = c 2 1 − c 2 {\\displaystyle [f'(x)]^{2}={\\frac {c^{2}}{1-c^{2}}}} which implies that f ′ ( x ) = m {\\displaystyle f'(x)=m} is a constant and therefore that the shortest curve that connects two points ( x 1 , y 1 ) {\\displaystyle \\left(x_{1},y_{1}\\right)} and ( x 2 , y 2 ) {\\displaystyle \\left(x_{2},y_{2}\\right)} is f ( x ) = m x + b with m = y 2 − y 1 x 2 − x 1 and b = x 2 y 1 − x 1 y 2 x 2 − x 1 {\\displaystyle f(x)=mx+b\\qquad {\\text{with}}\\ \\ m={\\frac {y_{2}-y_{1}}{x_{2}-x_{1}}}\\quad {\\text{and}}\\quad b={\\frac {x_{2}y_{1}-x_{1}y_{2}}{x_{2}-x_{1}}}} and we have thus found the extremal function f ( x ) {\\displaystyle f(x)} that minimizes the functional A [ y ] {\\displaystyle A[y]} so that A [ f ] {\\displaystyle A[f]} is a minimum. The equation for a straight line is y = m x + b . {\\displaystyle y=mx+b.} In other words, the shortest distance between two points is a straight line. == Beltrami's identity == In physics problems it may be the case that ∂ L ∂ x = 0 , {\\displaystyle {\\frac {\\partial L}{\\partial x}}=0,} meaning the integrand is a function of f ( x ) {\\displaystyle f(x)} and f ′ ( x ) {\\displaystyle f'(x)} but x {\\displaystyle x} does not appear separately. In that case, the Euler–Lagrange equation can be simplified to the Beltrami identity L − f ′ ∂ L ∂ f ′ = C , {\\displaystyle L-f'{\\frac {\\partial L}{\\partial f'}}=C\\,,} where C {\\displaystyle C} is a constant. The left hand side is the Legendre transformation of L {\\displaystyle L} with respect to f ′ ( x ) . {\\displaystyle f'(x).} The intuition behind this result is that, if the variable x {\\displaystyle x} is actually time, then the statement ∂ L ∂ x = 0 {\\displaystyle {\\frac {\\partial L}{\\partial x}}=0} implies that the Lagrangian is time-independent. By Noether's theorem, there is an associated conserved quantity. In this case, this quantity is the Hamiltonian, the Legendre transform of the Lagrangian, which (often) coincides with the energy of the system. This is (minus) the constant in Beltrami's identity. == Euler–Poisson equation == If S {\\displaystyle S} depends on higher-derivatives of y ( x ) {\\displaystyle y(x)} , that is, if S = ∫ a b f ( x , y ( x ) , y ′ ( x ) , … , y ( n ) ( x ) ) d x , {\\displaystyle S=\\int _{a}^{b}f(x,y(x),y'(x),\\dots ,y^{(n)}(x))dx,} then y {\\displaystyle y} must satisfy the Euler–Poisson equation, ∂ f ∂ y − d d x ( ∂ f ∂ y ′ ) + ⋯ + ( − 1 ) n d n d x n [ ∂ f ∂ y ( n ) ] = 0. {\\displaystyle {\\frac {\\partial f}{\\partial y}}-{\\frac {d}{dx}}\\left({\\frac {\\partial f}{\\partial y'}}\\right)+\\dots +(-1)^{n}{\\frac {d^{n}}{dx^{n}}}\\left[{\\frac {\\partial f}{\\partial y^{(n)}}}\\right]=0.} == Du Bois-Reymond's theorem == The discussion thus far has assumed that extremal functions possess two continuous derivatives, although the existence of the integral J {\\displaystyle J} requires only first derivatives of trial functions. The condition that the first variation vanishes at an extremal may be regarded as a weak form of the Euler–Lagrange equation. The theorem of Du Bois-Reymond asserts that this weak form implies the strong form. If L {\\displaystyle L} has continuous first and second derivatives with respect to all of its arguments, and if ∂ 2 L ∂ f ′ 2 ≠ 0 , {\\displaystyle {\\frac {\\partial ^{2}L}{\\partial f'^{2}}}\\neq 0,} then f {\\displaystyle f} has two continuous derivatives, and it satisfies the Euler–Lagrange equation. == Lavrentiev phenomenon == Hilbert was the first to give good conditions for the Euler–Lagrange equations to give a stationary solution. Within a convex area and a positive thrice differentiable Lagrangian the solutions are composed of a countable collection of sections that either go along the boundary or satisfy the Euler–Lagrange equations in the interior. However Lavrentiev in 1926 showed that there are circumstances where there is no optimum solution but one can be approached arbitrarily closely by increasing numbers of sections. The Lavrentiev Phenomenon identifies a difference in the infimum of a minimization problem across different classes of admissible functions. For instance the following problem, presented by Manià in 1934: L [ x ] = ∫ 0 1 ( x 3 − t ) 2 x ′ 6 , {\\displaystyle L[x]=\\int _{0}^{1}(x^{3}-t)^{2}x'^{6},} A = { x ∈ W 1 , 1 ( 0 , 1 ) : x ( 0 ) = 0 , x ( 1 ) = 1 } . {\\displaystyle {A}=\\{x\\in W^{1,1}(0,1):x(0)=0,\\ x(1)=1\\}.} Clearly, x ( t ) = t 1 3 {\\displaystyle x(t)=t^{\\frac {1}{3}}} minimizes the functional, but we find any function x ∈ W 1 , ∞ {\\displaystyle x\\in W^{1,\\infty }} gives a value bounded away from the infimum. Examples (in one-dimension) are traditionally manifested across W 1 , 1 {\\displaystyle W^{1,1}} and W 1 , ∞ , {\\displaystyle W^{1,\\infty },} but Ball and Mizel procured the first functional that displayed Lavrentiev's Phenomenon across W 1 , p {\\displaystyle W^{1,p}} and W 1 , q {\\displaystyle W^{1,q}} for 1 ≤ p < q < ∞ . {\\displaystyle 1\\leq p 0 , {\\displaystyle n(x,y)={\\begin{cases}n_{(-)}&{\\text{if}}\\quad x<0,\\\\n_{(+)}&{\\text{if}}\\quad x>0,\\end{cases}}} where n ( − ) {\\displaystyle n_{(-)}} and n ( + ) {\\displaystyle n_{(+)}} are constants. Then the Euler–Lagrange equation holds as before in the region where x < 0 {\\displaystyle x<0} or x > 0 {\\displaystyle x>0} , and in fact the path is a straight line there, since the refractive index is constant. At the x = 0 {\\displaystyle x=0} , f {\\displaystyle f} must be continuous, but f ′ {\\displaystyle f'} may be discontinuous. After integration by parts in the separate regions and using the Euler–Lagrange equations, the first variation takes the form δ A [ f 0 , f 1 ] = f 1 ( 0 ) [ n ( − ) f 0 ′ ( 0 − ) 1 + f 0 ′ ( 0 − ) 2 − n ( + ) f 0 ′ ( 0 + ) 1 + f 0 ′ ( 0 + ) 2 ] . {\\displaystyle \\delta A[f_{0},f_{1}]=f_{1}(0)\\left[n_{(-)}{\\frac {f_{0}'(0^{-})}{\\sqrt {1+f_{0}'(0^{-})^{2}}}}-n_{(+)}{\\frac {f_{0}'(0^{+})}{\\sqrt {1+f_{0}'(0^{+})^{2}}}}\\right].} The factor multiplying n ( − ) {\\displaystyle n_{(-)}} is the sine of angle of the incident ray with the x {\\displaystyle x} axis, and the factor multiplying n ( + ) {\\displaystyle n_{(+)}} is the sine of angle of the refracted ray with the x {\\displaystyle x} axis. Snell's law for refraction requires that these terms be equal. As this calculation demonstrates, Snell's law is equivalent to vanishing of the first variation of the optical path length. ==== Fermat's principle in three dimensions ==== It is expedient to use vector notation: let X = ( x 1 , x 2 , x 3 ) , {\\displaystyle X=(x_{1},x_{2},x_{3}),} let t {\\displaystyle t} be a parameter, let X ( t ) {\\displaystyle X(t)} be the parametric representation of a curve C , {\\displaystyle C,} and let X ˙ ( t ) {\\displaystyle {\\dot {X}}(t)} be its tangent vector. The optical length of the curve is given by A [ C ] = ∫ t 0 t 1 n ( X ) X ˙ ⋅ X ˙ d t . {\\displaystyle A[C]=\\int _{t_{0}}^{t_{1}}n(X){\\sqrt {{\\dot {X}}\\cdot {\\dot {X}}}}\\,dt.} Note that this integral is invariant with respect to changes in the parametric representation of C . {\\displaystyle C.} The Euler–Lagrange equations for a minimizing curve have the symmetric form d d t P = X ˙ ⋅ X ˙ ∇ n , {\\displaystyle {\\frac {d}{dt}}P={\\sqrt {{\\dot {X}}\\cdot {\\dot {X}}}}\\,\\nabla n,} where P = n ( X ) X ˙ X ˙ ⋅ X ˙ . {\\displaystyle P={\\frac {n(X){\\dot {X}}}{\\sqrt {{\\dot {X}}\\cdot {\\dot {X}}}}}.} It follows from the definition that P {\\displaystyle P} satisfies P ⋅ P = n ( X ) 2 . {\\displaystyle P\\cdot P=n(X)^{2}.} Therefore, the integral may also be written as A [ C ] = ∫ t 0 t 1 P ⋅ X ˙ d t . {\\displaystyle A[C]=\\int _{t_{0}}^{t_{1}}P\\cdot {\\dot {X}}\\,dt.} This form suggests that if we can find a function ψ {\\displaystyle \\psi } whose gradient is given by P , {\\displaystyle P,} then the integral A {\\displaystyle A} is given by the difference of ψ {\\displaystyle \\psi } at the endpoints of the interval of integration. Thus the problem of studying the curves that make the integral stationary can be related to the study of the level surfaces of ψ {\\displaystyle \\psi } . In order to find such a function, we turn to the wave equation, which governs the propagation of light. This formalism is used in the context of Lagrangian optics and Hamiltonian optics. ===== Connection with the wave equation ===== The wave equation for an inhomogeneous medium is u t t = c 2 ∇ ⋅ ∇ u , {\\displaystyle u_{tt}=c^{2}\\nabla \\cdot \\nabla u,} where c {\\displaystyle c} is the velocity, which generally depends upon X {\\displaystyle X} . Wave fronts for light are characteristic surfaces for this partial differential equation: they satisfy φ t 2 = c ( X ) 2 ∇ φ ⋅ ∇ φ . {\\displaystyle \\varphi _{t}^{2}=c(X)^{2}\\,\\nabla \\varphi \\cdot \\nabla \\varphi .} We may look for solutions in the form φ ( t , X ) = t − ψ ( X ) . {\\displaystyle \\varphi (t,X)=t-\\psi (X).} In that case, ψ {\\displaystyle \\psi } satisfies ∇ ψ ⋅ ∇ ψ = n 2 , {\\displaystyle \\nabla \\psi \\cdot \\nabla \\psi =n^{2},} where n = 1 / c {\\displaystyle n=1/c} . According to the theory of first-order partial differential equations, if P = ∇ ψ , {\\displaystyle P=\\nabla \\psi ,} then P {\\displaystyle P} satisfies d P d s = n ∇ n , {\\displaystyle {\\frac {dP}{ds}}=n\\,\\nabla n,} along a system of curves (the light rays) that are given by d X d s = P . {\\displaystyle {\\frac {dX}{ds}}=P.} These equations for solution of a first-order partial differential equation are identical to the Euler–Lagrange equations if we make the identification d s d t = X ˙ ⋅ X ˙ n . {\\displaystyle {\\frac {ds}{dt}}={\\frac {\\sqrt {{\\dot {X}}\\cdot {\\dot {X}}}}{n}}.} We conclude that the function ψ {\\displaystyle \\psi } is the value of the minimizing integral A {\\displaystyle A} as a function of the upper end point. That is, when a family of minimizing curves is constructed, the values of the optical length satisfy the characteristic equation corresponding the wave equation. Hence, solving the associated partial differential equation of first order is equivalent to finding families of solutions of the variational problem. This is the essential content of the Hamilton–Jacobi theory, which applies to more general variational problems. === Mechanics === In classical mechanics, the action, S , {\\displaystyle S,} is defined as the time integral of the Lagrangian, L {\\displaystyle L} . The Lagrangian is the difference of energies, L = T − U , {\\displaystyle L=T-U,} where T {\\displaystyle T} is the kinetic energy of a mechanical system and U {\\displaystyle U} its potential energy. Hamilton's principle (or the action principle) states that the motion of a conservative holonomic (integrable constraints) mechanical system is such that the action integral S = ∫ t 0 t 1 L ( x , x ˙ , t ) d t {\\displaystyle S=\\int _{t_{0}}^{t_{1}}L(x,{\\dot {x}},t)\\,dt} is stationary with respect to variations in the path x ( t ) {\\displaystyle x(t)} . The Euler–Lagrange equations for this system are known as Lagrange's equations: d d t ∂ L ∂ x ˙ = ∂ L ∂ x , {\\displaystyle {\\frac {d}{dt}}{\\frac {\\partial L}{\\partial {\\dot {x}}}}={\\frac {\\partial L}{\\partial x}},} and they are equivalent to Newton's equations of motion (for such systems). The conjugate momenta P {\\displaystyle P} are defined by p = ∂ L ∂ x ˙ . {\\displaystyle p={\\frac {\\partial L}{\\partial {\\dot {x}}}}.} For example, if T = 1 2 m x ˙ 2 , {\\displaystyle T={\\frac {1}{2}}m{\\dot {x}}^{2},} then p = m x ˙ . {\\displaystyle p=m{\\dot {x}}.} Hamiltonian mechanics results if the conjugate momenta are introduced in place of x ˙ {\\displaystyle {\\dot {x}}} by a Legendre transformation of the Lagrangian L {\\displaystyle L} into the Hamiltonian H {\\displaystyle H} defined by H ( x , p , t ) = p x ˙ − L ( x , x ˙ , t ) . {\\displaystyle H(x,p,t)=p\\,{\\dot {x}}-L(x,{\\dot {x}},t).} The Hamiltonian is the total energy of the system: H = T + U {\\displaystyle H=T+U} . Analogy with Fermat's principle suggests that solutions of Lagrange's equations (the particle trajectories) may be described in terms of level surfaces of some function of X {\\displaystyle X} . This function is a solution of the Hamilton–Jacobi equation: ∂ ψ ∂ t + H ( x , ∂ ψ ∂ x , t ) = 0. {\\displaystyle {\\frac {\\partial \\psi }{\\partial t}}+H\\left(x,{\\frac {\\partial \\psi }{\\partial x}},t\\right)=0.} === Further applications === Further applications of the calculus of variations include the following: The derivation of the catenary shape Solution to Newton's minimal resistance problem Solution to the brachistochrone problem Solution to the tautochrone problem Solution to isoperimetric problems Calculating geodesics Finding minimal surfaces and solving Plateau's problem Optimal control Analytical mechanics, or reformulations of Newton's laws of motion, most notably Lagrangian and Hamiltonian mechanics; Geometric optics, especially Lagrangian and Hamiltonian optics; Variational method (quantum mechanics), one way of finding approximations to the lowest energy eigenstate or ground state, and some excited states; Variational Bayesian methods, a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning; Variational methods in general relativity, a family of techniques using calculus of variations to solve problems in Einstein's general theory of relativity; Finite element method is a variational method for finding numerical solutions to boundary-value problems in differential equations; Total variation denoising, an image processing method for filtering high variance or noisy signals. == Variations and sufficient condition for a minimum == Calculus of variations is concerned with variations of functionals, which are small changes in the functional's value due to small changes in the function that is its argument. The first variation is defined as the linear part of the change in the functional, and the second variation is defined as the quadratic part. For example, if J [ y ] {\\displaystyle J[y]} is a functional with the function y = y ( x ) {\\displaystyle y=y(x)} as its argument, and there is a small change in its argument from y {\\displaystyle y} to y + h , {\\displaystyle y+h,} where h = h ( x ) {\\displaystyle h=h(x)} is a function in the same function space as y {\\displaystyle y} , then the corresponding change in the functional is Δ J [ h ] = J [ y + h ] − J [ y ] . {\\displaystyle \\Delta J[h]=J[y+h]-J[y].} The functional J [ y ] {\\displaystyle J[y]} is said to be differentiable if Δ J [ h ] = φ [ h ] + ε ‖ h ‖ , {\\displaystyle \\Delta J[h]=\\varphi [h]+\\varepsilon \\|h\\|,} where φ [ h ] {\\displaystyle \\varphi [h]} is a linear functional, ‖ h ‖ {\\displaystyle \\|h\\|} is the norm of h , {\\displaystyle h,} and ε → 0 {\\displaystyle \\varepsilon \\to 0} as ‖ h ‖ → 0. {\\displaystyle \\|h\\|\\to 0.} The linear functional φ [ h ] {\\displaystyle \\varphi [h]} is the first variation of J [ y ] {\\displaystyle J[y]} and is denoted by, δ J [ h ] = φ [ h ] . {\\displaystyle \\delta J[h]=\\varphi [h].} The functional J [ y ] {\\displaystyle J[y]} is said to be twice differentiable if Δ J [ h ] = φ 1 [ h ] + φ 2 [ h ] + ε ‖ h ‖ 2 , {\\displaystyle \\Delta J[h]=\\varphi _{1}[h]+\\varphi _{2}[h]+\\varepsilon \\|h\\|^{2},} where φ 1 [ h ] {\\displaystyle \\varphi _{1}[h]} is a linear functional (the first variation), φ 2 [ h ] {\\displaystyle \\varphi _{2}[h]} is a quadratic functional, and ε → 0 {\\displaystyle \\varepsilon \\to 0} as ‖ h ‖ → 0. {\\displaystyle \\|h\\|\\to 0.} The quadratic functional φ 2 [ h ] {\\displaystyle \\varphi _{2}[h]} is the second variation of J [ y ] {\\displaystyle J[y]} and is denoted by, δ 2 J [ h ] = φ 2 [ h ] . {\\displaystyle \\delta ^{2}J[h]=\\varphi _{2}[h].} The second variation δ 2 J [ h ] {\\displaystyle \\delta ^{2}J[h]} is said to be strongly positive if δ 2 J [ h ] ≥ k ‖ h ‖ 2 , {\\displaystyle \\delta ^{2}J[h]\\geq k\\|h\\|^{2},} for all h {\\displaystyle h} and for some constant k > 0 {\\displaystyle k>0} . Using the above definitions, especially the definitions of first variation, second variation, and strongly positive, the following sufficient condition for a minimum of a functional can be stated. == See also == == Notes == == References == == Further reading == Benesova, B. and Kruzik, M.: \"Weak Lower Semicontinuity of Integral Functionals and Applications\". SIAM Review 59(4) (2017), 703–766. Bolza, O.: Lectures on the Calculus of Variations. Chelsea Publishing Company, 1904, available on Digital Mathematics library. 2nd edition republished in 1961, paperback in 2005, ISBN 978-1-4181-8201-4. Cassel, Kevin W.: Variational Methods with Applications in Science and Engineering, Cambridge University Press, 2013. Clegg, J.C.: Calculus of Variations, Interscience Publishers Inc., 1968. Courant, R.: Dirichlet's principle, conformal mapping and minimal surfaces. Interscience, 1950. Dacorogna, Bernard: \"Introduction\" Introduction to the Calculus of Variations, 3rd edition. 2014, World Scientific Publishing, ISBN 978-1-78326-551-0. Elsgolc, L.E.: Calculus of Variations, Pergamon Press Ltd., 1962. Forsyth, A.R.: Calculus of Variations, Dover, 1960. Fox, Charles: An Introduction to the Calculus of Variations, Dover Publ., 1987. Giaquinta, Mariano; Hildebrandt, Stefan: Calculus of Variations I and II, Springer-Verlag, ISBN 978-3-662-03278-7 and ISBN 978-3-662-06201-2 Jost, J. and X. Li-Jost: Calculus of Variations. Cambridge University Press, 1998. Lebedev, L.P. and Cloud, M.J.: The Calculus of Variations and Functional Analysis with Optimal Control and Applications in Mechanics, World Scientific, 2003, pages 1–98. Logan, J. David: Applied Mathematics, 3rd edition. Wiley-Interscience, 2006 Pike, Ralph W. \"Chapter 8: Calculus of Variations\". Optimization for Engineering Systems. Louisiana State University. Archived from the original on 2007-07-05. Roubicek, T.: \"Calculus of variations\". Chap.17 in: Mathematical Tools for Physicists. (Ed. M. Grinfeld) J. Wiley, Weinheim, 2014, ISBN 978-3-527-41188-7, pp. 551–588. Sagan, Hans: Introduction to the Calculus of Variations, Dover, 1992. Weinstock, Robert: Calculus of Variations with Applications to Physics and Engineering, Dover, 1974 (reprint of 1952 ed.). == External links == Variational calculus. Encyclopedia of Mathematics. calculus of variations. PlanetMath. Calculus of Variations. MathWorld. Calculus of variations. Example problems. Mathematics - Calculus of Variations and Integral Equations. Lectures on YouTube. Selected papers on Geodesic Fields. Part I, Part II."
  },
  "chunks": [
    {
      "id": "calculusofvariations_033ccf46_c0000",
      "article_id": "calculusofvariations_033ccf46",
      "section": "Lead",
      "heading_path": "Lead",
      "start_char": 0,
      "end_char": 843,
      "content": "The calculus of variations (or variational calculus) is a field of mathematical analysis that uses variations, which are small changes in functions and functionals, to find maxima and minima of functionals: mappings from a set of functions to the real numbers. Functionals are often expressed as definite integrals involving functions and their derivatives. Functions that maximize or minimize functionals may be found using the Euler–Lagrange equation of the calculus of variations. A simple example of such a problem is to find the curve of shortest length connecting two points. If there are no constraints, the solution is a straight line between the points. However, if the curve is constrained to lie on a surface in space, then the solution is less obvious, and possibly many solutions may exist. Such solutions are known as geodesics.",
      "char_count": 842,
      "token_estimate": 210,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0001",
      "article_id": "calculusofvariations_033ccf46",
      "section": "Lead",
      "heading_path": "Lead",
      "start_char": 843,
      "end_char": 1624,
      "content": "A related problem is posed by Fermat's principle: light follows the path of shortest optical length connecting two points, which depends upon the material of the medium. One corresponding concept in mechanics is the principle of least/stationary action. Many important problems involve functions of several variables. Solutions of boundary value problems for the Laplace equation satisfy the Dirichlet's principle. Plateau's problem requires finding a surface of minimal area that spans a given contour in space: a solution can often be found by dipping a frame in soapy water. Although such experiments are relatively easy to perform, their mathematical formulation is far from simple: there may be more than one locally minimizing surface, and they may have non-trivial topology.",
      "char_count": 781,
      "token_estimate": 195,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0002",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 1638,
      "end_char": 2530,
      "content": "== History == The calculus of variations began with the work of Isaac Newton, such as with Newton's minimal resistance problem, which he formulated and solved in 1685, and later published in his Principia in 1687, which was the first problem in the field to be formulated and correctly solved, and was also one of the most difficult problems tackled by variational methods prior to the twentieth century. This problem was followed by the brachistochrone curve problem raised by Johann Bernoulli (1696), which was similar to one raised by Galileo Galilei in 1638, but he did not solve the problem explicity nor did he use the methods based on calculus. Bernoulli had solved the problem, using the principle of least time in the process, but not calculus of variations, whereas Newton did to solve the problem in 1697, and as a result, he pioneered the field with his work on the two problems.",
      "char_count": 891,
      "token_estimate": 222,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0003",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 2530,
      "end_char": 3433,
      "content": "The problem would immediately occupy the attention of Jacob Bernoulli and the Marquis de l'Hôpital, but Leonhard Euler first elaborated the subject, beginning in 1733. Joseph-Louis Lagrange was influenced by Euler's work to contribute greatly to the theory. After Euler saw the 1755 work of the 19-year-old Lagrange, Euler dropped his own partly geometric approach in favor of Lagrange's purely analytic approach and renamed the subject the calculus of variations in his 1756 lecture Elementa Calculi Variationum. Adrien-Marie Legendre (1786) laid down a method, not entirely satisfactory, for the discrimination of maxima and minima. Isaac Newton and Gottfried Leibniz also gave some early attention to the subject. To this discrimination Vincenzo Brunacci (1810), Carl Friedrich Gauss (1829), Siméon Poisson (1831), Mikhail Ostrogradsky (1834), and Carl Jacobi (1837) have been among the contributors.",
      "char_count": 903,
      "token_estimate": 225,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0004",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 3434,
      "end_char": 4366,
      "content": "An important general work is that of Pierre Frédéric Sarrus (1842) which was condensed and improved by Augustin-Louis Cauchy (1844). Other valuable treatises and memoirs have been written by Strauch (1849), John Hewitt Jellett (1850), Otto Hesse (1857), Alfred Clebsch (1858), and Lewis Buffett Carll (1885), but perhaps the most important work of the century is that of Karl Weierstrass. His celebrated course on the theory is epoch-making, and it may be asserted that he was the first to place it on a firm and unquestionable foundation. The 20th and the 23rd Hilbert problem published in 1900 encouraged further development. In the 20th century David Hilbert, Oskar Bolza, Gilbert Ames Bliss, Emmy Noether, Leonida Tonelli, Henri Lebesgue and Jacques Hadamard among others made significant contributions. Marston Morse applied calculus of variations in what is now called Morse theory. Lev Pontryagin, Ralph Rockafellar and F. H.",
      "char_count": 932,
      "token_estimate": 233,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0005",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Extrema ==",
      "heading_path": "== Extrema ==",
      "start_char": 4557,
      "end_char": 5523,
      "content": "== Extrema == The calculus of variations is concerned with the maxima or minima (collectively called extrema) of functionals. A functional maps functions to scalars, so functionals have been described as \"functions of functions.\" Functionals have extrema with respect to the elements y {\\displaystyle y} of a given function space defined over a given domain. A functional J [ y ] {\\displaystyle J[y]} is said to have an extremum at the function f {\\displaystyle f} if Δ J = J [ y ] − J [ f ] {\\displaystyle \\Delta J=J[y]-J[f]} has the same sign for all y {\\displaystyle y} in an arbitrarily small neighborhood of f . {\\displaystyle f.} The function f {\\displaystyle f} is called an extremal function or extremal. The extremum J [ f ] {\\displaystyle J[f]} is called a local maximum if Δ J ≤ 0 {\\displaystyle \\Delta J\\leq 0} everywhere in an arbitrarily small neighborhood of f , {\\displaystyle f,} and a local minimum if Δ J ≥ 0 {\\displaystyle \\Delta J\\geq 0} there.",
      "char_count": 965,
      "token_estimate": 241,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0006",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Extrema ==",
      "heading_path": "== Extrema ==",
      "start_char": 5523,
      "end_char": 6219,
      "content": "For a function space of continuous functions, extrema of corresponding functionals are called strong extrema or weak extrema, depending on whether the first derivatives of the continuous functions are respectively all continuous or not. Both strong and weak extrema of functionals are for a space of continuous functions but strong extrema have the additional requirement that the first derivatives of the functions in the space be continuous. Thus a strong extremum is also a weak extremum, but the converse may not hold. Finding strong extrema is more difficult than finding weak extrema. An example of a necessary condition that is used for finding weak extrema is the Euler–Lagrange equation.",
      "char_count": 696,
      "token_estimate": 174,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0007",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Euler–Lagrange equation ==",
      "heading_path": "== Euler–Lagrange equation ==",
      "start_char": 6236,
      "end_char": 7189,
      "content": "== Euler–Lagrange equation == Finding the extrema of functionals is similar to finding the maxima and minima of functions. The maxima and minima of a function may be located by finding the points where its derivative vanishes (i.e., is equal to zero). The extrema of functionals may be obtained by finding functions for which the functional derivative is equal to zero. This leads to solving the associated Euler–Lagrange equation. Consider the functional J [ y ] = ∫ x 1 x 2 L ( x , y ( x ) , y ′ ( x ) ) d x , {\\displaystyle J[y]=\\int _{x_{1}}^{x_{2}}L\\left(x,y(x),y'(x)\\right)\\,dx,} where x 1 , x 2 {\\displaystyle x_{1},x_{2}} are constants, y ( x ) {\\displaystyle y(x)} is twice continuously differentiable, y ′ ( x ) = d y d x , {\\displaystyle y'(x)={\\frac {dy}{dx}},} L ( x , y ( x ) , y ′ ( x ) ) {\\displaystyle L\\left(x,y(x),y'(x)\\right)} is twice continuously differentiable with respect to its arguments x , y , {\\displaystyle x,y,} and y ′ .",
      "char_count": 952,
      "token_estimate": 238,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0008",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Euler–Lagrange equation ==",
      "heading_path": "== Euler–Lagrange equation ==",
      "start_char": 7189,
      "end_char": 8005,
      "content": "{\\displaystyle y'.} If the functional J [ y ] {\\displaystyle J[y]} attains a local minimum at f , {\\displaystyle f,} and η ( x ) {\\displaystyle \\eta (x)} is an arbitrary function that has at least one derivative and vanishes at the endpoints x 1 {\\displaystyle x_{1}} and x 2 , {\\displaystyle x_{2},} then for any number ε {\\displaystyle \\varepsilon } close to 0, J [ f ] ≤ J [ f + ε η ] . {\\displaystyle J[f]\\leq J[f+\\varepsilon \\eta ]\\,.} The term ε η {\\displaystyle \\varepsilon \\eta } is called the variation of the function f {\\displaystyle f} and is denoted by δ f . {\\displaystyle \\delta f.} Substituting f + ε η {\\displaystyle f+\\varepsilon \\eta } for y {\\displaystyle y} in the functional J [ y ] , {\\displaystyle J[y],} the result is a function of ε , {\\displaystyle \\varepsilon ,} Φ ( ε ) = J [ f + ε η ] .",
      "char_count": 816,
      "token_estimate": 204,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0009",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Euler–Lagrange equation ==",
      "heading_path": "== Euler–Lagrange equation ==",
      "start_char": 8006,
      "end_char": 8814,
      "content": "{\\displaystyle \\Phi (\\varepsilon )=J[f+\\varepsilon \\eta ]\\,.} Since the functional J [ y ] {\\displaystyle J[y]} has a minimum for y = f {\\displaystyle y=f} the function Φ ( ε ) {\\displaystyle \\Phi (\\varepsilon )} has a minimum at ε = 0 {\\displaystyle \\varepsilon =0} and thus, Φ ′ ( 0 ) ≡ d Φ d ε | ε = 0 = ∫ x 1 x 2 d L d ε | ε = 0 d x = 0 . {\\displaystyle \\Phi '(0)\\equiv \\left.{\\frac {d\\Phi }{d\\varepsilon }}\\right|_{\\varepsilon =0}=\\int _{x_{1}}^{x_{2}}\\left.{\\frac {dL}{d\\varepsilon }}\\right|_{\\varepsilon =0}dx=0\\,.} Taking the total derivative of L [ x , y , y ′ ] , {\\displaystyle L\\left[x,y,y'\\right],} where y = f + ε η {\\displaystyle y=f+\\varepsilon \\eta } and y ′ = f ′ + ε η ′ {\\displaystyle y'=f'+\\varepsilon \\eta '} are considered as functions of ε {\\displaystyle \\varepsilon } rather than x ,",
      "char_count": 808,
      "token_estimate": 202,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0010",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Euler–Lagrange equation ==",
      "heading_path": "== Euler–Lagrange equation ==",
      "start_char": 8815,
      "end_char": 9375,
      "content": "{\\displaystyle x,} yields d L d ε = ∂ L ∂ y d y d ε + ∂ L ∂ y ′ d y ′ d ε {\\displaystyle {\\frac {dL}{d\\varepsilon }}={\\frac {\\partial L}{\\partial y}}{\\frac {dy}{d\\varepsilon }}+{\\frac {\\partial L}{\\partial y'}}{\\frac {dy'}{d\\varepsilon }}} and because d y d ε = η {\\displaystyle {\\frac {dy}{d\\varepsilon }}=\\eta } and d y ′ d ε = η ′ , {\\displaystyle {\\frac {dy'}{d\\varepsilon }}=\\eta ',} d L d ε = ∂ L ∂ y η + ∂ L ∂ y ′ η ′ . {\\displaystyle {\\frac {dL}{d\\varepsilon }}={\\frac {\\partial L}{\\partial y}}\\eta +{\\frac {\\partial L}{\\partial y'}}\\eta '.} Therefore,",
      "char_count": 560,
      "token_estimate": 140,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0011",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Euler–Lagrange equation ==",
      "heading_path": "== Euler–Lagrange equation ==",
      "start_char": 9376,
      "end_char": 10215,
      "content": "∫ x 1 x 2 d L d ε | ε = 0 d x = ∫ x 1 x 2 ( ∂ L ∂ f η + ∂ L ∂ f ′ η ′ ) d x = ∫ x 1 x 2 ∂ L ∂ f η d x + ∂ L ∂ f ′ η | x 1 x 2 − ∫ x 1 x 2 η d d x ∂ L ∂ f ′ d x = ∫ x 1 x 2 ( ∂ L ∂ f η − η d d x ∂ L ∂ f ′ ) d x {\\displaystyle {\\begin{aligned}\\int _{x_{1}}^{x_{2}}\\left.{\\frac {dL}{d\\varepsilon }}\\right|_{\\varepsilon =0}dx&=\\int _{x_{1}}^{x_{2}}\\left({\\frac {\\partial L}{\\partial f}}\\eta +{\\frac {\\partial L}{\\partial f'}}\\eta '\\right)\\,dx\\\\&=\\int _{x_{1}}^{x_{2}}{\\frac {\\partial L}{\\partial f}}\\eta \\,dx+\\left.{\\frac {\\partial L}{\\partial f'}}\\eta \\right|_{x_{1}}^{x_{2}}-\\int _{x_{1}}^{x_{2}}\\eta {\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}\\,dx\\\\&=\\int _{x_{1}}^{x_{2}}\\left({\\frac {\\partial L}{\\partial f}}\\eta -\\eta {\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}\\right)\\,dx\\\\\\end{aligned}}} where L [ x , y , y ′ ] → L [ x , f ,",
      "char_count": 839,
      "token_estimate": 209,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0012",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Euler–Lagrange equation ==",
      "heading_path": "== Euler–Lagrange equation ==",
      "start_char": 10216,
      "end_char": 11113,
      "content": "f ′ ] {\\displaystyle L\\left[x,y,y'\\right]\\to L\\left[x,f,f'\\right]} when ε = 0 {\\displaystyle \\varepsilon =0} and we have used integration by parts on the second term. The second term on the second line vanishes because η = 0 {\\displaystyle \\eta =0} at x 1 {\\displaystyle x_{1}} and x 2 {\\displaystyle x_{2}} by definition. Also, as previously mentioned the left side of the equation is zero so that ∫ x 1 x 2 η ( x ) ( ∂ L ∂ f − d d x ∂ L ∂ f ′ ) d x = 0 . {\\displaystyle \\int _{x_{1}}^{x_{2}}\\eta (x)\\left({\\frac {\\partial L}{\\partial f}}-{\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}\\right)\\,dx=0\\,.} According to the fundamental lemma of calculus of variations, the part of the integrand in parentheses is zero, i.e. ∂ L ∂ f − d d x ∂ L ∂ f ′ = 0 {\\displaystyle {\\frac {\\partial L}{\\partial f}}-{\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}=0} which is called the Euler–Lagrange equation.",
      "char_count": 897,
      "token_estimate": 224,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0013",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Euler–Lagrange equation ==",
      "heading_path": "== Euler–Lagrange equation ==",
      "start_char": 11114,
      "end_char": 11691,
      "content": "The left hand side of this equation is called the functional derivative of J [ f ] {\\displaystyle J[f]} and is denoted δ J {\\displaystyle \\delta J} or δ f ( x ) . {\\displaystyle \\delta f(x).} In general this gives a second-order ordinary differential equation which can be solved to obtain the extremal function f ( x ) . {\\displaystyle f(x).} The Euler–Lagrange equation is a necessary, but not sufficient, condition for an extremum J [ f ] . {\\displaystyle J[f].} A sufficient condition for a minimum is given in the section Variations and sufficient condition for a minimum.",
      "char_count": 577,
      "token_estimate": 144,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0014",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== = Example ==",
      "heading_path": "== = Example ==",
      "start_char": 11678,
      "end_char": 12616,
      "content": "== = Example === In order to illustrate this process, consider the problem of finding the extremal function y = f ( x ) , {\\displaystyle y=f(x),} which is the shortest curve that connects two points ( x 1 , y 1 ) {\\displaystyle \\left(x_{1},y_{1}\\right)} and ( x 2 , y 2 ) . {\\displaystyle \\left(x_{2},y_{2}\\right).} The arc length of the curve is given by A [ y ] = ∫ x 1 x 2 1 + [ y ′ ( x ) ] 2 d x , {\\displaystyle A[y]=\\int _{x_{1}}^{x_{2}}{\\sqrt {1+[y'(x)]^{2}}}\\,dx\\,,} with y ′ ( x ) = d y d x , y 1 = f ( x 1 ) , y 2 = f ( x 2 ) . {\\displaystyle y'(x)={\\frac {dy}{dx}}\\,,\\ \\ y_{1}=f(x_{1})\\,,\\ \\ y_{2}=f(x_{2})\\,.} Note that assuming y is a function of x loses generality; ideally both should be a function of some other parameter. This approach is good solely for instructive purposes. The Euler–Lagrange equation will now be used to find the extremal function f ( x ) {\\displaystyle f(x)} that minimizes the functional A [ y ] .",
      "char_count": 937,
      "token_estimate": 234,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0015",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== = Example ==",
      "heading_path": "== = Example ==",
      "start_char": 12616,
      "end_char": 13572,
      "content": "{\\displaystyle A[y].} ∂ L ∂ f − d d x ∂ L ∂ f ′ = 0 {\\displaystyle {\\frac {\\partial L}{\\partial f}}-{\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}=0} with L = 1 + [ f ′ ( x ) ] 2 . {\\displaystyle L={\\sqrt {1+[f'(x)]^{2}}}\\,.} Since f {\\displaystyle f} does not appear explicitly in L , {\\displaystyle L,} the first term in the Euler–Lagrange equation vanishes for all f ( x ) {\\displaystyle f(x)} and thus, d d x ∂ L ∂ f ′ = 0 . {\\displaystyle {\\frac {d}{dx}}{\\frac {\\partial L}{\\partial f'}}=0\\,.} Substituting for L {\\displaystyle L} and taking the derivative, d d x f ′ ( x ) 1 + [ f ′ ( x ) ] 2 = 0 . {\\displaystyle {\\frac {d}{dx}}\\ {\\frac {f'(x)}{\\sqrt {1+[f'(x)]^{2}}}}\\ =0\\,.} Thus f ′ ( x ) 1 + [ f ′ ( x ) ] 2 = c , {\\displaystyle {\\frac {f'(x)}{\\sqrt {1+[f'(x)]^{2}}}}=c\\,,} for some constant c {\\displaystyle c} . Then [ f ′ ( x ) ] 2 1 + [ f ′ ( x ) ] 2 = c 2 , {\\displaystyle {\\frac {[f'(x)]^{2}}{1+[f'(x)]^{2}}}=c^{2}\\,,} where 0 ≤ c 2 < 1.",
      "char_count": 956,
      "token_estimate": 239,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0016",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== = Example ==",
      "heading_path": "== = Example ==",
      "start_char": 13573,
      "end_char": 14527,
      "content": "{\\displaystyle 0\\leq c^{2}<1.} Solving, we get [ f ′ ( x ) ] 2 = c 2 1 − c 2 {\\displaystyle [f'(x)]^{2}={\\frac {c^{2}}{1-c^{2}}}} which implies that f ′ ( x ) = m {\\displaystyle f'(x)=m} is a constant and therefore that the shortest curve that connects two points ( x 1 , y 1 ) {\\displaystyle \\left(x_{1},y_{1}\\right)} and ( x 2 , y 2 ) {\\displaystyle \\left(x_{2},y_{2}\\right)} is f ( x ) = m x + b with m = y 2 − y 1 x 2 − x 1 and b = x 2 y 1 − x 1 y 2 x 2 − x 1 {\\displaystyle f(x)=mx+b\\qquad {\\text{with}}\\ \\ m={\\frac {y_{2}-y_{1}}{x_{2}-x_{1}}}\\quad {\\text{and}}\\quad b={\\frac {x_{2}y_{1}-x_{1}y_{2}}{x_{2}-x_{1}}}} and we have thus found the extremal function f ( x ) {\\displaystyle f(x)} that minimizes the functional A [ y ] {\\displaystyle A[y]} so that A [ f ] {\\displaystyle A[f]} is a minimum. The equation for a straight line is y = m x + b . {\\displaystyle y=mx+b.} In other words, the shortest distance between two points is a straight line.",
      "char_count": 954,
      "token_estimate": 238,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0017",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Beltrami's identity ==",
      "heading_path": "== Beltrami's identity ==",
      "start_char": 14538,
      "end_char": 15455,
      "content": "== Beltrami's identity == In physics problems it may be the case that ∂ L ∂ x = 0 , {\\displaystyle {\\frac {\\partial L}{\\partial x}}=0,} meaning the integrand is a function of f ( x ) {\\displaystyle f(x)} and f ′ ( x ) {\\displaystyle f'(x)} but x {\\displaystyle x} does not appear separately. In that case, the Euler–Lagrange equation can be simplified to the Beltrami identity L − f ′ ∂ L ∂ f ′ = C , {\\displaystyle L-f'{\\frac {\\partial L}{\\partial f'}}=C\\,,} where C {\\displaystyle C} is a constant. The left hand side is the Legendre transformation of L {\\displaystyle L} with respect to f ′ ( x ) . {\\displaystyle f'(x).} The intuition behind this result is that, if the variable x {\\displaystyle x} is actually time, then the statement ∂ L ∂ x = 0 {\\displaystyle {\\frac {\\partial L}{\\partial x}}=0} implies that the Lagrangian is time-independent. By Noether's theorem, there is an associated conserved quantity.",
      "char_count": 916,
      "token_estimate": 229,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0018",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Beltrami's identity ==",
      "heading_path": "== Beltrami's identity ==",
      "start_char": 15455,
      "end_char": 15652,
      "content": "In this case, this quantity is the Hamiltonian, the Legendre transform of the Lagrangian, which (often) coincides with the energy of the system. This is (minus) the constant in Beltrami's identity.",
      "char_count": 197,
      "token_estimate": 49,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0019",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Euler–Poisson equation ==",
      "heading_path": "== Euler–Poisson equation ==",
      "start_char": 15656,
      "end_char": 16239,
      "content": "== Euler–Poisson equation == If S {\\displaystyle S} depends on higher-derivatives of y ( x ) {\\displaystyle y(x)} , that is, if S = ∫ a b f ( x , y ( x ) , y ′ ( x ) , … , y ( n ) ( x ) ) d x , {\\displaystyle S=\\int _{a}^{b}f(x,y(x),y'(x),\\dots ,y^{(n)}(x))dx,} then y {\\displaystyle y} must satisfy the Euler–Poisson equation, ∂ f ∂ y − d d x ( ∂ f ∂ y ′ ) + ⋯ + ( − 1 ) n d n d x n [ ∂ f ∂ y ( n ) ] = 0. {\\displaystyle {\\frac {\\partial f}{\\partial y}}-{\\frac {d}{dx}}\\left({\\frac {\\partial f}{\\partial y'}}\\right)+\\dots +(-1)^{n}{\\frac {d^{n}}{dx^{n}}}\\left[{\\frac {\\partial f}{\\partial y^{(n)}}}\\right]=0.}",
      "char_count": 610,
      "token_estimate": 152,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0020",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Du Bois-Reymond's theorem ==",
      "heading_path": "== Du Bois-Reymond's theorem ==",
      "start_char": 16270,
      "end_char": 16988,
      "content": "== Du Bois-Reymond's theorem == The discussion thus far has assumed that extremal functions possess two continuous derivatives, although the existence of the integral J {\\displaystyle J} requires only first derivatives of trial functions. The condition that the first variation vanishes at an extremal may be regarded as a weak form of the Euler–Lagrange equation. The theorem of Du Bois-Reymond asserts that this weak form implies the strong form. If L {\\displaystyle L} has continuous first and second derivatives with respect to all of its arguments, and if ∂ 2 L ∂ f ′ 2 ≠ 0 , {\\displaystyle {\\frac {\\partial ^{2}L}{\\partial f'^{2}}}\\neq 0,} then f {\\displaystyle f} has two continuous derivatives, and it satisfies the Euler–Lagrange equation.",
      "char_count": 748,
      "token_estimate": 187,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0021",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Lavrentiev phenomenon ==",
      "heading_path": "== Lavrentiev phenomenon ==",
      "start_char": 17015,
      "end_char": 17915,
      "content": "== Lavrentiev phenomenon == Hilbert was the first to give good conditions for the Euler–Lagrange equations to give a stationary solution. Within a convex area and a positive thrice differentiable Lagrangian the solutions are composed of a countable collection of sections that either go along the boundary or satisfy the Euler–Lagrange equations in the interior. However Lavrentiev in 1926 showed that there are circumstances where there is no optimum solution but one can be approached arbitrarily closely by increasing numbers of sections. The Lavrentiev Phenomenon identifies a difference in the infimum of a minimization problem across different classes of admissible functions. For instance the following problem, presented by Manià in 1934: L [ x ] = ∫ 0 1 ( x 3 − t ) 2 x ′ 6 , {\\displaystyle L[x]=\\int _{0}^{1}(x^{3}-t)^{2}x'^{6},} A = { x ∈ W 1 , 1 ( 0 , 1 ) : x ( 0 ) = 0 , x ( 1 ) = 1 } .",
      "char_count": 899,
      "token_estimate": 224,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0022",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Lavrentiev phenomenon ==",
      "heading_path": "== Lavrentiev phenomenon ==",
      "start_char": 17915,
      "end_char": 18729,
      "content": "{\\displaystyle {A}=\\{x\\in W^{1,1}(0,1):x(0)=0,\\ x(1)=1\\}.} Clearly, x ( t ) = t 1 3 {\\displaystyle x(t)=t^{\\frac {1}{3}}} minimizes the functional, but we find any function x ∈ W 1 , ∞ {\\displaystyle x\\in W^{1,\\infty }} gives a value bounded away from the infimum. Examples (in one-dimension) are traditionally manifested across W 1 , 1 {\\displaystyle W^{1,1}} and W 1 , ∞ , {\\displaystyle W^{1,\\infty },} but Ball and Mizel procured the first functional that displayed Lavrentiev's Phenomenon across W 1 , p {\\displaystyle W^{1,p}} and W 1 , q {\\displaystyle W^{1,q}} for 1 ≤ p < q < ∞ . {\\displaystyle 1\\leq p 0 , {\\displaystyle n(x,y)={\\begin{cases}n_{(-)}&{\\text{if}}\\quad x<0,\\\\n_{(+)}&{\\text{if}}\\quad x>0,\\end{cases}}} where n ( − ) {\\displaystyle n_{(-)}} and n ( + ) {\\displaystyle n_{(+)}} are constants.",
      "char_count": 814,
      "token_estimate": 203,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0023",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Lavrentiev phenomenon ==",
      "heading_path": "== Lavrentiev phenomenon ==",
      "start_char": 18730,
      "end_char": 19314,
      "content": "Then the Euler–Lagrange equation holds as before in the region where x < 0 {\\displaystyle x<0} or x > 0 {\\displaystyle x>0} , and in fact the path is a straight line there, since the refractive index is constant. At the x = 0 {\\displaystyle x=0} , f {\\displaystyle f} must be continuous, but f ′ {\\displaystyle f'} may be discontinuous. After integration by parts in the separate regions and using the Euler–Lagrange equations, the first variation takes the form δ A [ f 0 , f 1 ] = f 1 ( 0 ) [ n ( − ) f 0 ′ ( 0 − ) 1 + f 0 ′ ( 0 − ) 2 − n ( + ) f 0 ′ ( 0 + ) 1 + f 0 ′ ( 0 + ) 2 ] .",
      "char_count": 584,
      "token_estimate": 146,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0024",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Lavrentiev phenomenon ==",
      "heading_path": "== Lavrentiev phenomenon ==",
      "start_char": 19315,
      "end_char": 19948,
      "content": "{\\displaystyle \\delta A[f_{0},f_{1}]=f_{1}(0)\\left[n_{(-)}{\\frac {f_{0}'(0^{-})}{\\sqrt {1+f_{0}'(0^{-})^{2}}}}-n_{(+)}{\\frac {f_{0}'(0^{+})}{\\sqrt {1+f_{0}'(0^{+})^{2}}}}\\right].} The factor multiplying n ( − ) {\\displaystyle n_{(-)}} is the sine of angle of the incident ray with the x {\\displaystyle x} axis, and the factor multiplying n ( + ) {\\displaystyle n_{(+)}} is the sine of angle of the refracted ray with the x {\\displaystyle x} axis. Snell's law for refraction requires that these terms be equal. As this calculation demonstrates, Snell's law is equivalent to vanishing of the first variation of the optical path length.",
      "char_count": 633,
      "token_estimate": 158,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0025",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== = Mechanics ==",
      "heading_path": "== = Mechanics ==",
      "start_char": 19939,
      "end_char": 20643,
      "content": "== = Mechanics === In classical mechanics, the action, S , {\\displaystyle S,} is defined as the time integral of the Lagrangian, L {\\displaystyle L} . The Lagrangian is the difference of energies, L = T − U , {\\displaystyle L=T-U,} where T {\\displaystyle T} is the kinetic energy of a mechanical system and U {\\displaystyle U} its potential energy. Hamilton's principle (or the action principle) states that the motion of a conservative holonomic (integrable constraints) mechanical system is such that the action integral S = ∫ t 0 t 1 L ( x , x ˙ , t ) d t {\\displaystyle S=\\int _{t_{0}}^{t_{1}}L(x,{\\dot {x}},t)\\,dt} is stationary with respect to variations in the path x ( t ) {\\displaystyle x(t)} .",
      "char_count": 703,
      "token_estimate": 175,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0026",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== = Mechanics ==",
      "heading_path": "== = Mechanics ==",
      "start_char": 20643,
      "end_char": 21619,
      "content": "The Euler–Lagrange equations for this system are known as Lagrange's equations: d d t ∂ L ∂ x ˙ = ∂ L ∂ x , {\\displaystyle {\\frac {d}{dt}}{\\frac {\\partial L}{\\partial {\\dot {x}}}}={\\frac {\\partial L}{\\partial x}},} and they are equivalent to Newton's equations of motion (for such systems). The conjugate momenta P {\\displaystyle P} are defined by p = ∂ L ∂ x ˙ . {\\displaystyle p={\\frac {\\partial L}{\\partial {\\dot {x}}}}.} For example, if T = 1 2 m x ˙ 2 , {\\displaystyle T={\\frac {1}{2}}m{\\dot {x}}^{2},} then p = m x ˙ . {\\displaystyle p=m{\\dot {x}}.} Hamiltonian mechanics results if the conjugate momenta are introduced in place of x ˙ {\\displaystyle {\\dot {x}}} by a Legendre transformation of the Lagrangian L {\\displaystyle L} into the Hamiltonian H {\\displaystyle H} defined by H ( x , p , t ) = p x ˙ − L ( x , x ˙ , t ) . {\\displaystyle H(x,p,t)=p\\,{\\dot {x}}-L(x,{\\dot {x}},t).} The Hamiltonian is the total energy of the system: H = T + U {\\displaystyle H=T+U} .",
      "char_count": 976,
      "token_estimate": 244,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0027",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== = Mechanics ==",
      "heading_path": "== = Mechanics ==",
      "start_char": 21620,
      "end_char": 22022,
      "content": "Analogy with Fermat's principle suggests that solutions of Lagrange's equations (the particle trajectories) may be described in terms of level surfaces of some function of X {\\displaystyle X} . This function is a solution of the Hamilton–Jacobi equation: ∂ ψ ∂ t + H ( x , ∂ ψ ∂ x , t ) = 0. {\\displaystyle {\\frac {\\partial \\psi }{\\partial t}}+H\\left(x,{\\frac {\\partial \\psi }{\\partial x}},t\\right)=0.}",
      "char_count": 402,
      "token_estimate": 100,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0028",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== = Further applications ==",
      "heading_path": "== = Further applications ==",
      "start_char": 22034,
      "end_char": 22903,
      "content": "== = Further applications === Further applications of the calculus of variations include the following: The derivation of the catenary shape Solution to Newton's minimal resistance problem Solution to the brachistochrone problem Solution to the tautochrone problem Solution to isoperimetric problems Calculating geodesics Finding minimal surfaces and solving Plateau's problem Optimal control Analytical mechanics, or reformulations of Newton's laws of motion, most notably Lagrangian and Hamiltonian mechanics; Geometric optics, especially Lagrangian and Hamiltonian optics; Variational method (quantum mechanics), one way of finding approximations to the lowest energy eigenstate or ground state, and some excited states; Variational Bayesian methods, a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning;",
      "char_count": 868,
      "token_estimate": 217,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0029",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== = Further applications ==",
      "heading_path": "== = Further applications ==",
      "start_char": 22903,
      "end_char": 23291,
      "content": "Variational methods in general relativity, a family of techniques using calculus of variations to solve problems in Einstein's general theory of relativity; Finite element method is a variational method for finding numerical solutions to boundary-value problems in differential equations; Total variation denoising, an image processing method for filtering high variance or noisy signals.",
      "char_count": 388,
      "token_estimate": 97,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0030",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Variations and sufficient condition for a minimum ==",
      "heading_path": "== Variations and sufficient condition for a minimum ==",
      "start_char": 23319,
      "end_char": 24120,
      "content": "== Variations and sufficient condition for a minimum == Calculus of variations is concerned with variations of functionals, which are small changes in the functional's value due to small changes in the function that is its argument. The first variation is defined as the linear part of the change in the functional, and the second variation is defined as the quadratic part. For example, if J [ y ] {\\displaystyle J[y]} is a functional with the function y = y ( x ) {\\displaystyle y=y(x)} as its argument, and there is a small change in its argument from y {\\displaystyle y} to y + h , {\\displaystyle y+h,} where h = h ( x ) {\\displaystyle h=h(x)} is a function in the same function space as y {\\displaystyle y} , then the corresponding change in the functional is Δ J [ h ] = J [ y + h ] − J [ y ] .",
      "char_count": 800,
      "token_estimate": 200,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0031",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Variations and sufficient condition for a minimum ==",
      "heading_path": "== Variations and sufficient condition for a minimum ==",
      "start_char": 24120,
      "end_char": 24699,
      "content": "{\\displaystyle \\Delta J[h]=J[y+h]-J[y].} The functional J [ y ] {\\displaystyle J[y]} is said to be differentiable if Δ J [ h ] = φ [ h ] + ε ‖ h ‖ , {\\displaystyle \\Delta J[h]=\\varphi [h]+\\varepsilon \\|h\\|,} where φ [ h ] {\\displaystyle \\varphi [h]} is a linear functional, ‖ h ‖ {\\displaystyle \\|h\\|} is the norm of h , {\\displaystyle h,} and ε → 0 {\\displaystyle \\varepsilon \\to 0} as ‖ h ‖ → 0. {\\displaystyle \\|h\\|\\to 0.} The linear functional φ [ h ] {\\displaystyle \\varphi [h]} is the first variation of J [ y ] {\\displaystyle J[y]} and is denoted by, δ J [ h ] = φ [ h ] .",
      "char_count": 579,
      "token_estimate": 144,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "calculusofvariations_033ccf46_c0032",
      "article_id": "calculusofvariations_033ccf46",
      "section": "== Variations and sufficient condition for a minimum ==",
      "heading_path": "== Variations and sufficient condition for a minimum ==",
      "start_char": 24700,
      "end_char": 25671,
      "content": "{\\displaystyle \\delta J[h]=\\varphi [h].} The functional J [ y ] {\\displaystyle J[y]} is said to be twice differentiable if Δ J [ h ] = φ 1 [ h ] + φ 2 [ h ] + ε ‖ h ‖ 2 , {\\displaystyle \\Delta J[h]=\\varphi _{1}[h]+\\varphi _{2}[h]+\\varepsilon \\|h\\|^{2},} where φ 1 [ h ] {\\displaystyle \\varphi _{1}[h]} is a linear functional (the first variation), φ 2 [ h ] {\\displaystyle \\varphi _{2}[h]} is a quadratic functional, and ε → 0 {\\displaystyle \\varepsilon \\to 0} as ‖ h ‖ → 0. {\\displaystyle \\|h\\|\\to 0.} The quadratic functional φ 2 [ h ] {\\displaystyle \\varphi _{2}[h]} is the second variation of J [ y ] {\\displaystyle J[y]} and is denoted by, δ 2 J [ h ] = φ 2 [ h ] . {\\displaystyle \\delta ^{2}J[h]=\\varphi _{2}[h].} The second variation δ 2 J [ h ] {\\displaystyle \\delta ^{2}J[h]} is said to be strongly positive if δ 2 J [ h ] ≥ k ‖ h ‖ 2 , {\\displaystyle \\delta ^{2}J[h]\\geq k\\|h\\|^{2},} for all h {\\displaystyle h} and for some constant k > 0 {\\displaystyle k>0} .",
      "char_count": 971,
      "token_estimate": 242,
      "token_start": null,
      "token_end": null
    }
  ],
  "questions": {
    "total_questions": 10,
    "items": [
      {
        "question": "How are the conjugate momenta (p) defined in the context of the Lagrangian (L)?",
        "answer": "The conjugate momenta (p) are defined by the equation p = ∂L/∂ẋ.",
        "related_chunk_ids": [
          "calculusofvariations_033ccf46_c0026"
        ],
        "category": "FACTUAL"
      },
      {
        "question": "Under what condition is the second variation, δ²J[h], considered to be strongly positive?",
        "answer": "The second variation δ²J[h] is said to be strongly positive if δ²J[h] ≥ k‖h‖² for all h and for some constant k > 0.",
        "related_chunk_ids": [
          "calculusofvariations_033ccf46_c0032"
        ],
        "category": "FACTUAL"
      },
      {
        "question": "How is a local maximum of a functional distinguished from a local minimum?",
        "answer": "A local maximum is distinguished from a local minimum based on the sign of the change in the functional, ΔJ, in a small neighborhood of the extremal function f. An extremum is a local maximum if ΔJ is less than or equal to zero (ΔJ ≤ 0), and it is a local minimum if ΔJ is greater than or equal to zero (ΔJ ≥ 0).",
        "related_chunk_ids": [
          "calculusofvariations_033ccf46_c0005"
        ],
        "category": "INTERPRETATION"
      },
      {
        "question": "Summarize the origins of the calculus of variations, including the key figures, the initial problems they addressed, and how these problems exemplify the core concepts of the field.",
        "answer": "The calculus of variations began with Isaac Newton's work on the minimal resistance problem, which he formulated and solved in 1685. This was followed by the brachistochrone curve problem raised by Johann Bernoulli in 1696, which Newton also solved. These early problems are prime examples of the core goal of variational calculus: finding a function that minimizes or maximizes a functional. For instance, the minimal resistance problem sought a shape that minimized resistance, and the brachistochrone problem sought a curve that minimized travel time. Bernoulli's solution for the brachistochrone problem specifically used the principle of least time, a concept related to Fermat's principle in optics and the principle of least action in mechanics, which are also key problems addressed by the calculus of variations.",
        "related_chunk_ids": [
          "calculusofvariations_033ccf46_c0000",
          "calculusofvariations_033ccf46_c0001",
          "calculusofvariations_033ccf46_c0002"
        ],
        "category": "LONG_ANSWER"
      },
      {
        "question": "Trace the evolution of methods for identifying and classifying extrema in the calculus of variations, from early attempts to the establishment of a firm theoretical foundation.",
        "answer": "The calculus of variations is fundamentally concerned with finding the maxima or minima, collectively known as extrema, of functionals. An early, though not entirely satisfactory, method for this was established by Adrien-Marie Legendre in 1786 for the \"discrimination of maxima and minima.\" Other contributors to this specific problem included Vincenzo Brunacci, Carl Friedrich Gauss, and Carl Jacobi. However, the most important 19th-century work was by Karl Weierstrass, whose celebrated course on the theory is considered epoch-making and the first to place it on a \"firm and unquestionable foundation.\"",
        "related_chunk_ids": [
          "calculusofvariations_033ccf46_c0003",
          "calculusofvariations_033ccf46_c0004",
          "calculusofvariations_033ccf46_c0005"
        ],
        "category": "LONG_ANSWER"
      },
      {
        "question": "Explain the general approach for finding the extrema of a functional and how this relates to the Euler-Lagrange equation. What specific type of extremum is the Euler-Lagrange equation used to find?",
        "answer": "The general approach for finding the extrema of a functional is to find functions for which the functional derivative is zero, which is analogous to finding the maxima and minima of a regular function. This process involves analyzing a variation of a function that is assumed to be a local minimum, which leads to solving the associated Euler–Lagrange equation. The Euler-Lagrange equation serves as a necessary condition specifically for finding weak extrema.",
        "related_chunk_ids": [
          "calculusofvariations_033ccf46_c0006",
          "calculusofvariations_033ccf46_c0007",
          "calculusofvariations_033ccf46_c0008"
        ],
        "category": "INTERPRETATION"
      },
      {
        "question": "What are some examples of minimization problems from different fields that are addressed by the calculus of variations?",
        "answer": "The calculus of variations addresses several minimization problems. A foundational example is finding the shortest curve between two points, which is a straight line if unconstrained, or a geodesic if constrained to a surface. Related concepts in physics include Fermat's principle, where light follows the path of shortest optical length, and the principle of least/stationary action in mechanics. Another example is Plateau's problem, which involves finding a surface of minimal area that spans a given contour.",
        "related_chunk_ids": [
          "calculusofvariations_033ccf46_c0000",
          "calculusofvariations_033ccf46_c0001"
        ],
        "category": "LONG_ANSWER"
      },
      {
        "question": "Trace the development of the calculus of variations by describing the key contributions of Isaac Newton, Leonhard Euler, Joseph-Louis Lagrange, and Karl Weierstrass.",
        "answer": "The calculus of variations was pioneered by Isaac Newton, who formulated and solved the minimal resistance problem in 1685 and the brachistochrone curve problem in 1697. In the 18th century, Leonhard Euler began elaborating on the subject in 1733. He was influenced by the work of Joseph-Louis Lagrange, and in turn, Euler adopted Lagrange's purely analytic approach and renamed the field \"calculus of variations\" in 1756. Later, in the 19th century, Karl Weierstrass's work was considered epoch-making and the most important of the century, as he was the first to place the theory on a \"firm and unquestionable foundation.\"",
        "related_chunk_ids": [
          "calculusofvariations_033ccf46_c0002",
          "calculusofvariations_033ccf46_c0003",
          "calculusofvariations_033ccf46_c0004"
        ],
        "category": "LONG_ANSWER"
      },
      {
        "question": "How are the extrema of functionals classified for a space of continuous functions, and what is the key difference between these classifications?",
        "answer": "For a space of continuous functions, the extrema of functionals are classified as either strong or weak. The distinction is based on the continuity of the first derivatives of the functions. Strong extrema require that the first derivatives of the functions in the space are all continuous, while weak extrema do not have this requirement. As a result, a strong extremum is also a weak extremum, but the reverse is not necessarily true.",
        "related_chunk_ids": [
          "calculusofvariations_033ccf46_c0005",
          "calculusofvariations_033ccf46_c0006"
        ],
        "category": "INTERPRETATION"
      },
      {
        "question": "Explain the method used to begin the derivation of the Euler-Lagrange equation, starting from the functional J[y] and culminating in the expression for the total derivative of L.",
        "answer": "To find the function f that minimizes the functional J[y], a variation, εη, is added to f. This creates a new function of ε, defined as Φ(ε) = J[f + εη]. Because f represents a local minimum for the functional J, the function Φ(ε) must have a minimum at ε = 0, which means its derivative, Φ'(0), must equal zero. The calculation of this derivative requires finding the total derivative of L with respect to ε. This is found by considering y and y' as functions of ε, yielding the expression: dL/dε = (∂L/∂y)η + (∂L/∂y')η'.",
        "related_chunk_ids": [
          "calculusofvariations_033ccf46_c0007",
          "calculusofvariations_033ccf46_c0008",
          "calculusofvariations_033ccf46_c0009",
          "calculusofvariations_033ccf46_c0010"
        ],
        "category": "LONG_ANSWER"
      }
    ]
  },
  "metadata": {
    "export_date": "2025-07-31T06:36:46.576Z",
    "content_format": "markdown",
    "total_chunks": 33,
    "description": "Complete article dataset including content, chunks, and generated questions"
  }
}