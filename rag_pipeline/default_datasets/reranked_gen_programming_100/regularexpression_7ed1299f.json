{
  "article": {
    "id": "regularexpression_7ed1299f",
    "title": "Regular expression",
    "url": "https://en.wikipedia.org/wiki/Regular_expression",
    "lang": "en",
    "created_at": "2025-07-30T10:35:04.459268",
    "content": "---\nid: regularexpression_7ed1299f\nurl: https://en.wikipedia.org/wiki/Regular_expression\ntitle: Regular expression\nlang: en\ncreated_at: '2025-07-30T10:30:57.244104'\nchecksum: 0d9370de565844c0c187e7784244ee8ae2e72e237f2f2365ea2ea60025c38e3d\noptions:\n  chunk_size: 1000\n  chunk_overlap: 200\n  split_strategy: header_aware\n  total_questions: 10\n  llm_model: gemini-2.5-pro\nstats:\n  word_count: 4963\n  char_count: 31208\n  num_chunks: 35\n  original_chunks: 36\n  filtered_out: 1\n  num_sections: 0\n---\nA regular expression (shortened as regex or regexp), sometimes referred to as a rational expression, is a sequence of characters that specifies a match pattern in text. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation. Regular expression techniques are developed in theoretical computer science and formal language theory. The concept of regular expressions began in the 1950s, when the American mathematician Stephen Cole Kleene formalized the concept of a regular language. They came into common use with Unix text-processing utilities. Different syntaxes for writing regular expressions have existed since the 1980s, one being the POSIX standard and another, widely used, being the Perl syntax. Regular expressions are used in search engines, in search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK, and in lexical analysis. Regular expressions are supported in many programming languages. Library implementations are often called an \"engine\", and many of these are available for reuse. == History == Regular expressions originated in 1951, when mathematician Stephen Cole Kleene described regular languages using his mathematical notation called regular events. These arose in theoretical computer science, in the subfields of automata theory (models of computation) and the description and classification of formal languages, motivated by Kleene's attempt to describe early artificial neural networks. (Kleene introduced it as an alternative to McCulloch & Pitts's \"prehensible\", but admitted \"We would welcome any suggestions as to a more descriptive term.\") Other early implementations of pattern matching include the SNOBOL language, which did not use regular expressions, but instead its own pattern matching constructs. Regular expressions entered popular use from 1968 in two uses: pattern matching in a text editor and lexical analysis in a compiler. Among the first appearances of regular expressions in program form was when Ken Thompson built Kleene's notation into the editor QED as a means to match patterns in text files. For speed, Thompson implemented regular expression matching by just-in-time compilation (JIT) to IBM 7094 code on the Compatible Time-Sharing System, an important early example of JIT compilation. He later added this capability to the Unix editor ed, which eventually led to the popular search tool grep's use of regular expressions (\"grep\" is a word derived from the command for regular expression searching in the ed editor: g/re/p meaning \"Global search for Regular Expression and Print matching lines\"). Around the same time that Thompson developed QED, a group of researchers including Douglas T. Ross implemented a tool based on regular expressions that is used for lexical analysis in compiler design. Many variations of these original forms of regular expressions were used in Unix programs at Bell Labs in the 1970s, including lex, sed, AWK, and expr, and in other programs such as vi, and Emacs (which has its own, incompatible syntax and behavior). Regexes were subsequently adopted by a wide range of programs, with these early forms standardized in the POSIX.2 standard in 1992. In the 1980s, the more complicated regexes arose in Perl, which originally derived from a regex library written by Henry Spencer (1986), who later wrote an implementation for Tcl called Advanced Regular Expressions. The Tcl library is a hybrid NFA/DFA implementation with improved performance characteristics. Software projects that have adopted Spencer's Tcl regular expression implementation include PostgreSQL. Perl later expanded on Spencer's original library to add many new features. Part of the effort in the design of Raku (formerly named Perl 6) is to improve Perl's regex integration, and to increase their scope and capabilities to allow the definition of parsing expression grammars. The result is a mini-language called Raku rules, which are used to define Raku grammar as well as provide a tool to programmers in the language. These rules maintain existing features of Perl 5.x regexes, but also allow BNF-style definition of a recursive descent parser via sub-rules. The use of regexes in structured information standards for document and database modeling started in the 1960s and expanded in the 1980s when industry standards like ISO SGML (precursored by ANSI \"GCA 101-1983\") consolidated. The kernel of the structure specification language standards consists of regexes. Its use is evident in the DTD element group syntax. Prior to the use of regular expressions, many search languages allowed simple wildcards, for example \"*\" to match any sequence of characters, and \"?\" to match a single character. Relics of this can be found today in the glob syntax for filenames, and in the SQL LIKE operator. Starting in 1997, Philip Hazel developed PCRE (Perl Compatible Regular Expressions), which attempts to closely mimic Perl's regex functionality and is used by many modern tools including PHP and Apache HTTP Server. Today, regexes are widely supported in programming languages, text processing programs (particularly lexers), advanced text editors, and some other programs. Regex support is part of the standard library of many programming languages, including Java and Python, and is built into the syntax of others, including Perl and ECMAScript. In the late 2010s, several companies started to offer hardware, FPGA, GPU implementations of PCRE compatible regex engines that are faster compared to CPU implementations. == Patterns == The phrase regular expressions, or regexes, is often used to mean the specific, standard textual syntax for representing patterns for matching text, as distinct from the mathematical notation described below. Each character in a regular expression (that is, each character in the string describing its pattern) is either a metacharacter, having a special meaning, or a regular character that has a literal meaning. For example, in the regex b., 'b' is a literal character that matches just 'b', while '.' is a metacharacter that matches every character except a newline. Therefore, this regex matches, for example, 'b%', or 'bx', or 'b5'. Together, metacharacters and literal characters can be used to identify text of a given pattern or process a number of instances of it. Pattern matches may vary from a precise equality to a very general similarity, as controlled by the metacharacters. For example, . is a very general pattern, [a-z] (match all lowercase letters from 'a' to 'z') is less general and b is a precise pattern (matches just 'b'). The metacharacter syntax is designed specifically to represent prescribed targets in a concise and flexible way to direct the automation of text processing of a variety of input data, in a form easy to type using a standard ASCII keyboard. A very simple case of a regular expression in this syntax is to locate a word spelled two different ways in a text editor, the regular expression seriali[sz]e matches both \"serialise\" and \"serialize\". Wildcard characters also achieve this, but are more limited in what they can pattern, as they have fewer metacharacters and a simple language-base. The usual context of wildcard characters is in globbing similar names in a list of files, whereas regexes are usually employed in applications that pattern-match text strings in general. For example, the regex ^[ \\t]+|[ \\t]+$ matches excess whitespace at the beginning or end of a line. An advanced regular expression that matches any numeral is [+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?. A regex processor translates a regular expression in the above syntax into an internal representation that can be executed and matched against a string representing the text being searched in. One possible approach is the Thompson's construction algorithm to construct a nondeterministic finite automaton (NFA), which is then made deterministic and the resulting deterministic finite automaton (DFA) is run on the target text string to recognize substrings that match the regular expression. The picture shows the NFA scheme N(s*) obtained from the regular expression s*, where s denotes a simpler regular expression in turn, which has already been recursively translated to the NFA N(s). == Basic concepts == A regular expression, often called a pattern, specifies a set of strings required for a particular purpose. A simple way to specify a finite set of strings is to list its elements or members. However, there are often more concise ways: for example, the set containing the three strings \"Handel\", \"Händel\", and \"Haendel\" can be specified by the pattern H(ä|ae?)ndel; we say that this pattern matches each of the three strings. However, there can be many ways to write a regular expression for the same set of strings: for example, (Hän|Han|Haen)del also specifies the same set of three strings in this example. Most formalisms provide the following operations to construct regular expressions. Boolean \"or\" A vertical bar separates alternatives. For example, gray|grey can match \"gray\" or \"grey\". Grouping Parentheses are used to define the scope and precedence of the operators (among other uses). For example, gray|grey and gr(a|e)y are equivalent patterns which both describe the set of \"gray\" or \"grey\". Quantification A quantifier after an element (such as a token, character, or group) specifies how many times the preceding element is allowed to repeat. The most common quantifiers are the question mark ?, the asterisk * (derived from the Kleene star), and the plus sign + (Kleene plus). Wildcard The wildcard . matches any character. For example, a.b matches any string that contains an \"a\", and then any character and then \"b\". a.*b matches any string that contains an \"a\", and then the character \"b\" at some later point. These constructions can be combined to form arbitrarily complex expressions, much like one can construct arithmetical expressions from numbers and the operations +, −, ×, and ÷. The precise syntax for regular expressions varies among tools and with context; more detail is given in § Syntax. == Formal language theory == Regular expressions describe regular languages in formal language theory. They have the same expressive power as regular grammars. But the language of regular expressions itself, is context-free language. === Formal definition === Regular expressions consist of constants, which denote sets of strings, and operator symbols, which denote operations over these sets. The following definition is standard, and found as such in most textbooks on formal language theory. Given a finite alphabet Σ, the following constants are defined as regular expressions: (empty set) ∅ denoting the set ∅. (empty string) ε denoting the set containing only the \"empty\" string, which has no characters at all. (literal character) a in Σ denoting the set containing only the character a. Given regular expressions R and S, the following operations over them are defined to produce regular expressions: (concatenation) (RS) denotes the set of strings that can be obtained by concatenating a string accepted by R and a string accepted by S (in that order). For example, let R denote {\"ab\", \"c\"} and S denote {\"d\", \"ef\"}. Then, (RS) denotes {\"abd\", \"abef\", \"cd\", \"cef\"}. (alternation) (R|S) denotes the set union of sets described by R and S. For example, if R describes {\"ab\", \"c\"} and S describes {\"ab\", \"d\", \"ef\"}, expression (R|S) describes {\"ab\", \"c\", \"d\", \"ef\"}. (Kleene star) (R*) denotes the smallest superset of the set described by R that contains ε and is closed under string concatenation. This is the set of all strings that can be made by concatenating any finite number (including zero) of strings from the set described by R. For example, if R denotes {\"0\", \"1\"}, (R*) denotes the set of all finite binary strings (including the empty string). If R denotes {\"ab\", \"c\"}, (R*) denotes {ε, \"ab\", \"c\", \"abab\", \"abc\", \"cab\", \"cc\", \"ababab\", \"abcab\", ...}. To avoid parentheses, it is assumed that the Kleene star has the highest priority followed by concatenation, then alternation. If there is no ambiguity, then parentheses may be omitted. For example, (ab)c can be written as abc, and a|(b(c*)) can be written as a|bc*. Many textbooks use the symbols ∪, +, or ∨ for alternation instead of the vertical bar. Examples: a|b* denotes {ε, \"a\", \"b\", \"bb\", \"bbb\", ...} (a|b)* denotes the set of all strings with no symbols other than \"a\" and \"b\", including the empty string: {ε, \"a\", \"b\", \"aa\", \"ab\", \"ba\", \"bb\", \"aaa\", ...} ab*(c|ε) denotes the set of strings starting with \"a\", then zero or more \"b\"s and finally optionally a \"c\": {\"a\", \"ac\", \"ab\", \"abc\", \"abb\", \"abbc\", ...} (0|(1(01*0)*1))* denotes the set of binary numbers that are multiples of 3: { ε, \"0\", \"00\", \"11\", \"000\", \"011\", \"110\", \"0000\", \"0011\", \"0110\", \"1001\", \"1100\", \"1111\", \"00000\", ...} === Expressive power and compactness === The formal definition of regular expressions is minimal on purpose, and avoids defining ? and +—these can be expressed as follows: a+=aa*, and a?=(a|ε). Sometimes the complement operator is added, to give a generalized regular expression; here Rc matches all strings over Σ* that do not match R. In principle, the complement operator is redundant, because it does not grant any more expressive power. However, it can make a regular expression much more concise—eliminating a single complement operator can cause a double exponential blow-up of its length. Regular expressions in this sense can express the regular languages, exactly the class of languages accepted by deterministic finite automata. There is, however, a significant difference in compactness. Some classes of regular languages can only be described by deterministic finite automata whose size grows exponentially in the size of the shortest equivalent regular expressions. The standard example here is the languages Lk consisting of all strings over the alphabet {a,b} whose kth-from-last letter equals a. On the one hand, a regular expression describing L4 is given by ( a ∣ b ) ∗ a ( a ∣ b ) ( a ∣ b ) ( a ∣ b ) {\\displaystyle (a\\mid b)^{*}a(a\\mid b)(a\\mid b)(a\\mid b)} . Generalizing this pattern to Lk gives the expression: ( a ∣ b ) ∗ a ( a ∣ b ) ( a ∣ b ) ⋯ ( a ∣ b ) ⏟ k − 1 times . {\\displaystyle (a\\mid b)^{*}a\\underbrace {(a\\mid b)(a\\mid b)\\cdots (a\\mid b)} _{k-1{\\text{ times}}}.\\,} On the other hand, it is known that every deterministic finite automaton accepting the language Lk must have at least 2k states. Luckily, there is a simple mapping from regular expressions to the more general nondeterministic finite automata (NFAs) that does not lead to such a blowup in size; for this reason NFAs are often used as alternative representations of regular languages. NFAs are a simple variation of the type-3 grammars of the Chomsky hierarchy. In the opposite direction, there are many languages easily described by a DFA that are not easily described by a regular expression. For instance, determining the validity of a given ISBN requires computing the modulus of the integer base 11, and can be easily implemented with an 11-state DFA. However, converting it to a regular expression results in a 2,14 megabytes file . Given a regular expression, Thompson's construction algorithm computes an equivalent nondeterministic finite automaton. A conversion in the opposite direction is achieved by Kleene's algorithm. Finally, many real-world \"regular expression\" engines implement features that cannot be described by the regular expressions in the sense of formal language theory; rather, they implement regexes. See below for more on this. === Deciding equivalence of regular expressions === As seen in many of the examples above, there is more than one way to construct a regular expression to achieve the same results. It is possible to write an algorithm that, for two given regular expressions, decides whether the described languages are equal; the algorithm reduces each expression to a minimal deterministic finite state machine, and determines whether they are isomorphic (equivalent). Algebraic laws for regular expressions can be obtained using a method by Gischer which is best explained along an example: In order to check whether (X+Y)∗ and (X∗ Y∗)∗ denote the same regular language, for all regular expressions X, Y, it is necessary and sufficient to check whether the particular regular expressions (a+b)∗ and (a∗ b∗)∗ denote the same language over the alphabet Σ={a,b}. More generally, an equation E=F between regular-expression terms with variables holds if, and only if, its instantiation with different variables replaced by different symbol constants holds. Every regular expression can be written solely in terms of the Kleene star and set unions over finite words. This is a surprisingly difficult problem. As simple as the regular expressions are, there is no method to systematically rewrite them to some normal form. The lack of axiom in the past led to the star height problem. In 1991, Dexter Kozen axiomatized regular expressions as a Kleene algebra, using equational and Horn clause axioms. Already in 1964, Redko had proved that no finite set of purely equational axioms can characterize the algebra of regular languages. == Syntax == A regex pattern matches a target string. The pattern is composed of a sequence of atoms. An atom is a single point within the regex pattern which it tries to match to the target string. The simplest atom is a literal, but grouping parts of the pattern to match an atom will require using ( ) as metacharacters. Metacharacters help form: atoms; quantifiers telling how many atoms (and whether it is a greedy quantifier or not); a logical OR character, which offers a set of alternatives, and a logical NOT character, which negates an atom's existence; and backreferences to refer to previous atoms of a completing pattern of atoms. A match is made, not when all the atoms of the string are matched, but rather when all the pattern atoms in the regex have matched. The idea is to make a small pattern of characters stand for a large number of possible strings, rather than compiling a large list of all the literal possibilities. Depending on the regex processor there are about fourteen metacharacters, characters that may or may not have their literal character meaning, depending on context, or whether they are \"escaped\", i.e. preceded by an escape sequence, in this case, the backslash \\. Modern and POSIX extended regexes use metacharacters more often than their literal meaning, so to avoid \"backslash-osis\" or leaning toothpick syndrome, they have a metacharacter escape to a literal mode; starting out, however, they instead have the four bracketing metacharacters ( ) and { } be primarily literal, and \"escape\" this usual meaning to become metacharacters. Common standards implement both. The usual metacharacters are {}[]()^$.|*+? and \\. The usual characters that become metacharacters when escaped are dswDSW and N. === Delimiters === When entering a regex in a programming language, they may be represented as a usual string literal, hence usually quoted; this is common in C, Java, and Python for instance, where the regex re is entered as \"re\". However, they are often written with slashes as delimiters, as in /re/ for the regex re. This originates in ed, where / is the editor command for searching, and an expression /re/ can be used to specify a range of lines (matching the pattern), which can be combined with other commands on either side, most famously g/re/p as in grep (\"global regex print\"), which is included in most Unix-based operating systems, such as Linux distributions. A similar convention is used in sed, where search and replace is given by s/re/replacement/ and patterns can be joined with a comma to specify a range of lines as in /re1/,/re2/. This notation is particularly well known due to its use in Perl, where it forms part of the syntax distinct from normal string literals. In some cases, such as sed and Perl, alternative delimiters can be used to avoid collision with contents, and to avoid having to escape occurrences of the delimiter character in the contents. For example, in sed the command s,/,X, will replace a / with an X, using commas as delimiters. === IEEE POSIX Standard === The IEEE POSIX standard has three sets of compliance: BRE (Basic Regular Expressions), ERE (Extended Regular Expressions), and SRE (Simple Regular Expressions). SRE is deprecated, in favor of BRE, as both provide backward compatibility. The subsection below covering the character classes applies to both BRE and ERE. BRE and ERE work together. ERE adds ?, +, and |, and it removes the need to escape the metacharacters ( ) and { }, which are required in BRE. Furthermore, as long as the POSIX standard syntax for regexes is adhered to, there can be, and often is, additional syntax to serve specific (yet POSIX compliant) applications. Although POSIX.2 leaves some implementation specifics undefined, BRE and ERE provide a \"standard\" which has since been adopted as the default syntax of many tools, where the choice of BRE or ERE modes is usually a supported option. For example, GNU grep has the following options: \"grep -E\" for ERE, and \"grep -G\" for BRE (the default), and \"grep -P\" for Perl regexes. Perl regexes have become a de facto standard, having a rich and powerful set of atomic expressions. Perl has no \"basic\" or \"extended\" levels. As in POSIX EREs, ( ) and { } are treated as metacharacters unless escaped; other metacharacters are known to be literal or symbolic based on context alone. Additional functionality includes lazy matching, backreferences, named capture groups, and recursive patterns. ==== POSIX basic and extended ==== In the POSIX standard, Basic Regular Syntax (BRE) requires that the metacharacters ( ) and { } be designated \\(\\) and \\{\\}, whereas Extended Regular Syntax (ERE) does not. Examples: .at matches any three-character string ending with \"at\", including \"hat\", \"cat\", \"bat\", \"4at\", \"#at\" and \" at\" (starting with a space). [hc]at matches \"hat\" and \"cat\". [^b]at matches all strings matched by .at except \"bat\". [^hc]at matches all strings matched by .at other than \"hat\" and \"cat\". ^[hc]at matches \"hat\" and \"cat\", but only at the beginning of the string or line. [hc]at$ matches \"hat\" and \"cat\", but only at the end of the string or line. \\[.\\] matches any single character surrounded by \"[\" and \"]\" since the brackets are escaped, for example: \"[a]\", \"[b]\", \"\", \"[@]\", \"[]]\", and \"[ ]\" (bracket space bracket). s.* matches s followed by zero or more characters, for example: \"s\", \"saw\", \"seed\", \"s3w96.7\", and \"s6#h%(>>>m n mQ\". According to Russ Cox, the POSIX specification requires ambiguous subexpressions to be handled in a way different from Perl's. The committee replaced Perl's rules with one that is simple to explain, but the new \"simple\" rules are actually more complex to implement: they were incompatible with pre-existing tooling and made it essentially impossible to define a \"lazy match\" (see below) extension. As a result, very few programs actually implement the POSIX subexpression rules (even when they implement other parts of the POSIX syntax). ==== Metacharacters in POSIX extended ==== The meaning of metacharacters escaped with a backslash is reversed for some characters in the POSIX Extended Regular Expression (ERE) syntax. With this syntax, a backslash causes the metacharacter to be treated as a literal character. So, for example, \\( \\) is now ( ) and \\{ \\} is now { }. Additionally, support is removed for \\n backreferences and the following metacharacters are added: Examples: [hc]?at matches \"at\", \"hat\", and \"cat\". [hc]*at matches \"at\", \"hat\", \"cat\", \"hhat\", \"chat\", \"hcat\", \"cchchat\", and so on. [hc]+at matches \"hat\", \"cat\", \"hhat\", \"chat\", \"hcat\", \"cchchat\", and so on, but not \"at\". cat|dog matches \"cat\" or \"dog\". POSIX Extended Regular Expressions can often be used with modern Unix utilities by including the command line flag -E. ==== Character classes ==== The character class is the most basic regex concept after a literal match. It makes one small sequence of characters match a larger set of characters. For example, [A-Z] could stand for any uppercase letter in the English alphabet, and \\d could mean any digit. Character classes apply to both POSIX levels. When specifying a range of characters, such as [a-Z] (i.e. lowercase a to uppercase Z), the computer's locale settings determine the contents by the numeric ordering of the character encoding. They could store digits in that sequence, or the ordering could be abc...zABC...Z, or aAbBcC...zZ. So the POSIX standard defines a character class, which will be known by the regex processor installed. Those definitions are in the following table: POSIX character classes can only be used within bracket expressions. For example, [[:upper:]ab] matches the uppercase letters and lowercase \"a\" and \"b\". An additional non-POSIX class understood by some tools is [:word:], which is usually defined as [:alnum:] plus underscore. This reflects the fact that in many programming languages these are the characters that may be used in identifiers. The editor Vim further distinguishes word and word-head classes (using the notation \\w and \\h) since in many programming languages the characters that can begin an identifier are not the same as those that can occur in other positions: numbers are generally excluded, so an identifier would look like \\h\\w* or [[:alpha:]_][[:alnum:]_]* in POSIX notation. Note that what the POSIX regex standards call character classes are commonly referred to as POSIX character classes in other regex flavors which support them. With most other regex flavors, the term character class is used to describe what POSIX calls bracket expressions. === Perl and PCRE === Because of its expressive power and (relative) ease of reading, many other utilities and programming languages have adopted syntax similar to Perl's—for example, Java, JavaScript, Julia, Python, Ruby, Qt, Microsoft's .NET Framework, and XML Schema. Some languages and tools such as Boost and PHP support multiple regex flavors. Perl-derivative regex implementations are not identical and usually implement a subset of features found in Perl 5.0, released in 1994. Perl sometimes does incorporate features initially found in other languages. For example, Perl 5.10 implements syntactic extensions originally developed in PCRE and Python. === Lazy matching === In Python and some other implementations (e.g. Java), the three common quantifiers (*, + and ?) are greedy by default because they match as many characters as possible. The regex \".+\" (including the double-quotes) applied to the string \"Ganymede,\" he continued, \"is the largest moon in the Solar System.\" matches the entire line (because the entire line begins and ends with a double-quote) instead of matching only the first part, \"Ganymede,\". The aforementioned quantifiers may, however, be made lazy or minimal or reluctant, matching as few characters as possible, by appending a question mark: \".+?\" matches only \"Ganymede,\". === Possessive matching === In Java and Python 3.11+, quantifiers may be made possessive by appending a plus sign, which disables backing off (in a backtracking engine), even if doing so would allow the overall match to succeed: While the regex \".*\" applied to the string \"Ganymede,\" he continued, \"is the largest moon in the Solar System.\" matches the entire line, the regex \".*+\" does not match at all, because .*+ consumes the entire input, including the final \". Thus, possessive quantifiers are most useful with negated character classes, e.g. \"[^\"]*+\", which matches \"Ganymede,\" when applied to the same string. Another common extension serving the same function is atomic grouping, which disables backtracking for a parenthesized group. The typical syntax is (?>group). For example, while ^(wi|w)i$ matches both wi and wii, ^(?>wi|w)i$ only matches wii because the engine is forbidden from backtracking and so cannot try setting the group to \"w\" after matching \"wi\". Possessive quantifiers are easier to implement than greedy and lazy quantifiers, and are typically more efficient at runtime. === IETF I-Regexp === IETF RFC 9485 describes \"I-Regexp: An Interoperable Regular Expression Format\". It specifies a limited subset of regular-expression idioms designed to be interoperable, i.e. produce the same effect, in a large number of regular-expression libraries. I-Regexp is also limited to matching, i.e. providing a true or false match between a regular expression and a given piece of text. Thus, it lacks advanced features such as capture groups, lookahead, and backreferences. == Patterns for non-regular languages == Many features found in virtually all modern regular expression libraries provide an expressive power that exceeds the regular languages. For example, many implementations allow grouping subexpressions with parentheses and recalling the value they match in the same expression (backreferences). This means that, among other things, a pattern can match strings of repeated words like \"papa\" or \"WikiWiki\", called squares in formal language theory. The pattern for these strings is (.+)\\1. The language of squares is not regular, nor is it context-free, due to the pumping lemma. However, pattern matching with an unbounded number of backreferences, as supported by numerous modern tools, is still context sensitive. The general problem of matching any number of backreferences is NP-complete, and the execution time for known algorithms grows exponentially by the number of backreference groups used. However, many tools, libraries, and engines that provide such constructions still use the term regular expression for their patterns. This has led to a nomenclature where the term regular expression has different meanings in formal language theory and pattern matching. For this reason, some people have taken to using the term regex, regexp, or simply pattern to describe the latter. Larry Wall, author of the Perl programming language, writes in an essay about the design of Raku: \"Regular expressions\" […] are only marginally related to real regular expressions. Nevertheless, the term has grown with the capabilities of our pattern matching engines, so I'm not going to try to fight linguistic necessity here. I will, however, generally call them \"regexes\" (or \"regexen\", when I'm in an Anglo-Saxon mood). === Assertions === Other features not found in describing regular languages include assertions. These include the ubiquitous ^ and $, used since at least 1970, as well as some more sophisticated extensions like lookaround that appeared in 1994. Lookarounds define the surrounding of a match and do not spill into the match itself, a feature only relevant for the use case of string searching. Some of them can be simulated in a regular language by treating the surroundings as a part of the language as well. The look-ahead assertions (?=...) and (?!...) have been attested since at least 1994, starting with Perl 5. The lookbehind assertions (?<=...) and (?"
  },
  "chunks": [
    {
      "id": "regularexpression_7ed1299f_c0000",
      "article_id": "regularexpression_7ed1299f",
      "section": "Lead",
      "heading_path": "Lead",
      "start_char": 0,
      "end_char": 981,
      "content": "A regular expression (shortened as regex or regexp), sometimes referred to as a rational expression, is a sequence of characters that specifies a match pattern in text. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation. Regular expression techniques are developed in theoretical computer science and formal language theory. The concept of regular expressions began in the 1950s, when the American mathematician Stephen Cole Kleene formalized the concept of a regular language. They came into common use with Unix text-processing utilities. Different syntaxes for writing regular expressions have existed since the 1980s, one being the POSIX standard and another, widely used, being the Perl syntax. Regular expressions are used in search engines, in search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK, and in lexical analysis.",
      "char_count": 980,
      "token_estimate": 245,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0001",
      "article_id": "regularexpression_7ed1299f",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 1156,
      "end_char": 2029,
      "content": "== History == Regular expressions originated in 1951, when mathematician Stephen Cole Kleene described regular languages using his mathematical notation called regular events. These arose in theoretical computer science, in the subfields of automata theory (models of computation) and the description and classification of formal languages, motivated by Kleene's attempt to describe early artificial neural networks. (Kleene introduced it as an alternative to McCulloch & Pitts's \"prehensible\", but admitted \"We would welcome any suggestions as to a more descriptive term.\") Other early implementations of pattern matching include the SNOBOL language, which did not use regular expressions, but instead its own pattern matching constructs. Regular expressions entered popular use from 1968 in two uses: pattern matching in a text editor and lexical analysis in a compiler.",
      "char_count": 872,
      "token_estimate": 218,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0002",
      "article_id": "regularexpression_7ed1299f",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 2029,
      "end_char": 2914,
      "content": "Among the first appearances of regular expressions in program form was when Ken Thompson built Kleene's notation into the editor QED as a means to match patterns in text files. For speed, Thompson implemented regular expression matching by just-in-time compilation (JIT) to IBM 7094 code on the Compatible Time-Sharing System, an important early example of JIT compilation. He later added this capability to the Unix editor ed, which eventually led to the popular search tool grep's use of regular expressions (\"grep\" is a word derived from the command for regular expression searching in the ed editor: g/re/p meaning \"Global search for Regular Expression and Print matching lines\"). Around the same time that Thompson developed QED, a group of researchers including Douglas T. Ross implemented a tool based on regular expressions that is used for lexical analysis in compiler design.",
      "char_count": 885,
      "token_estimate": 221,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0003",
      "article_id": "regularexpression_7ed1299f",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 2915,
      "end_char": 3787,
      "content": "Many variations of these original forms of regular expressions were used in Unix programs at Bell Labs in the 1970s, including lex, sed, AWK, and expr, and in other programs such as vi, and Emacs (which has its own, incompatible syntax and behavior). Regexes were subsequently adopted by a wide range of programs, with these early forms standardized in the POSIX.2 standard in 1992. In the 1980s, the more complicated regexes arose in Perl, which originally derived from a regex library written by Henry Spencer (1986), who later wrote an implementation for Tcl called Advanced Regular Expressions. The Tcl library is a hybrid NFA/DFA implementation with improved performance characteristics. Software projects that have adopted Spencer's Tcl regular expression implementation include PostgreSQL. Perl later expanded on Spencer's original library to add many new features.",
      "char_count": 872,
      "token_estimate": 218,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0004",
      "article_id": "regularexpression_7ed1299f",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 3788,
      "end_char": 4639,
      "content": "Part of the effort in the design of Raku (formerly named Perl 6) is to improve Perl's regex integration, and to increase their scope and capabilities to allow the definition of parsing expression grammars. The result is a mini-language called Raku rules, which are used to define Raku grammar as well as provide a tool to programmers in the language. These rules maintain existing features of Perl 5.x regexes, but also allow BNF-style definition of a recursive descent parser via sub-rules. The use of regexes in structured information standards for document and database modeling started in the 1960s and expanded in the 1980s when industry standards like ISO SGML (precursored by ANSI \"GCA 101-1983\") consolidated. The kernel of the structure specification language standards consists of regexes. Its use is evident in the DTD element group syntax.",
      "char_count": 851,
      "token_estimate": 212,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0005",
      "article_id": "regularexpression_7ed1299f",
      "section": "== History ==",
      "heading_path": "== History ==",
      "start_char": 4640,
      "end_char": 5636,
      "content": "Prior to the use of regular expressions, many search languages allowed simple wildcards, for example \"*\" to match any sequence of characters, and \"?\" to match a single character. Relics of this can be found today in the glob syntax for filenames, and in the SQL LIKE operator. Starting in 1997, Philip Hazel developed PCRE (Perl Compatible Regular Expressions), which attempts to closely mimic Perl's regex functionality and is used by many modern tools including PHP and Apache HTTP Server. Today, regexes are widely supported in programming languages, text processing programs (particularly lexers), advanced text editors, and some other programs. Regex support is part of the standard library of many programming languages, including Java and Python, and is built into the syntax of others, including Perl and ECMAScript. In the late 2010s, several companies started to offer hardware, FPGA, GPU implementations of PCRE compatible regex engines that are faster compared to CPU implementations.",
      "char_count": 996,
      "token_estimate": 249,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0006",
      "article_id": "regularexpression_7ed1299f",
      "section": "== Patterns ==",
      "heading_path": "== Patterns ==",
      "start_char": 5638,
      "end_char": 6559,
      "content": "== Patterns == The phrase regular expressions, or regexes, is often used to mean the specific, standard textual syntax for representing patterns for matching text, as distinct from the mathematical notation described below. Each character in a regular expression (that is, each character in the string describing its pattern) is either a metacharacter, having a special meaning, or a regular character that has a literal meaning. For example, in the regex b., 'b' is a literal character that matches just 'b', while '.' is a metacharacter that matches every character except a newline. Therefore, this regex matches, for example, 'b%', or 'bx', or 'b5'. Together, metacharacters and literal characters can be used to identify text of a given pattern or process a number of instances of it. Pattern matches may vary from a precise equality to a very general similarity, as controlled by the metacharacters. For example, .",
      "char_count": 920,
      "token_estimate": 230,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0007",
      "article_id": "regularexpression_7ed1299f",
      "section": "== Patterns ==",
      "heading_path": "== Patterns ==",
      "start_char": 6559,
      "end_char": 7476,
      "content": "is a very general pattern, [a-z] (match all lowercase letters from 'a' to 'z') is less general and b is a precise pattern (matches just 'b'). The metacharacter syntax is designed specifically to represent prescribed targets in a concise and flexible way to direct the automation of text processing of a variety of input data, in a form easy to type using a standard ASCII keyboard. A very simple case of a regular expression in this syntax is to locate a word spelled two different ways in a text editor, the regular expression seriali[sz]e matches both \"serialise\" and \"serialize\". Wildcard characters also achieve this, but are more limited in what they can pattern, as they have fewer metacharacters and a simple language-base. The usual context of wildcard characters is in globbing similar names in a list of files, whereas regexes are usually employed in applications that pattern-match text strings in general.",
      "char_count": 917,
      "token_estimate": 229,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0008",
      "article_id": "regularexpression_7ed1299f",
      "section": "== Patterns ==",
      "heading_path": "== Patterns ==",
      "start_char": 7477,
      "end_char": 8365,
      "content": "For example, the regex ^[ \\t]+|[ \\t]+$ matches excess whitespace at the beginning or end of a line. An advanced regular expression that matches any numeral is [+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?. A regex processor translates a regular expression in the above syntax into an internal representation that can be executed and matched against a string representing the text being searched in. One possible approach is the Thompson's construction algorithm to construct a nondeterministic finite automaton (NFA), which is then made deterministic and the resulting deterministic finite automaton (DFA) is run on the target text string to recognize substrings that match the regular expression. The picture shows the NFA scheme N(s*) obtained from the regular expression s*, where s denotes a simpler regular expression in turn, which has already been recursively translated to the NFA N(s).",
      "char_count": 888,
      "token_estimate": 222,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0009",
      "article_id": "regularexpression_7ed1299f",
      "section": "== Basic concepts ==",
      "heading_path": "== Basic concepts ==",
      "start_char": 8372,
      "end_char": 9291,
      "content": "== Basic concepts == A regular expression, often called a pattern, specifies a set of strings required for a particular purpose. A simple way to specify a finite set of strings is to list its elements or members. However, there are often more concise ways: for example, the set containing the three strings \"Handel\", \"Händel\", and \"Haendel\" can be specified by the pattern H(ä|ae?)ndel; we say that this pattern matches each of the three strings. However, there can be many ways to write a regular expression for the same set of strings: for example, (Hän|Han|Haen)del also specifies the same set of three strings in this example. Most formalisms provide the following operations to construct regular expressions. Boolean \"or\" A vertical bar separates alternatives. For example, gray|grey can match \"gray\" or \"grey\". Grouping Parentheses are used to define the scope and precedence of the operators (among other uses).",
      "char_count": 918,
      "token_estimate": 229,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0010",
      "article_id": "regularexpression_7ed1299f",
      "section": "== Basic concepts ==",
      "heading_path": "== Basic concepts ==",
      "start_char": 9291,
      "end_char": 10215,
      "content": "For example, gray|grey and gr(a|e)y are equivalent patterns which both describe the set of \"gray\" or \"grey\". Quantification A quantifier after an element (such as a token, character, or group) specifies how many times the preceding element is allowed to repeat. The most common quantifiers are the question mark ?, the asterisk * (derived from the Kleene star), and the plus sign + (Kleene plus). Wildcard The wildcard . matches any character. For example, a.b matches any string that contains an \"a\", and then any character and then \"b\". a.*b matches any string that contains an \"a\", and then the character \"b\" at some later point. These constructions can be combined to form arbitrarily complex expressions, much like one can construct arithmetical expressions from numbers and the operations +, −, ×, and ÷. The precise syntax for regular expressions varies among tools and with context; more detail is given in § Syntax.",
      "char_count": 924,
      "token_estimate": 231,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0011",
      "article_id": "regularexpression_7ed1299f",
      "section": "== Formal language theory ==",
      "heading_path": "== Formal language theory ==",
      "start_char": 10224,
      "end_char": 10430,
      "content": "== Formal language theory == Regular expressions describe regular languages in formal language theory. They have the same expressive power as regular grammars. But the language of regular expressions itself, is context-free language.",
      "char_count": 233,
      "token_estimate": 58,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0012",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Formal definition ==",
      "heading_path": "== = Formal definition ==",
      "start_char": 10455,
      "end_char": 11398,
      "content": "== = Formal definition === Regular expressions consist of constants, which denote sets of strings, and operator symbols, which denote operations over these sets. The following definition is standard, and found as such in most textbooks on formal language theory. Given a finite alphabet Σ, the following constants are defined as regular expressions: (empty set) ∅ denoting the set ∅. (empty string) ε denoting the set containing only the \"empty\" string, which has no characters at all. (literal character) a in Σ denoting the set containing only the character a. Given regular expressions R and S, the following operations over them are defined to produce regular expressions: (concatenation) (RS) denotes the set of strings that can be obtained by concatenating a string accepted by R and a string accepted by S (in that order). For example, let R denote {\"ab\", \"c\"} and S denote {\"d\", \"ef\"}. Then, (RS) denotes {\"abd\", \"abef\", \"cd\", \"cef\"}.",
      "char_count": 942,
      "token_estimate": 235,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0013",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Formal definition ==",
      "heading_path": "== = Formal definition ==",
      "start_char": 11398,
      "end_char": 12360,
      "content": "(alternation) (R|S) denotes the set union of sets described by R and S. For example, if R describes {\"ab\", \"c\"} and S describes {\"ab\", \"d\", \"ef\"}, expression (R|S) describes {\"ab\", \"c\", \"d\", \"ef\"}. (Kleene star) (R*) denotes the smallest superset of the set described by R that contains ε and is closed under string concatenation. This is the set of all strings that can be made by concatenating any finite number (including zero) of strings from the set described by R. For example, if R denotes {\"0\", \"1\"}, (R*) denotes the set of all finite binary strings (including the empty string). If R denotes {\"ab\", \"c\"}, (R*) denotes {ε, \"ab\", \"c\", \"abab\", \"abc\", \"cab\", \"cc\", \"ababab\", \"abcab\", ...}. To avoid parentheses, it is assumed that the Kleene star has the highest priority followed by concatenation, then alternation. If there is no ambiguity, then parentheses may be omitted. For example, (ab)c can be written as abc, and a|(b(c*)) can be written as a|bc*.",
      "char_count": 962,
      "token_estimate": 240,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0014",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Formal definition ==",
      "heading_path": "== = Formal definition ==",
      "start_char": 12361,
      "end_char": 12992,
      "content": "Many textbooks use the symbols ∪, +, or ∨ for alternation instead of the vertical bar. Examples: a|b* denotes {ε, \"a\", \"b\", \"bb\", \"bbb\", ...} (a|b)* denotes the set of all strings with no symbols other than \"a\" and \"b\", including the empty string: {ε, \"a\", \"b\", \"aa\", \"ab\", \"ba\", \"bb\", \"aaa\", ...} ab*(c|ε) denotes the set of strings starting with \"a\", then zero or more \"b\"s and finally optionally a \"c\": {\"a\", \"ac\", \"ab\", \"abc\", \"abb\", \"abbc\", ...} (0|(1(01*0)*1))* denotes the set of binary numbers that are multiples of 3: { ε, \"0\", \"00\", \"11\", \"000\", \"011\", \"110\", \"0000\", \"0011\", \"0110\", \"1001\", \"1100\", \"1111\", \"00000\", ...}",
      "char_count": 631,
      "token_estimate": 157,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0015",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Expressive power and compactness ==",
      "heading_path": "== = Expressive power and compactness ==",
      "start_char": 13008,
      "end_char": 13809,
      "content": "== = Expressive power and compactness === The formal definition of regular expressions is minimal on purpose, and avoids defining ? and +—these can be expressed as follows: a+=aa*, and a?=(a|ε). Sometimes the complement operator is added, to give a generalized regular expression; here Rc matches all strings over Σ* that do not match R. In principle, the complement operator is redundant, because it does not grant any more expressive power. However, it can make a regular expression much more concise—eliminating a single complement operator can cause a double exponential blow-up of its length. Regular expressions in this sense can express the regular languages, exactly the class of languages accepted by deterministic finite automata. There is, however, a significant difference in compactness.",
      "char_count": 800,
      "token_estimate": 200,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0016",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Expressive power and compactness ==",
      "heading_path": "== = Expressive power and compactness ==",
      "start_char": 13809,
      "end_char": 14638,
      "content": "Some classes of regular languages can only be described by deterministic finite automata whose size grows exponentially in the size of the shortest equivalent regular expressions. The standard example here is the languages Lk consisting of all strings over the alphabet {a,b} whose kth-from-last letter equals a. On the one hand, a regular expression describing L4 is given by ( a ∣ b ) ∗ a ( a ∣ b ) ( a ∣ b ) ( a ∣ b ) {\\displaystyle (a\\mid b)^{*}a(a\\mid b)(a\\mid b)(a\\mid b)} . Generalizing this pattern to Lk gives the expression: ( a ∣ b ) ∗ a ( a ∣ b ) ( a ∣ b ) ⋯ ( a ∣ b ) ⏟ k − 1 times . {\\displaystyle (a\\mid b)^{*}a\\underbrace {(a\\mid b)(a\\mid b)\\cdots (a\\mid b)} _{k-1{\\text{ times}}}.\\,} On the other hand, it is known that every deterministic finite automaton accepting the language Lk must have at least 2k states.",
      "char_count": 829,
      "token_estimate": 207,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0017",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Expressive power and compactness ==",
      "heading_path": "== = Expressive power and compactness ==",
      "start_char": 14639,
      "end_char": 15540,
      "content": "Luckily, there is a simple mapping from regular expressions to the more general nondeterministic finite automata (NFAs) that does not lead to such a blowup in size; for this reason NFAs are often used as alternative representations of regular languages. NFAs are a simple variation of the type-3 grammars of the Chomsky hierarchy. In the opposite direction, there are many languages easily described by a DFA that are not easily described by a regular expression. For instance, determining the validity of a given ISBN requires computing the modulus of the integer base 11, and can be easily implemented with an 11-state DFA. However, converting it to a regular expression results in a 2,14 megabytes file . Given a regular expression, Thompson's construction algorithm computes an equivalent nondeterministic finite automaton. A conversion in the opposite direction is achieved by Kleene's algorithm.",
      "char_count": 901,
      "token_estimate": 225,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0018",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Expressive power and compactness ==",
      "heading_path": "== = Expressive power and compactness ==",
      "start_char": 15541,
      "end_char": 15765,
      "content": "Finally, many real-world \"regular expression\" engines implement features that cannot be described by the regular expressions in the sense of formal language theory; rather, they implement regexes. See below for more on this.",
      "char_count": 224,
      "token_estimate": 56,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0019",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Deciding equivalence of regular expressions ==",
      "heading_path": "== = Deciding equivalence of regular expressions ==",
      "start_char": 15777,
      "end_char": 16624,
      "content": "== = Deciding equivalence of regular expressions === As seen in many of the examples above, there is more than one way to construct a regular expression to achieve the same results. It is possible to write an algorithm that, for two given regular expressions, decides whether the described languages are equal; the algorithm reduces each expression to a minimal deterministic finite state machine, and determines whether they are isomorphic (equivalent). Algebraic laws for regular expressions can be obtained using a method by Gischer which is best explained along an example: In order to check whether (X+Y)∗ and (X∗ Y∗)∗ denote the same regular language, for all regular expressions X, Y, it is necessary and sufficient to check whether the particular regular expressions (a+b)∗ and (a∗ b∗)∗ denote the same language over the alphabet Σ={a,b}.",
      "char_count": 846,
      "token_estimate": 211,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0020",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Deciding equivalence of regular expressions ==",
      "heading_path": "== = Deciding equivalence of regular expressions ==",
      "start_char": 16624,
      "end_char": 17389,
      "content": "More generally, an equation E=F between regular-expression terms with variables holds if, and only if, its instantiation with different variables replaced by different symbol constants holds. Every regular expression can be written solely in terms of the Kleene star and set unions over finite words. This is a surprisingly difficult problem. As simple as the regular expressions are, there is no method to systematically rewrite them to some normal form. The lack of axiom in the past led to the star height problem. In 1991, Dexter Kozen axiomatized regular expressions as a Kleene algebra, using equational and Horn clause axioms. Already in 1964, Redko had proved that no finite set of purely equational axioms can characterize the algebra of regular languages.",
      "char_count": 765,
      "token_estimate": 191,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0021",
      "article_id": "regularexpression_7ed1299f",
      "section": "== Syntax ==",
      "heading_path": "== Syntax ==",
      "start_char": 17351,
      "end_char": 18292,
      "content": "== Syntax == A regex pattern matches a target string. The pattern is composed of a sequence of atoms. An atom is a single point within the regex pattern which it tries to match to the target string. The simplest atom is a literal, but grouping parts of the pattern to match an atom will require using ( ) as metacharacters. Metacharacters help form: atoms; quantifiers telling how many atoms (and whether it is a greedy quantifier or not); a logical OR character, which offers a set of alternatives, and a logical NOT character, which negates an atom's existence; and backreferences to refer to previous atoms of a completing pattern of atoms. A match is made, not when all the atoms of the string are matched, but rather when all the pattern atoms in the regex have matched. The idea is to make a small pattern of characters stand for a large number of possible strings, rather than compiling a large list of all the literal possibilities.",
      "char_count": 940,
      "token_estimate": 235,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0022",
      "article_id": "regularexpression_7ed1299f",
      "section": "== Syntax ==",
      "heading_path": "== Syntax ==",
      "start_char": 18292,
      "end_char": 19089,
      "content": "Depending on the regex processor there are about fourteen metacharacters, characters that may or may not have their literal character meaning, depending on context, or whether they are \"escaped\", i.e. preceded by an escape sequence, in this case, the backslash \\. Modern and POSIX extended regexes use metacharacters more often than their literal meaning, so to avoid \"backslash-osis\" or leaning toothpick syndrome, they have a metacharacter escape to a literal mode; starting out, however, they instead have the four bracketing metacharacters ( ) and { } be primarily literal, and \"escape\" this usual meaning to become metacharacters. Common standards implement both. The usual metacharacters are {}[]()^$.|*+? and \\. The usual characters that become metacharacters when escaped are dswDSW and N.",
      "char_count": 797,
      "token_estimate": 199,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0023",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Delimiters ==",
      "heading_path": "== = Delimiters ==",
      "start_char": 19096,
      "end_char": 19951,
      "content": "== = Delimiters === When entering a regex in a programming language, they may be represented as a usual string literal, hence usually quoted; this is common in C, Java, and Python for instance, where the regex re is entered as \"re\". However, they are often written with slashes as delimiters, as in /re/ for the regex re. This originates in ed, where / is the editor command for searching, and an expression /re/ can be used to specify a range of lines (matching the pattern), which can be combined with other commands on either side, most famously g/re/p as in grep (\"global regex print\"), which is included in most Unix-based operating systems, such as Linux distributions. A similar convention is used in sed, where search and replace is given by s/re/replacement/ and patterns can be joined with a comma to specify a range of lines as in /re1/,/re2/.",
      "char_count": 854,
      "token_estimate": 213,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0024",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Delimiters ==",
      "heading_path": "== = Delimiters ==",
      "start_char": 19951,
      "end_char": 20374,
      "content": "This notation is particularly well known due to its use in Perl, where it forms part of the syntax distinct from normal string literals. In some cases, such as sed and Perl, alternative delimiters can be used to avoid collision with contents, and to avoid having to escape occurrences of the delimiter character in the contents. For example, in sed the command s,/,X, will replace a / with an X, using commas as delimiters.",
      "char_count": 423,
      "token_estimate": 105,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0025",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = IEEE POSIX Standard ==",
      "heading_path": "== = IEEE POSIX Standard ==",
      "start_char": 20384,
      "end_char": 21282,
      "content": "== = IEEE POSIX Standard === The IEEE POSIX standard has three sets of compliance: BRE (Basic Regular Expressions), ERE (Extended Regular Expressions), and SRE (Simple Regular Expressions). SRE is deprecated, in favor of BRE, as both provide backward compatibility. The subsection below covering the character classes applies to both BRE and ERE. BRE and ERE work together. ERE adds ?, +, and |, and it removes the need to escape the metacharacters ( ) and { }, which are required in BRE. Furthermore, as long as the POSIX standard syntax for regexes is adhered to, there can be, and often is, additional syntax to serve specific (yet POSIX compliant) applications. Although POSIX.2 leaves some implementation specifics undefined, BRE and ERE provide a \"standard\" which has since been adopted as the default syntax of many tools, where the choice of BRE or ERE modes is usually a supported option.",
      "char_count": 897,
      "token_estimate": 224,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0026",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = IEEE POSIX Standard ==",
      "heading_path": "== = IEEE POSIX Standard ==",
      "start_char": 21282,
      "end_char": 21828,
      "content": "For example, GNU grep has the following options: \"grep -E\" for ERE, and \"grep -G\" for BRE (the default), and \"grep -P\" for Perl regexes. Perl regexes have become a de facto standard, having a rich and powerful set of atomic expressions. Perl has no \"basic\" or \"extended\" levels. As in POSIX EREs, ( ) and { } are treated as metacharacters unless escaped; other metacharacters are known to be literal or symbolic based on context alone. Additional functionality includes lazy matching, backreferences, named capture groups, and recursive patterns.",
      "char_count": 546,
      "token_estimate": 136,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0027",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Perl and PCRE ==",
      "heading_path": "== = Perl and PCRE ==",
      "start_char": 21823,
      "end_char": 22462,
      "content": "== = Perl and PCRE === Because of its expressive power and (relative) ease of reading, many other utilities and programming languages have adopted syntax similar to Perl's—for example, Java, JavaScript, Julia, Python, Ruby, Qt, Microsoft's .NET Framework, and XML Schema. Some languages and tools such as Boost and PHP support multiple regex flavors. Perl-derivative regex implementations are not identical and usually implement a subset of features found in Perl 5.0, released in 1994. Perl sometimes does incorporate features initially found in other languages. For example, Perl 5.10 implements syntactic extensions originally developed in PCRE and Python.",
      "char_count": 659,
      "token_estimate": 164,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0028",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Lazy matching ==",
      "heading_path": "== = Lazy matching ==",
      "start_char": 22483,
      "end_char": 23115,
      "content": "== = Lazy matching === In Python and some other implementations (e.g. Java), the three common quantifiers (*, + and ?) are greedy by default because they match as many characters as possible. The regex \".+\" (including the double-quotes) applied to the string \"Ganymede,\" he continued, \"is the largest moon in the Solar System.\" matches the entire line (because the entire line begins and ends with a double-quote) instead of matching only the first part, \"Ganymede,\". The aforementioned quantifiers may, however, be made lazy or minimal or reluctant, matching as few characters as possible, by appending a question mark: \".+?\" matches only \"Ganymede,\".",
      "char_count": 652,
      "token_estimate": 163,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0029",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Possessive matching ==",
      "heading_path": "== = Possessive matching ==",
      "start_char": 23142,
      "end_char": 23920,
      "content": "== = Possessive matching === In Java and Python 3.11+, quantifiers may be made possessive by appending a plus sign, which disables backing off (in a backtracking engine), even if doing so would allow the overall match to succeed: While the regex \".*\" applied to the string \"Ganymede,\" he continued, \"is the largest moon in the Solar System.\" matches the entire line, the regex \".*+\" does not match at all, because .*+ consumes the entire input, including the final \". Thus, possessive quantifiers are most useful with negated character classes, e.g. \"[^\"]*+\", which matches \"Ganymede,\" when applied to the same string. Another common extension serving the same function is atomic grouping, which disables backtracking for a parenthesized group. The typical syntax is (?>group).",
      "char_count": 777,
      "token_estimate": 194,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0030",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Possessive matching ==",
      "heading_path": "== = Possessive matching ==",
      "start_char": 23920,
      "end_char": 24242,
      "content": "For example, while ^(wi|w)i$ matches both wi and wii, ^(?>wi|w)i$ only matches wii because the engine is forbidden from backtracking and so cannot try setting the group to \"w\" after matching \"wi\". Possessive quantifiers are easier to implement than greedy and lazy quantifiers, and are typically more efficient at runtime.",
      "char_count": 322,
      "token_estimate": 80,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0031",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = IETF I-Regexp ==",
      "heading_path": "== = IETF I-Regexp ==",
      "start_char": 24237,
      "end_char": 24708,
      "content": "== = IETF I-Regexp === IETF RFC 9485 describes \"I-Regexp: An Interoperable Regular Expression Format\". It specifies a limited subset of regular-expression idioms designed to be interoperable, i.e. produce the same effect, in a large number of regular-expression libraries. I-Regexp is also limited to matching, i.e. providing a true or false match between a regular expression and a given piece of text. Thus, it lacks advanced features such as capture groups, lookahead, and backreferences.",
      "char_count": 491,
      "token_estimate": 122,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0032",
      "article_id": "regularexpression_7ed1299f",
      "section": "== Patterns for non-regular languages ==",
      "heading_path": "== Patterns for non-regular languages ==",
      "start_char": 24748,
      "end_char": 25688,
      "content": "== Patterns for non-regular languages == Many features found in virtually all modern regular expression libraries provide an expressive power that exceeds the regular languages. For example, many implementations allow grouping subexpressions with parentheses and recalling the value they match in the same expression (backreferences). This means that, among other things, a pattern can match strings of repeated words like \"papa\" or \"WikiWiki\", called squares in formal language theory. The pattern for these strings is (.+)\\1. The language of squares is not regular, nor is it context-free, due to the pumping lemma. However, pattern matching with an unbounded number of backreferences, as supported by numerous modern tools, is still context sensitive. The general problem of matching any number of backreferences is NP-complete, and the execution time for known algorithms grows exponentially by the number of backreference groups used.",
      "char_count": 939,
      "token_estimate": 234,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0033",
      "article_id": "regularexpression_7ed1299f",
      "section": "== Patterns for non-regular languages ==",
      "heading_path": "== Patterns for non-regular languages ==",
      "start_char": 25688,
      "end_char": 26497,
      "content": "However, many tools, libraries, and engines that provide such constructions still use the term regular expression for their patterns. This has led to a nomenclature where the term regular expression has different meanings in formal language theory and pattern matching. For this reason, some people have taken to using the term regex, regexp, or simply pattern to describe the latter. Larry Wall, author of the Perl programming language, writes in an essay about the design of Raku: \"Regular expressions\" […] are only marginally related to real regular expressions. Nevertheless, the term has grown with the capabilities of our pattern matching engines, so I'm not going to try to fight linguistic necessity here. I will, however, generally call them \"regexes\" (or \"regexen\", when I'm in an Anglo-Saxon mood).",
      "char_count": 809,
      "token_estimate": 202,
      "token_start": null,
      "token_end": null
    },
    {
      "id": "regularexpression_7ed1299f_c0034",
      "article_id": "regularexpression_7ed1299f",
      "section": "== = Assertions ==",
      "heading_path": "== = Assertions ==",
      "start_char": 26476,
      "end_char": 27117,
      "content": "== = Assertions === Other features not found in describing regular languages include assertions. These include the ubiquitous ^ and $, used since at least 1970, as well as some more sophisticated extensions like lookaround that appeared in 1994. Lookarounds define the surrounding of a match and do not spill into the match itself, a feature only relevant for the use case of string searching. Some of them can be simulated in a regular language by treating the surroundings as a part of the language as well. The look-ahead assertions (?=...) and (?!...) have been attested since at least 1994, starting with Perl 5. The lookbehind assertions (?<=...) and (?",
      "char_count": 659,
      "token_estimate": 164,
      "token_start": null,
      "token_end": null
    }
  ],
  "questions": {
    "total_questions": 7,
    "items": [
      {
        "question": "Who developed PCRE (Perl Compatible Regular Expressions) and in what year did development start?",
        "answer": "Philip Hazel started developing PCRE in 1997.",
        "related_chunk_ids": [
          "regularexpression_7ed1299f_c0005"
        ],
        "category": "FACTUAL",
        "reranked_relative_chunk_ids": [
          "regularexpression_7ed1299f_c0005"
        ]
      },
      {
        "question": "Who formalized the concept of a regular language, and in what decade did this occur?",
        "answer": "The American mathematician Stephen Cole Kleene formalized the concept of a regular language in the 1950s.",
        "related_chunk_ids": [
          "regularexpression_7ed1299f_c0000"
        ],
        "category": "FACTUAL",
        "reranked_relative_chunk_ids": [
          "regularexpression_7ed1299f_c0000"
        ]
      },
      {
        "question": "What is the assumed operator precedence in regular expressions when parentheses are omitted?",
        "answer": "The Kleene star has the highest priority, followed by concatenation, and then alternation.",
        "related_chunk_ids": [
          "regularexpression_7ed1299f_c0013"
        ],
        "category": "FACTUAL",
        "reranked_relative_chunk_ids": [
          "regularexpression_7ed1299f_c0013"
        ]
      },
      {
        "question": "Describe the evolution of regular expressions from their theoretical origins with Stephen Cole Kleene to their practical implementation and popularization in early Unix tools.",
        "answer": "The concept of regular expressions began in the 1950s, specifically originating in 1951 when mathematician Stephen Cole Kleene described regular languages using his mathematical notation called 'regular events'. This arose from theoretical computer science as an attempt to describe early artificial neural networks. Regular expressions entered popular use starting in 1968. A key step in their practical application was when Ken Thompson implemented Kleene's notation in the QED editor and later in the Unix editor 'ed'. This work directly led to the creation of the popular search tool 'grep', which derived its name from the 'ed' command for searching with regular expressions.",
        "related_chunk_ids": [
          "regularexpression_7ed1299f_c0000",
          "regularexpression_7ed1299f_c0001",
          "regularexpression_7ed1299f_c0002"
        ],
        "category": "LONG_ANSWER",
        "reranked_relative_chunk_ids": [
          "regularexpression_7ed1299f_c0001",
          "regularexpression_7ed1299f_c0002",
          "regularexpression_7ed1299f_c0000"
        ]
      },
      {
        "question": "How did Perl's regular expressions influence subsequent regex developments and implementations?",
        "answer": "Perl's regular expressions, which arose in the 1980s from a library by Henry Spencer, had a significant influence on later developments. In 1997, Philip Hazel developed PCRE (Perl Compatible Regular Expressions) specifically to mimic Perl's regex functionality, which was then adopted by many modern tools. Additionally, the design of the Raku language (formerly Perl 6) involved improving and expanding Perl's regex integration, resulting in a more powerful mini-language called Raku rules for defining grammars.",
        "related_chunk_ids": [
          "regularexpression_7ed1299f_c0003",
          "regularexpression_7ed1299f_c0004",
          "regularexpression_7ed1299f_c0005"
        ],
        "category": "LONG_ANSWER",
        "reranked_relative_chunk_ids": [
          "regularexpression_7ed1299f_c0003",
          "regularexpression_7ed1299f_c0005",
          "regularexpression_7ed1299f_c0004"
        ]
      },
      {
        "question": "Provide a comprehensive overview of how a regular expression is used, covering its syntax, purpose, and the process by which it is executed.",
        "answer": "A regular expression is a textual syntax for representing text patterns, composed of literal characters that match themselves and metacharacters with special meanings. Its purpose is to direct the automation of text processing in a concise and flexible way, for example, using the expression `seriali[sz]e` to match both \"serialise\" and \"serialize\". When processed, the regex syntax is translated into an internal representation, such as a nondeterministic finite automaton (NFA) via Thompson's construction algorithm. This NFA is then converted into a deterministic finite automaton (DFA) that is run on the target text to identify matching substrings.",
        "related_chunk_ids": [
          "regularexpression_7ed1299f_c0006",
          "regularexpression_7ed1299f_c0007",
          "regularexpression_7ed1299f_c0008"
        ],
        "category": "LONG_ANSWER",
        "reranked_relative_chunk_ids": [
          "regularexpression_7ed1299f_c0006",
          "regularexpression_7ed1299f_c0008",
          "regularexpression_7ed1299f_c0007"
        ]
      },
      {
        "question": "Trace the historical development of regular expressions, from their theoretical origins and early Unix implementations to their later evolution in languages like Perl and their adoption in industry standards.",
        "answer": "Regular expressions originated in 1951 with Stephen Cole Kleene's mathematical notation called \"regular events.\" They entered popular use in 1968, notably when Ken Thompson implemented them in the `ed` editor, leading to the `grep` tool. Throughout the 1970s, many variations were used in Unix programs like `lex`, `sed`, and `AWK`, with these early forms being standardized by POSIX.2 in 1992. In the 1980s, more complex regexes arose in Perl, based on a library by Henry Spencer. This evolution continued with Raku (formerly Perl 6), which expanded their capabilities to define parsing expression grammars. Concurrently, regexes were also adopted in structured information standards like ISO SGML, a process that began in the 1960s and expanded in the 1980s.",
        "related_chunk_ids": [
          "regularexpression_7ed1299f_c0001",
          "regularexpression_7ed1299f_c0002",
          "regularexpression_7ed1299f_c0003",
          "regularexpression_7ed1299f_c0004"
        ],
        "category": "LONG_ANSWER",
        "reranked_relative_chunk_ids": [
          "regularexpression_7ed1299f_c0001",
          "regularexpression_7ed1299f_c0003",
          "regularexpression_7ed1299f_c0002",
          "regularexpression_7ed1299f_c0004"
        ]
      }
    ]
  },
  "metadata": {
    "export_date": "2025-07-30T10:37:30.938Z",
    "content_format": "markdown",
    "total_chunks": 35,
    "description": "Complete article dataset including content, chunks, and generated questions"
  }
}